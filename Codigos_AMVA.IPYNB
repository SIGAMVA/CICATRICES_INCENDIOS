{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion areas quemedas historico\n",
    "\n",
    "En este codigo se realiza el procedimiento para la deteccion de areas quemadas en un periodo de tiempo dado. este se realiza por medio de bloques cada x dias para evitar problemas de memoria o tiempo. El script funciona con autenticacion de google earth engine asi que se tiene que crear un usuario y projecto con acceso a la API de google earth engine en google cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=_ZkMg_NovOriedmxQIH7xzrq94C0eccNzLLt998Z6KM&tc=rn4MDBxKuCc7yKZCezoEDy_hwWLVPKTvnYXAk8L6wzs&cc=fyyT7OuphG-qONpNmPGP5pGWyTDHaUX-GJM0bMCUSSY>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=_ZkMg_NovOriedmxQIH7xzrq94C0eccNzLLt998Z6KM&tc=rn4MDBxKuCc7yKZCezoEDy_hwWLVPKTvnYXAk8L6wzs&cc=fyyT7OuphG-qONpNmPGP5pGWyTDHaUX-GJM0bMCUSSY</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** Earth Engine *** Share your feedback by taking our Annual Developer Satisfaction Survey: https://google.qualtrics.com/jfe/form/SV_7TDKVSyKvBdmMqW?ref=4i2o6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector inicializado. Regi√≥n cargada con 1 features\n",
      "Filtro nubes objetivo: < 100%\n",
      "Filtro nubes previas: < 50%\n",
      "D√≠as por bloque: 4\n",
      "üöÄ Iniciando procesamiento desde 2025-09 hasta 2025-09\n",
      "üìã Procesando en bloques de 4 d√≠as\n",
      "üîÑ Creando mosaicos para fechas objetivo e im√°genes previas\n",
      "üîÑ Usando 4 fechas previas para comparaci√≥n\n",
      "\n",
      "üîÑ BLOQUE 1: 2025-09-01 a 2025-09-04\n",
      "  üìÖ Procesando bloque: 2025-09-01 a 2025-09-04\n",
      "    Procesando mosaico objetivo del 2025-09-01 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-12: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-08: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-01 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-01 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 1 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-01 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-01 (mosaico 2 imgs) vs 2025-08-08 (1 imagen): 0 pol√≠gonos\n",
      "    Procesando mosaico objetivo del 2025-09-02 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-12: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-08: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-02 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 6 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-02 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 7 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-02 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 9 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-02 (mosaico 2 imgs) vs 2025-08-08 (1 imagen): 1 pol√≠gonos\n",
      "    Procesando mosaico objetivo del 2025-09-04 (3 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-12: 2 im√°genes ‚Üí mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-04 (mosaico 3 imgs) vs 2025-08-20 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-04 (mosaico 3 imgs) vs 2025-08-12 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "    Mosaico 2025-09-04 ya procesado, omitiendo 2025-09-04\n",
      "  ‚úÖ Bloque completado: 24 pol√≠gonos detectados\n",
      "  üì§ Exportaci√≥n iniciada: Bloque_20250901_a_20250904\n",
      "\n",
      "üîÑ BLOQUE 2: 2025-09-05 a 2025-09-08\n",
      "  üìÖ Procesando bloque: 2025-09-05 a 2025-09-08\n",
      "    Procesando mosaico objetivo del 2025-09-07 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-12: 2 im√°genes ‚Üí mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-07 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-07 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-07 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 1 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-07 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 1 pol√≠gonos\n",
      "    Mosaico 2025-09-07 ya procesado, omitiendo 2025-09-06\n",
      "    Mosaico 2025-09-07 ya procesado, omitiendo 2025-09-07\n",
      "    Procesando mosaico objetivo del 2025-09-09 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-12: 2 im√°genes ‚Üí mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 5 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 15 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 18 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 47 pol√≠gonos\n",
      "  ‚úÖ Bloque completado: 87 pol√≠gonos detectados\n",
      "  üì§ Exportaci√≥n iniciada: Bloque_20250905_a_20250908\n",
      "\n",
      "üîÑ BLOQUE 3: 2025-09-09 a 2025-09-12\n",
      "  üìÖ Procesando bloque: 2025-09-09 a 2025-09-12\n",
      "    Procesando mosaico objetivo del 2025-09-09 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-12: 2 im√°genes ‚Üí mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 5 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 15 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 18 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-09 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 47 pol√≠gonos\n",
      "    Procesando mosaico objetivo del 2025-09-11 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-09: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 im√°genes ‚Üí mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-11 (mosaico 2 imgs) vs 2025-09-09 (mosaico 2 imgs): 4 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-11 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-11 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 2 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-11 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 4 pol√≠gonos\n",
      "    Mosaico 2025-09-11 ya procesado, omitiendo 2025-09-11\n",
      "    Procesando mosaico objetivo del 2025-09-12 (2 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-11: 1 imagen\n",
      "        - 2025-09-09: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-12 (mosaico 2 imgs) vs 2025-09-11 (1 imagen): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-12 (mosaico 2 imgs) vs 2025-09-09 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-12 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-12 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 0 pol√≠gonos\n",
      "  ‚úÖ Bloque completado: 95 pol√≠gonos detectados\n",
      "  üì§ Exportaci√≥n iniciada: Bloque_20250909_a_20250912\n",
      "\n",
      "üîÑ BLOQUE 4: 2025-09-13 a 2025-09-16\n",
      "  üìÖ Procesando bloque: 2025-09-13 a 2025-09-16\n",
      "    Procesando mosaico objetivo del 2025-09-14 (4 im√°genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas √∫nicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-11: 1 imagen\n",
      "        - 2025-09-09: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-09-02: 2 im√°genes ‚Üí mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - Comparaci√≥n 2025-09-14 (mosaico 4 imgs) vs 2025-09-11 (1 imagen): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-14 (mosaico 4 imgs) vs 2025-09-09 (mosaico 2 imgs): 2 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-14 (mosaico 4 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 pol√≠gonos\n",
      "      - Comparaci√≥n 2025-09-14 (mosaico 4 imgs) vs 2025-08-22 (1 imagen): 0 pol√≠gonos\n",
      "    Mosaico 2025-09-14 ya procesado, omitiendo 2025-09-14\n",
      "  ‚úÖ Bloque completado: 2 pol√≠gonos detectados\n",
      "  üì§ Exportaci√≥n iniciada: Bloque_20250913_a_20250916\n",
      "\n",
      "üîÑ BLOQUE 5: 2025-09-17 a 2025-09-20\n",
      "  üìÖ Procesando bloque: 2025-09-17 a 2025-09-20\n",
      "  üìÑ No se encontraron √°reas quemadas en este bloque\n",
      "\n",
      "üîÑ BLOQUE 6: 2025-09-21 a 2025-09-24\n",
      "  üìÖ Procesando bloque: 2025-09-21 a 2025-09-24\n",
      "  üìÑ No se encontraron √°reas quemadas en este bloque\n",
      "\n",
      "üîÑ BLOQUE 7: 2025-09-25 a 2025-09-28\n",
      "  üìÖ Procesando bloque: 2025-09-25 a 2025-09-28\n",
      "  üìÑ No se encontraron √°reas quemadas en este bloque\n",
      "\n",
      "üîÑ BLOQUE 8: 2025-09-29 a 2025-09-30\n",
      "  üìÖ Procesando bloque: 2025-09-29 a 2025-09-30\n",
      "  üìÑ No se encontraron √°reas quemadas en este bloque\n",
      "\n",
      "üéâ ¬°Procesamiento completado! Se procesaron 8 bloques.\n",
      "üìã Revisa tu Google Drive para los archivos exportados.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='incendios-461918')\n",
    "\n",
    "\n",
    "class QuemadaDetector:\n",
    "    def __init__(self, region_asset_path, max_dias_busqueda=1, num_imagenes_previas=5, \n",
    "                 nubes_max_objetivo=100, nubes_max_previas=50, dias_por_bloque=10):\n",
    "        \"\"\"\n",
    "        Inicializar el detector de √°reas quemadas\n",
    "        \n",
    "        Args:\n",
    "            region_asset_path (str): Ruta del asset de la regi√≥n en GEE\n",
    "            max_dias_busqueda (int): M√°ximo de d√≠as para buscar imagen si no hay en fecha exacta\n",
    "            num_imagenes_previas (int): N√∫mero de im√°genes previas para comparar\n",
    "            nubes_max_objetivo (float): % m√°ximo de nubes para im√°genes objetivo\n",
    "            nubes_max_previas (float): % m√°ximo de nubes para im√°genes previas\n",
    "            dias_por_bloque (int): N√∫mero de d√≠as por bloque de procesamiento\n",
    "        \"\"\"\n",
    "        self.region = ee.FeatureCollection(region_asset_path)\n",
    "        self.max_dias_busqueda = max_dias_busqueda\n",
    "        self.num_imagenes_previas = num_imagenes_previas\n",
    "        self.nubes_max_objetivo = nubes_max_objetivo\n",
    "        self.nubes_max_previas = nubes_max_previas\n",
    "        self.dias_por_bloque = dias_por_bloque\n",
    "        \n",
    "        # Par√°metros de umbrales\n",
    "        self.umbral_diff_ndvi = -0.3\n",
    "        self.umbral_ndvi_prev = 0.5\n",
    "        self.umbral_bais2 = 6\n",
    "        self.umbral_mirbi = 0\n",
    "        \n",
    "        # Colecci√≥n base para im√°genes objetivo\n",
    "        self.coleccion_objetivo = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "            .filterBounds(self.region) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', self.nubes_max_objetivo)) \\\n",
    "            .map(self.mask_s2_clouds) \\\n",
    "            .map(lambda img: img.clip(self.region))\n",
    "        \n",
    "        # Colecci√≥n base para im√°genes previas\n",
    "        self.coleccion_previas = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "            .filterBounds(self.region) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', self.nubes_max_previas)) \\\n",
    "            .map(self.mask_s2_clouds) \\\n",
    "            .map(lambda img: img.clip(self.region))\n",
    "        \n",
    "        print(f\"Detector inicializado. Regi√≥n cargada con {self.region.size().getInfo()} features\")\n",
    "        print(f\"Filtro nubes objetivo: < {self.nubes_max_objetivo}%\")\n",
    "        print(f\"Filtro nubes previas: < {self.nubes_max_previas}%\")\n",
    "        print(f\"D√≠as por bloque: {self.dias_por_bloque}\")\n",
    "    \n",
    "    def mask_s2_clouds(self, image):\n",
    "        \"\"\"Funci√≥n mejorada para enmascarar nubes (QA60 + SCL)\"\"\"\n",
    "        # 1. M√°scara QA60 (nubes y cirros)\n",
    "        qa = image.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        mask_qa = qa.bitwiseAnd(cloud_bit_mask).eq(0) \\\n",
    "                   .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        \n",
    "        # 2. M√°scara SCL: nubes medias, altas, sombras y cirros delgados\n",
    "        scl = image.select('SCL')\n",
    "        mask_scl = scl.neq(3) \\\n",
    "                     .And(scl.neq(8)) \\\n",
    "                     .And(scl.neq(9)) \\\n",
    "                     .And(scl.neq(10))\n",
    "        \n",
    "        # Combinar m√°scaras\n",
    "        mask_final = mask_qa.And(mask_scl)\n",
    "        \n",
    "        return image.updateMask(mask_final)\n",
    "    \n",
    "    def calcular_ndvi(self, image):\n",
    "        \"\"\"Calcular NDVI usando las bandas B8 (NIR) y B4 (rojo)\"\"\"\n",
    "        return image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    \n",
    "    def calcular_bais2(self, image):\n",
    "        \"\"\"Calcular √≠ndice BAIS2 para detecci√≥n de √°reas quemadas\"\"\"\n",
    "        b4 = image.select('B4')\n",
    "        b6 = image.select('B6')\n",
    "        b7 = image.select('B7')\n",
    "        b8a = image.select('B8A')\n",
    "        b12 = image.select('B12')\n",
    "        \n",
    "        bais2 = image.expression(\n",
    "            '(1 - sqrt((RE2 * RE3 * N2) / R)) * (sqrt((S2 - N2) / (S2 + N2)) + 1)', {\n",
    "                'RE2': b6,\n",
    "                'RE3': b7,\n",
    "                'N2': b8a,\n",
    "                'R': b4,\n",
    "                'S2': b12\n",
    "            }\n",
    "        ).rename('BAIS2')\n",
    "        \n",
    "        return bais2\n",
    "    \n",
    "    def calcular_mirbi(self, image):\n",
    "        \"\"\"Calcular MIRBI para detectar √°reas quemadas\"\"\"\n",
    "        return image.expression(\n",
    "            '10 * (B12 - (B11 + B8A) / 2)', {\n",
    "                'B12': image.select('B12'),\n",
    "                'B11': image.select('B11'),\n",
    "                'B8A': image.select('B8A')\n",
    "            }\n",
    "        ).rename('MIRBI')\n",
    "    \n",
    "    def buscar_mosaico_en_fecha(self, fecha_objetivo, coleccion):\n",
    "        \"\"\"\n",
    "        Buscar y crear mosaico para una fecha espec√≠fica, avanzando d√≠a a d√≠a si es necesario\n",
    "        \n",
    "        Args:\n",
    "            fecha_objetivo (ee.Date): Fecha objetivo\n",
    "            coleccion (ee.ImageCollection): Colecci√≥n donde buscar\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (mosaico, fecha_real, num_imagenes, encontrada)\n",
    "        \"\"\"\n",
    "        current_date = fecha_objetivo\n",
    "        \n",
    "        for i in range(self.max_dias_busqueda):\n",
    "            start_date = current_date\n",
    "            end_date = current_date.advance(1, 'day')\n",
    "            \n",
    "            img_collection = coleccion.filterDate(start_date, end_date)\n",
    "            num_imagenes = img_collection.size().getInfo()\n",
    "            \n",
    "            # Verificar si hay im√°genes\n",
    "            if num_imagenes > 0:\n",
    "                fecha_real = current_date.format('YYYY-MM-dd')\n",
    "                \n",
    "                if num_imagenes == 1:\n",
    "                    # Si solo hay una imagen, usarla directamente\n",
    "                    mosaico = ee.Image(img_collection.first())\n",
    "                else:\n",
    "                    # Si hay m√∫ltiples im√°genes, crear mosaico\n",
    "                    mosaico = img_collection.mosaic()\n",
    "                \n",
    "                # Agregar metadatos al mosaico\n",
    "                mosaico = mosaico.set({\n",
    "                    'fecha_mosaico': fecha_real,\n",
    "                    'num_imagenes_mosaico': num_imagenes,\n",
    "                    'system:time_start': current_date.millis()\n",
    "                })\n",
    "                \n",
    "                return mosaico, fecha_real, num_imagenes, True\n",
    "            \n",
    "            current_date = current_date.advance(1, 'day')\n",
    "        \n",
    "        return None, None, 0, False\n",
    "    \n",
    "    def crear_mosaicos_por_fecha(self, coleccion, num_fechas):\n",
    "        \"\"\"\n",
    "        Crear mosaicos para las N fechas m√°s recientes, agrupando im√°genes por fecha\n",
    "        \n",
    "        Args:\n",
    "            coleccion (ee.ImageCollection): Colecci√≥n de im√°genes\n",
    "            num_fechas (int): N√∫mero de fechas √∫nicas a obtener\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de mosaicos √∫nicos por fecha\n",
    "        \"\"\"\n",
    "        # Obtener lista de fechas √∫nicas\n",
    "        def extract_date(image):\n",
    "            return ee.Feature(None, {\n",
    "                'date': ee.Date(image.get('system:time_start')).format('YYYY-MM-dd'),\n",
    "                'timestamp': image.get('system:time_start')\n",
    "            })\n",
    "        \n",
    "        # Extraer fechas de todas las im√°genes\n",
    "        fechas_collection = coleccion.map(extract_date)\n",
    "        fechas_distintas = fechas_collection.distinct(['date']).sort('timestamp', False)\n",
    "        \n",
    "        # Tomar solo las N fechas m√°s recientes\n",
    "        fechas_lista = fechas_distintas.limit(num_fechas).aggregate_array('date')\n",
    "        fechas_info = fechas_lista.getInfo()\n",
    "        \n",
    "        print(f\"      Fechas √∫nicas encontradas: {len(fechas_info)} de {num_fechas} solicitadas\")\n",
    "        \n",
    "        mosaicos = []\n",
    "        \n",
    "        for fecha_str in fechas_info:\n",
    "            # Filtrar im√°genes de esta fecha espec√≠fica\n",
    "            fecha_ee = ee.Date(fecha_str)\n",
    "            inicio_dia = fecha_ee\n",
    "            fin_dia = fecha_ee.advance(1, 'day')\n",
    "            \n",
    "            imagenes_fecha = coleccion.filterDate(inicio_dia, fin_dia)\n",
    "            num_imagenes = imagenes_fecha.size().getInfo()\n",
    "            \n",
    "            if num_imagenes > 0:\n",
    "                if num_imagenes == 1:\n",
    "                    # Si solo hay una imagen, usarla directamente\n",
    "                    mosaico = ee.Image(imagenes_fecha.first())\n",
    "                    print(f\"        - {fecha_str}: 1 imagen\")\n",
    "                else:\n",
    "                    # Si hay m√∫ltiples im√°genes, crear mosaico\n",
    "                    mosaico = imagenes_fecha.mosaic()\n",
    "                    print(f\"        - {fecha_str}: {num_imagenes} im√°genes ‚Üí mosaico\")\n",
    "                \n",
    "                # Agregar metadatos al mosaico\n",
    "                mosaico = mosaico.set({\n",
    "                    'fecha_mosaico': fecha_str,\n",
    "                    'num_imagenes_mosaico': num_imagenes,\n",
    "                    'system:time_start': fecha_ee.millis()\n",
    "                })\n",
    "                \n",
    "                mosaicos.append(mosaico)\n",
    "        \n",
    "        return mosaicos\n",
    "    \n",
    "    def procesar_bloque(self, fecha_inicio, fecha_fin):\n",
    "        \"\"\"\n",
    "        Procesar un bloque espec√≠fico de d√≠as\n",
    "        \n",
    "        Args:\n",
    "            fecha_inicio (datetime.date): Fecha de inicio del bloque\n",
    "            fecha_fin (datetime.date): Fecha de fin del bloque\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de vectores del bloque\n",
    "        \"\"\"\n",
    "        print(f\"  üìÖ Procesando bloque: {fecha_inicio} a {fecha_fin}\")\n",
    "        \n",
    "        vectores_bloque = []\n",
    "        imagenes_procesadas = set()\n",
    "        \n",
    "        # Procesar cada d√≠a del bloque\n",
    "        current_date = fecha_inicio\n",
    "        while current_date <= fecha_fin:\n",
    "            fecha_str = current_date.strftime('%Y-%m-%d')\n",
    "            fecha_ee = ee.Date(fecha_str)\n",
    "            \n",
    "            # Buscar y crear mosaico para la fecha objetivo\n",
    "            mosaico_objetivo, fecha_real_ee, num_imgs_objetivo, encontrada = self.buscar_mosaico_en_fecha(fecha_ee, self.coleccion_objetivo)\n",
    "            \n",
    "            if not encontrada:\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            # Obtener fecha real como string\n",
    "            fecha_real = fecha_real_ee.getInfo()\n",
    "            \n",
    "            # Evitar procesar la misma fecha varias veces\n",
    "            if fecha_real in imagenes_procesadas:\n",
    "                print(f\"    Mosaico {fecha_real} ya procesado, omitiendo {fecha_str}\")\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            imagenes_procesadas.add(fecha_real)\n",
    "            \n",
    "            # Mostrar informaci√≥n del mosaico objetivo\n",
    "            if num_imgs_objetivo == 1:\n",
    "                print(f\"    Procesando mosaico objetivo del {fecha_real} (1 imagen)\")\n",
    "            else:\n",
    "                print(f\"    Procesando mosaico objetivo del {fecha_real} ({num_imgs_objetivo} im√°genes)\")\n",
    "            \n",
    "            # Para el % de nubes, calculamos un promedio ponderado aproximado\n",
    "            # (En un mosaico esto es m√°s complejo, pero usamos el valor m√°ximo permitido como referencia)\n",
    "            nubes_objetivo = self.nubes_max_objetivo  # Valor representativo para mosaicos\n",
    "            \n",
    "            # Calcular √≠ndices para mosaico objetivo\n",
    "            ndvi_objetivo = self.calcular_ndvi(mosaico_objetivo)\n",
    "            bais_objetivo = self.calcular_bais2(mosaico_objetivo)\n",
    "            mirbi_objetivo = self.calcular_mirbi(mosaico_objetivo)\n",
    "            \n",
    "            # Obtener im√°genes previas y crear mosaicos por fecha\n",
    "            target_millis = ee.Number(mosaico_objetivo.get('system:time_start'))\n",
    "            imagenes_previas_collection = self.coleccion_previas \\\n",
    "                .filter(ee.Filter.lt('system:time_start', target_millis)) \\\n",
    "                .sort('system:time_start', False)\n",
    "            \n",
    "            # Verificar si hay im√°genes previas\n",
    "            if imagenes_previas_collection.size().getInfo() == 0:\n",
    "                print(f\"    No hay im√°genes previas para {fecha_real}\")\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            # Crear mosaicos por fecha\n",
    "            print(f\"    Creando mosaicos para {self.num_imagenes_previas} fechas previas...\")\n",
    "            mosaicos_previos = self.crear_mosaicos_por_fecha(imagenes_previas_collection, self.num_imagenes_previas)\n",
    "            \n",
    "            if len(mosaicos_previos) == 0:\n",
    "                print(f\"    No se pudieron crear mosaicos previos para {fecha_real}\")\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            print(f\"    Creados {len(mosaicos_previos)} mosaicos previos\")\n",
    "            \n",
    "            # Procesar cada mosaico previo\n",
    "            for mosaico_prev in mosaicos_previos:\n",
    "                fecha_prev = mosaico_prev.get('fecha_mosaico').getInfo()\n",
    "                num_imgs_mosaico = mosaico_prev.get('num_imagenes_mosaico').getInfo()\n",
    "                \n",
    "                # Para el % de nubes, usamos un valor representativo\n",
    "                nubes_prev = self.nubes_max_previas  # Valor por defecto para mosaicos\n",
    "                \n",
    "                # Calcular NDVI del mosaico previo\n",
    "                ndvi_prev = self.calcular_ndvi(mosaico_prev)\n",
    "                \n",
    "                # Calcular diferencia de NDVI\n",
    "                diff_ndvi = ndvi_objetivo.subtract(ndvi_prev).rename('Diferencia_NDVI')\n",
    "                \n",
    "                # Crear m√°scara inicial: zonas donde diff NDVI < umbral\n",
    "                zonas = diff_ndvi.lt(self.umbral_diff_ndvi).selfMask()\n",
    "                zonas_clip = zonas.clip(self.region)\n",
    "                \n",
    "                # Convertir a vectores para obtener geometr√≠a de √°reas de inter√©s\n",
    "                try:\n",
    "                    vectores_temp = zonas_clip.reduceToVectors(\n",
    "                        geometry=self.region,\n",
    "                        scale=12.5,\n",
    "                        geometryType='polygon',\n",
    "                        eightConnected=True,\n",
    "                        labelProperty='zone',\n",
    "                        maxPixels=1e8\n",
    "                    )\n",
    "                    \n",
    "                    if vectores_temp.size().getInfo() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Unir pol√≠gonos\n",
    "                    union_poligono = vectores_temp.geometry()\n",
    "                    if union_poligono.area(maxError=1).getInfo() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    poligono_fc = ee.FeatureCollection([ee.Feature(union_poligono)])\n",
    "                    \n",
    "                    # Aplicar m√°scara combinada\n",
    "                    combined_mask = bais_objetivo.lt(self.umbral_bais2) \\\n",
    "                                   .Or(mirbi_objetivo.gt(self.umbral_mirbi)) \\\n",
    "                                   .And(ndvi_prev.gt(self.umbral_ndvi_prev)) \\\n",
    "                                   .selfMask() \\\n",
    "                                   .rename('Combined_mask')\n",
    "                    \n",
    "                    # Recortar a √°rea de inter√©s\n",
    "                    image_clip_union = combined_mask.clip(poligono_fc)\n",
    "                    \n",
    "                    # Vectorizar resultado final\n",
    "                    vectores_final = image_clip_union.reduceToVectors(\n",
    "                        geometry=self.region,\n",
    "                        scale=12.5,\n",
    "                        geometryType='polygon',\n",
    "                        eightConnected=True,\n",
    "                        maxPixels=1e8\n",
    "                    )\n",
    "                    \n",
    "                    # Agregar metadatos\n",
    "                    vectores_final = vectores_final.map(lambda feature: \n",
    "                        feature.set({\n",
    "                            'fecha_objetivo': fecha_real,\n",
    "                            'fecha_previa': fecha_prev,\n",
    "                            'nubes_objetivo': nubes_objetivo,\n",
    "                            'nubes_previa': nubes_prev,\n",
    "                            'imagenes_objetivo': num_imgs_objetivo,\n",
    "                            'imagenes_mosaico': num_imgs_mosaico,\n",
    "                            'bloque_inicio': fecha_inicio.strftime('%Y-%m-%d'),\n",
    "                            'bloque_fin': fecha_fin.strftime('%Y-%m-%d')\n",
    "                        })\n",
    "                    )\n",
    "                    \n",
    "                    vectores_bloque.append(vectores_final)\n",
    "                    num_poligonos = vectores_final.size().getInfo()\n",
    "                    \n",
    "                    # Mostrar informaci√≥n detallada\n",
    "                    objetivo_info = f\"mosaico {num_imgs_objetivo} imgs\" if num_imgs_objetivo > 1 else \"1 imagen\"\n",
    "                    previa_info = f\"mosaico {num_imgs_mosaico} imgs\" if num_imgs_mosaico > 1 else \"1 imagen\"\n",
    "                    \n",
    "                    print(f\"      - Comparaci√≥n {fecha_real} ({objetivo_info}) vs {fecha_prev} ({previa_info}): {num_poligonos} pol√≠gonos\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error procesando comparaci√≥n {fecha_real} vs {fecha_prev}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "        return vectores_bloque\n",
    "    \n",
    "    def procesar_rango(self, start_year, start_month, end_year, end_month, export_folder='GEE_exports'):\n",
    "        \"\"\"\n",
    "        Procesar un rango de fechas dividi√©ndolo en bloques\n",
    "        \n",
    "        Args:\n",
    "            start_year (int): A√±o inicial\n",
    "            start_month (int): Mes inicial (1-12)\n",
    "            end_year (int): A√±o final\n",
    "            end_month (int): Mes final (1-12)\n",
    "            export_folder (str): Carpeta de exportaci√≥n en Google Drive\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ Iniciando procesamiento desde {start_year}-{start_month:02d} hasta {end_year}-{end_month:02d}\")\n",
    "        print(f\"üìã Procesando en bloques de {self.dias_por_bloque} d√≠as\")\n",
    "        print(f\"üîÑ Creando mosaicos para fechas objetivo e im√°genes previas\")\n",
    "        print(f\"üîÑ Usando {self.num_imagenes_previas} fechas previas para comparaci√≥n\")\n",
    "        \n",
    "        # Fechas de inicio y fin\n",
    "        fecha_inicio_total = datetime.date(start_year, start_month, 1)\n",
    "        if end_month == 12:\n",
    "            fecha_fin_total = datetime.date(end_year + 1, 1, 1) - datetime.timedelta(days=1)\n",
    "        else:\n",
    "            fecha_fin_total = datetime.date(end_year, end_month + 1, 1) - datetime.timedelta(days=1)\n",
    "        \n",
    "        # Procesar por bloques\n",
    "        current_start = fecha_inicio_total\n",
    "        bloque_num = 1\n",
    "        \n",
    "        while current_start <= fecha_fin_total:\n",
    "            # Calcular fin del bloque actual\n",
    "            current_end = min(\n",
    "                current_start + datetime.timedelta(days=self.dias_por_bloque - 1),\n",
    "                fecha_fin_total\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nüîÑ BLOQUE {bloque_num}: {current_start} a {current_end}\")\n",
    "            \n",
    "            # Procesar bloque\n",
    "            vectores_bloque = self.procesar_bloque(current_start, current_end)\n",
    "            \n",
    "            # Exportar resultados del bloque si hay datos\n",
    "            if vectores_bloque:\n",
    "                vectores_combinados = ee.FeatureCollection(vectores_bloque).flatten()\n",
    "                total_poligonos = vectores_combinados.size().getInfo()\n",
    "                \n",
    "                if total_poligonos > 0:\n",
    "                    print(f\"  ‚úÖ Bloque completado: {total_poligonos} pol√≠gonos detectados\")\n",
    "                    \n",
    "                    # Crear nombre de archivo descriptivo\n",
    "                    start_str = current_start.strftime('%Y%m%d')\n",
    "                    end_str = current_end.strftime('%Y%m%d')\n",
    "                    description = f'Bloque_{start_str}_a_{end_str}'\n",
    "                    \n",
    "                    try:\n",
    "                        task = ee.batch.Export.table.toDrive(\n",
    "                            collection=vectores_combinados,\n",
    "                            description=description,\n",
    "                            folder=export_folder,\n",
    "                            fileFormat='SHP'\n",
    "                        )\n",
    "                        \n",
    "                        task.start()\n",
    "                        print(f\"  üì§ Exportaci√≥n iniciada: {description}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Error en exportaci√≥n del bloque: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"  üìÑ Bloque sin pol√≠gonos para exportar\")\n",
    "            else:\n",
    "                print(f\"  üìÑ No se encontraron √°reas quemadas en este bloque\")\n",
    "            \n",
    "            # Avanzar al siguiente bloque\n",
    "            current_start = current_end + datetime.timedelta(days=1)\n",
    "            bloque_num += 1\n",
    "            \n",
    "            # Esperar un poco entre bloques\n",
    "            time.sleep(2)\n",
    "        \n",
    "        print(f\"\\nüéâ ¬°Procesamiento completado! Se procesaron {bloque_num-1} bloques.\")\n",
    "        print(\"üìã Revisa tu Google Drive para los archivos exportados.\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicializar detector\n",
    "    detector = QuemadaDetector(\n",
    "        region_asset_path='projects/ee-ezabaleta/assets/Zona',\n",
    "        max_dias_busqueda=3,\n",
    "        num_imagenes_previas=4,\n",
    "        nubes_max_objetivo=100,    # M√°ximo 100% de nubes para im√°genes objetivo\n",
    "        nubes_max_previas=50,     # M√°ximo 50% de nubes para im√°genes previas\n",
    "        dias_por_bloque=4     # Procesar en bloques de 5 d√≠as\n",
    "    )\n",
    "    \n",
    "    # Procesar rango de fechas\n",
    "    # Ejemplo: procesar desde enero 2025 hasta junio 2025\n",
    "    detector.procesar_rango(\n",
    "        start_year=2025,\n",
    "        start_month=9,\n",
    "        end_year=2025,\n",
    "        end_month=9,\n",
    "        export_folder='GEE_exports'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro areas quemadas por area y conversion a shp\n",
    "\n",
    "En este codigo se unen todos los bloques calculados en el script anterior, se filtran por area y se guarda en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando procesamiento de √°reas quemadas\n",
      "üìÇ Carpeta de entrada: G:\\Mi unidad\\GEE_exports\n",
      "üíæ Archivo de salida: G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\n",
      "------------------------------------------------------------\n",
      "üîÑ Iniciando merge de archivos SHP...\n",
      "üìÅ Encontrados 72 archivos SHP\n",
      "  üìÑ Procesando: Bloque_20200101_a_20200104.shp\n",
      "    ‚úÖ Agregado: 216 features\n",
      "  üìÑ Procesando: Bloque_20200105_a_20200108.shp\n",
      "    ‚úÖ Agregado: 165 features\n",
      "  üìÑ Procesando: Bloque_20200109_a_20200112.shp\n",
      "    ‚úÖ Agregado: 244 features\n",
      "  üìÑ Procesando: Bloque_20200113_a_20200116.shp\n",
      "    ‚úÖ Agregado: 718 features\n",
      "  üìÑ Procesando: Bloque_20200117_a_20200120.shp\n",
      "    ‚úÖ Agregado: 457 features\n",
      "  üìÑ Procesando: Bloque_20200125_a_20200128.shp\n",
      "    ‚úÖ Agregado: 180 features\n",
      "  üìÑ Procesando: Bloque_20200129_a_20200201.shp\n",
      "    ‚úÖ Agregado: 138 features\n",
      "  üìÑ Procesando: Bloque_20200202_a_20200205.shp\n",
      "    ‚úÖ Agregado: 91 features\n",
      "  üìÑ Procesando: Bloque_20200206_a_20200209.shp\n",
      "    ‚úÖ Agregado: 223 features\n",
      "  üìÑ Procesando: Bloque_20200210_a_20200213.shp\n",
      "    ‚úÖ Agregado: 165 features\n",
      "  üìÑ Procesando: Bloque_20200214_a_20200217.shp\n",
      "    ‚úÖ Agregado: 65 features\n",
      "  üìÑ Procesando: Bloque_20200218_a_20200221.shp\n",
      "    ‚úÖ Agregado: 114 features\n",
      "  üìÑ Procesando: Bloque_20200222_a_20200225.shp\n",
      "    ‚úÖ Agregado: 18 features\n",
      "  üìÑ Procesando: Bloque_20200321_a_20200324.shp\n",
      "    ‚úÖ Agregado: 95 features\n",
      "  üìÑ Procesando: Bloque_20200317_a_20200320.shp\n",
      "    ‚úÖ Agregado: 189 features\n",
      "  üìÑ Procesando: Bloque_20200313_a_20200316.shp\n",
      "    ‚úÖ Agregado: 127 features\n",
      "  üìÑ Procesando: Bloque_20200309_a_20200312.shp\n",
      "    ‚úÖ Agregado: 245 features\n",
      "  üìÑ Procesando: Bloque_20200305_a_20200308.shp\n",
      "    ‚úÖ Agregado: 315 features\n",
      "  üìÑ Procesando: Bloque_20200301_a_20200304.shp\n",
      "    ‚úÖ Agregado: 1 features\n",
      "  üìÑ Procesando: Bloque_20200226_a_20200229.shp\n",
      "    ‚úÖ Agregado: 136 features\n",
      "  üìÑ Procesando: Bloque_20200325_a_20200328.shp\n",
      "    ‚úÖ Agregado: 177 features\n",
      "  üìÑ Procesando: Bloque_20200329_a_20200401.shp\n",
      "    ‚úÖ Agregado: 3 features\n",
      "  üìÑ Procesando: Bloque_20200402_a_20200405.shp\n",
      "    ‚úÖ Agregado: 154 features\n",
      "  üìÑ Procesando: Bloque_20200414_a_20200417.shp\n",
      "    ‚úÖ Agregado: 61 features\n",
      "  üìÑ Procesando: Bloque_20200422_a_20200425.shp\n",
      "    ‚úÖ Agregado: 6 features\n",
      "  üìÑ Procesando: Bloque_20200426_a_20200429.shp\n",
      "    ‚úÖ Agregado: 7 features\n",
      "  üìÑ Procesando: Bloque_20200430_a_20200503.shp\n",
      "    ‚úÖ Agregado: 1 features\n",
      "  üìÑ Procesando: Bloque_20200601_a_20200604.shp\n",
      "    ‚úÖ Agregado: 6 features\n",
      "  üìÑ Procesando: Bloque_20200524_a_20200527.shp\n",
      "    ‚úÖ Agregado: 17 features\n",
      "  üìÑ Procesando: Bloque_20200520_a_20200523.shp\n",
      "    ‚úÖ Agregado: 191 features\n",
      "  üìÑ Procesando: Bloque_20200516_a_20200519.shp\n",
      "    ‚úÖ Agregado: 50 features\n",
      "  üìÑ Procesando: Bloque_20200512_a_20200515.shp\n",
      "    ‚úÖ Agregado: 3 features\n",
      "  üìÑ Procesando: Bloque_20200605_a_20200608.shp\n",
      "    ‚úÖ Agregado: 105 features\n",
      "  üìÑ Procesando: Bloque_20200621_a_20200624.shp\n",
      "    ‚úÖ Agregado: 62 features\n",
      "  üìÑ Procesando: Bloque_20200703_a_20200706.shp\n",
      "    ‚úÖ Agregado: 10 features\n",
      "  üìÑ Procesando: Bloque_20200705_a_20200708.shp\n",
      "    ‚úÖ Agregado: 10 features\n",
      "  üìÑ Procesando: Bloque_20200713_a_20200716.shp\n",
      "    ‚úÖ Agregado: 136 features\n",
      "  üìÑ Procesando: Bloque_20200717_a_20200720.shp\n",
      "    ‚úÖ Agregado: 136 features\n",
      "  üìÑ Procesando: Bloque_20200814_a_20200817.shp\n",
      "    ‚úÖ Agregado: 19 features\n",
      "  üìÑ Procesando: Bloque_20200810_a_20200813.shp\n",
      "    ‚úÖ Agregado: 221 features\n",
      "  üìÑ Procesando: Bloque_20200802_a_20200805.shp\n",
      "    ‚úÖ Agregado: 113 features\n",
      "  üìÑ Procesando: Bloque_20200729_a_20200801.shp\n",
      "    ‚úÖ Agregado: 47 features\n",
      "  üìÑ Procesando: Bloque_20200725_a_20200728.shp\n",
      "    ‚úÖ Agregado: 102 features\n",
      "  üìÑ Procesando: Bloque_20200721_a_20200724.shp\n",
      "    ‚úÖ Agregado: 49 features\n",
      "  üìÑ Procesando: Bloque_20200826_a_20200829.shp\n",
      "    ‚úÖ Agregado: 39 features\n",
      "  üìÑ Procesando: Bloque_20200830_a_20200902.shp\n",
      "    ‚úÖ Agregado: 176 features\n",
      "  üìÑ Procesando: Bloque_20200903_a_20200906.shp\n",
      "    ‚úÖ Agregado: 124 features\n",
      "  üìÑ Procesando: Bloque_20200907_a_20200910.shp\n",
      "    ‚úÖ Agregado: 184 features\n",
      "  üìÑ Procesando: Bloque_20200911_a_20200914.shp\n",
      "    ‚úÖ Agregado: 16 features\n",
      "  üìÑ Procesando: Bloque_20200915_a_20200918.shp\n",
      "    ‚úÖ Agregado: 49 features\n",
      "  üìÑ Procesando: Bloque_20200919_a_20200922.shp\n",
      "    ‚úÖ Agregado: 9 features\n",
      "  üìÑ Procesando: Bloque_20200923_a_20200926.shp\n",
      "    ‚úÖ Agregado: 300 features\n",
      "  üìÑ Procesando: Bloque_20200927_a_20200930.shp\n",
      "    ‚úÖ Agregado: 60 features\n",
      "  üìÑ Procesando: Bloque_20201001_a_20201004.shp\n",
      "    ‚úÖ Agregado: 31 features\n",
      "  üìÑ Procesando: Bloque_20201009_a_20201012.shp\n",
      "    ‚úÖ Agregado: 634 features\n",
      "  üìÑ Procesando: Bloque_20201013_a_20201016.shp\n",
      "    ‚úÖ Agregado: 294 features\n",
      "  üìÑ Procesando: Bloque_20201017_a_20201020.shp\n",
      "    ‚úÖ Agregado: 161 features\n",
      "  üìÑ Procesando: Bloque_20201021_a_20201024.shp\n",
      "    ‚úÖ Agregado: 43 features\n",
      "  üìÑ Procesando: Bloque_20201102_a_20201105.shp\n",
      "    ‚úÖ Agregado: 14 features\n",
      "  üìÑ Procesando: Bloque_20201106_a_20201109.shp\n",
      "    ‚úÖ Agregado: 13 features\n",
      "  üìÑ Procesando: Bloque_20201110_a_20201113.shp\n",
      "    ‚úÖ Agregado: 27 features\n",
      "  üìÑ Procesando: Bloque_20201114_a_20201117.shp\n",
      "    ‚úÖ Agregado: 26 features\n",
      "  üìÑ Procesando: Bloque_20201212_a_20201215.shp\n",
      "    ‚úÖ Agregado: 109 features\n",
      "  üìÑ Procesando: Bloque_20201208_a_20201211.shp\n",
      "    ‚úÖ Agregado: 14 features\n",
      "  üìÑ Procesando: Bloque_20201204_a_20201207.shp\n",
      "    ‚úÖ Agregado: 37 features\n",
      "  üìÑ Procesando: Bloque_20201130_a_20201203.shp\n",
      "    ‚úÖ Agregado: 90 features\n",
      "  üìÑ Procesando: Bloque_20201126_a_20201129.shp\n",
      "    ‚úÖ Agregado: 54 features\n",
      "  üìÑ Procesando: Bloque_20201122_a_20201125.shp\n",
      "    ‚úÖ Agregado: 98 features\n",
      "  üìÑ Procesando: Bloque_20201216_a_20201219.shp\n",
      "    ‚úÖ Agregado: 46 features\n",
      "  üìÑ Procesando: Bloque_20201220_a_20201223.shp\n",
      "    ‚úÖ Agregado: 279 features\n",
      "  üìÑ Procesando: Bloque_20201224_a_20201227.shp\n",
      "    ‚úÖ Agregado: 59 features\n",
      "  üìÑ Procesando: Bloque_20201228_a_20201231.shp\n",
      "    ‚úÖ Agregado: 20 features\n",
      "üîÑ Combinando GeoDataFrames...\n",
      "‚úÖ Merge completado: 8594 features totales\n",
      "\n",
      "üìä Estad√≠sticas iniciales:\n",
      "  - Features totales: 8594\n",
      "  - CRS: EPSG:4326\n",
      "\n",
      "üîÑ Paso 1: Calculando √°rea y filtrando pol√≠gonos < 0.1 ha...\n",
      "üîÑ Calculando √°rea en hect√°reas...\n",
      "  üìä √Årea total inicial: 937.83 ha\n",
      "  üìä Pol√≠gonos < 0.1 ha: 7108\n",
      "  ‚úÖ Pol√≠gonos despu√©s del filtro: 1486\n",
      "  üìä √Årea total despu√©s del filtro: 709.85 ha\n",
      "\n",
      "üîÑ Paso 2: Dissolve por fecha_obje...\n",
      "  üìä Fechas √∫nicas: 70\n",
      "  ‚úÖ Pol√≠gonos despu√©s del dissolve: 1057\n",
      "\n",
      "üîÑ Paso 3: Recalculando √°rea y filtrando < 0.1 ha...\n",
      "üîÑ Calculando √°rea en hect√°reas...\n",
      "  üìä √Årea total antes del segundo filtro: 373.68 ha\n",
      "  üìä Pol√≠gonos < 0.1 ha: 303\n",
      "  ‚úÖ Pol√≠gonos despu√©s del segundo filtro: 754\n",
      "  üìä √Årea total despu√©s del segundo filtro: 360.05 ha\n",
      "\n",
      "üîÑ Paso 4: Aplicando buffer de 0.1 metros...\n",
      "\n",
      "üîÑ Paso 5: Recalculando √°rea final...\n",
      "üîÑ Calculando √°rea en hect√°reas...\n",
      "  ‚úÖ √Årea total final: 362.88 ha\n",
      "  üìä Pol√≠gonos finales: 754\n",
      "\n",
      "üíæ Guardando resultado en: G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\n",
      "‚úÖ Procesamiento completado exitosamente!\n",
      "\n",
      "üìä RESUMEN FINAL:\n",
      "  - Pol√≠gonos procesados: 754\n",
      "  - √Årea total: 362.88 ha\n",
      "  - Fechas √∫nicas: 69\n",
      "  - Rango de fechas: 2020-01-02 a 2020-12-29\n",
      "  - √Årea promedio por pol√≠gono: 0.48 ha\n",
      "  - √Årea m√°xima: 7.04 ha\n",
      "  - √Årea m√≠nima: 0.11 ha\n",
      "\n",
      "üéâ ¬°Procesamiento completado exitosamente!\n",
      "üìÅ Archivo guardado en: G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd \n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def merge_shp_files(input_folder):\n",
    "    \"\"\"\n",
    "    Funci√≥n para hacer merge de todos los archivos SHP en una carpeta\n",
    "    manteniendo solo los campos fecha_obje y fecha_prev\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Iniciando merge de archivos SHP...\")\n",
    "    \n",
    "    # Buscar todos los archivos .shp en la carpeta\n",
    "    shp_files = list(Path(input_folder).glob(\"*.shp\"))\n",
    "    \n",
    "    if not shp_files:\n",
    "        print(\"‚ùå No se encontraron archivos SHP en la carpeta especificada\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ Encontrados {len(shp_files)} archivos SHP\")\n",
    "    \n",
    "    # Lista para almacenar los GeoDataFrames\n",
    "    gdfs = []\n",
    "    \n",
    "    for shp_file in shp_files:\n",
    "        try:\n",
    "            print(f\"  üìÑ Procesando: {shp_file.name}\")\n",
    "            gdf = gpd.read_file(shp_file)\n",
    "            \n",
    "            # Verificar si las columnas existen\n",
    "            if 'fecha_obje' in gdf.columns and 'fecha_prev' in gdf.columns:\n",
    "                # Mantener solo las columnas necesarias y la geometr√≠a\n",
    "                gdf_filtered = gdf[['fecha_obje', 'fecha_prev', 'geometry']].copy()\n",
    "                gdfs.append(gdf_filtered)\n",
    "                print(f\"    ‚úÖ Agregado: {len(gdf_filtered)} features\")\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è  Columnas faltantes en {shp_file.name}\")\n",
    "                print(f\"    Columnas disponibles: {list(gdf.columns)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error procesando {shp_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not gdfs:\n",
    "        print(\"‚ùå No se pudieron procesar archivos SHP v√°lidos\")\n",
    "        return None\n",
    "    \n",
    "    # Combinar todos los GeoDataFrames\n",
    "    print(\"üîÑ Combinando GeoDataFrames...\")\n",
    "    merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "    \n",
    "    # Asegurar que el CRS sea consistente\n",
    "    if merged_gdf.crs is None:\n",
    "        print(\"‚ö†Ô∏è  CRS no definido, asignando EPSG:4326\")\n",
    "        merged_gdf = merged_gdf.set_crs('EPSG:4326')\n",
    "    \n",
    "    print(f\"‚úÖ Merge completado: {len(merged_gdf)} features totales\")\n",
    "    return merged_gdf\n",
    "\n",
    "def calculate_area_hectares(gdf):\n",
    "    \"\"\"\n",
    "    Calcular √°rea en hect√°reas\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Calculando √°rea en hect√°reas...\")\n",
    "    \n",
    "    # Si est√° en coordenadas geogr√°ficas, proyectar a un sistema m√©trico\n",
    "    if gdf.crs.is_geographic:\n",
    "        # Proyectar a Web Mercator para c√°lculo de √°rea\n",
    "        gdf_proj = gdf.to_crs('EPSG:3857')\n",
    "        area_m2 = gdf_proj.geometry.area\n",
    "    else:\n",
    "        area_m2 = gdf.geometry.area\n",
    "    \n",
    "    # Convertir a hect√°reas (1 ha = 10,000 m¬≤)\n",
    "    area_ha = area_m2 / 10000\n",
    "    \n",
    "    return area_ha\n",
    "\n",
    "def process_burned_areas(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para procesar las √°reas quemadas siguiendo el workflow especificado\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Iniciando procesamiento de √°reas quemadas\")\n",
    "    print(f\"üìÇ Carpeta de entrada: {input_folder}\")\n",
    "    print(f\"üíæ Archivo de salida: {output_file}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Paso 1: Merge de archivos SHP\n",
    "    merged_gdf = merge_shp_files(input_folder)\n",
    "    if merged_gdf is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas iniciales:\")\n",
    "    print(f\"  - Features totales: {len(merged_gdf)}\")\n",
    "    print(f\"  - CRS: {merged_gdf.crs}\")\n",
    "    \n",
    "    # Paso 2: Crear campo de √°rea y eliminar pol√≠gonos < 0.1 ha\n",
    "    print(\"\\nüîÑ Paso 1: Calculando √°rea y filtrando pol√≠gonos < 0.1 ha...\")\n",
    "    merged_gdf['area_ha'] = calculate_area_hectares(merged_gdf)\n",
    "    \n",
    "    # Mostrar estad√≠sticas antes del filtro\n",
    "    print(f\"  üìä √Årea total inicial: {merged_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  üìä Pol√≠gonos < 0.1 ha: {len(merged_gdf[merged_gdf['area_ha'] < 0.1])}\")\n",
    "    \n",
    "    # Filtrar pol√≠gonos >= 0.1 ha\n",
    "    filtered_gdf = merged_gdf[merged_gdf['area_ha'] >= 0.1].copy()\n",
    "    print(f\"  ‚úÖ Pol√≠gonos despu√©s del filtro: {len(filtered_gdf)}\")\n",
    "    print(f\"  üìä √Årea total despu√©s del filtro: {filtered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    \n",
    "    # Paso 3: Dissolve por fecha_obje sin crear multiparts\n",
    "    print(\"\\nüîÑ Paso 2: Dissolve por fecha_obje...\")\n",
    "    print(f\"  üìä Fechas √∫nicas: {filtered_gdf['fecha_obje'].nunique()}\")\n",
    "    \n",
    "    # Realizar dissolve por fecha_obje\n",
    "    dissolved_gdf = filtered_gdf.dissolve(by='fecha_obje', as_index=False)\n",
    "    \n",
    "    # Explotar multiparts si existen\n",
    "    dissolved_gdf = dissolved_gdf.explode(index_parts=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  ‚úÖ Pol√≠gonos despu√©s del dissolve: {len(dissolved_gdf)}\")\n",
    "    \n",
    "    # Paso 4: Recalcular √°rea y eliminar pol√≠gonos < 0.1 ha otra vez\n",
    "    print(\"\\nüîÑ Paso 3: Recalculando √°rea y filtrando < 0.1 ha...\")\n",
    "    dissolved_gdf['area_ha'] = calculate_area_hectares(dissolved_gdf)\n",
    "    \n",
    "    # Mostrar estad√≠sticas antes del segundo filtro\n",
    "    print(f\"  üìä √Årea total antes del segundo filtro: {dissolved_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  üìä Pol√≠gonos < 0.1 ha: {len(dissolved_gdf[dissolved_gdf['area_ha'] < 0.1])}\")\n",
    "    \n",
    "    # Segundo filtro\n",
    "    final_filtered_gdf = dissolved_gdf[dissolved_gdf['area_ha'] >= 0.1].copy()\n",
    "    print(f\"  ‚úÖ Pol√≠gonos despu√©s del segundo filtro: {len(final_filtered_gdf)}\")\n",
    "    print(f\"  üìä √Årea total despu√©s del segundo filtro: {final_filtered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    \n",
    "    # Paso 5: Aplicar buffer de 0.1 metros\n",
    "    print(\"\\nüîÑ Paso 4: Aplicando buffer de 0.1 metros...\")\n",
    "    \n",
    "    # Si est√° en coordenadas geogr√°ficas, proyectar temporalmente\n",
    "    if final_filtered_gdf.crs.is_geographic:\n",
    "        # Proyectar a Web Mercator para el buffer\n",
    "        buffered_gdf = final_filtered_gdf.to_crs('EPSG:3857')\n",
    "        buffered_gdf['geometry'] = buffered_gdf.geometry.buffer(0.1)\n",
    "        # Volver al CRS original\n",
    "        buffered_gdf = buffered_gdf.to_crs(final_filtered_gdf.crs)\n",
    "    else:\n",
    "        buffered_gdf = final_filtered_gdf.copy()\n",
    "        buffered_gdf['geometry'] = buffered_gdf.geometry.buffer(0.1)\n",
    "    \n",
    "    # Paso 6: Recalcular √°rea final\n",
    "    print(\"\\nüîÑ Paso 5: Recalculando √°rea final...\")\n",
    "    buffered_gdf['area_ha'] = calculate_area_hectares(buffered_gdf)\n",
    "    \n",
    "    print(f\"  ‚úÖ √Årea total final: {buffered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  üìä Pol√≠gonos finales: {len(buffered_gdf)}\")\n",
    "    \n",
    "    # Paso 7: Guardar resultado\n",
    "    print(f\"\\nüíæ Guardando resultado en: {output_file}\")\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Guardar archivo\n",
    "    buffered_gdf.to_file(output_file, driver='ESRI Shapefile')\n",
    "    \n",
    "    print(\"‚úÖ Procesamiento completado exitosamente!\")\n",
    "    \n",
    "    # Mostrar resumen final\n",
    "    print(\"\\nüìä RESUMEN FINAL:\")\n",
    "    print(f\"  - Pol√≠gonos procesados: {len(buffered_gdf)}\")\n",
    "    print(f\"  - √Årea total: {buffered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  - Fechas √∫nicas: {buffered_gdf['fecha_obje'].nunique()}\")\n",
    "    print(f\"  - Rango de fechas: {buffered_gdf['fecha_obje'].min()} a {buffered_gdf['fecha_obje'].max()}\")\n",
    "    print(f\"  - √Årea promedio por pol√≠gono: {buffered_gdf['area_ha'].mean():.2f} ha\")\n",
    "    print(f\"  - √Årea m√°xima: {buffered_gdf['area_ha'].max():.2f} ha\")\n",
    "    print(f\"  - √Årea m√≠nima: {buffered_gdf['area_ha'].min():.2f} ha\")\n",
    "    \n",
    "    return buffered_gdf\n",
    "\n",
    "# Configuraci√≥n y ejecuci√≥n\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurar rutas\n",
    "    input_folder = r\"G:\\Mi unidad\\GEE_exports\"\n",
    "    output_file = r\"G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\"\n",
    "    \n",
    "    # Verificar que la carpeta existe\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"‚ùå La carpeta {input_folder} no existe\")\n",
    "        print(\"Por favor, verifica la ruta y vuelve a intentar\")\n",
    "    else:\n",
    "        # Ejecutar procesamiento\n",
    "        resultado = process_burned_areas(input_folder, output_file)\n",
    "        \n",
    "        if resultado is not None:\n",
    "            print(f\"\\nüéâ ¬°Procesamiento completado exitosamente!\")\n",
    "            print(f\"üìÅ Archivo guardado en: {output_file}\")\n",
    "        else:\n",
    "            print(\"‚ùå El procesamiento fall√≥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion areas quemadas diarias\n",
    "\n",
    "En este codigo se utiliza la logica de bandas en imagenes satelitales para definir poligonos que representen areas quemadas para el dia actual, en caso que no se encuentren imagenes disponibles para el dia se omite el proceso. Tambien se actualiza la capa de incendios detectados en ArcGis Web. El script funciona con autenticacion de google earth engine asi que se tiene que crear un usuario y projecto con acceso a la API de google earth engine en google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arcpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01marcpy\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Inicializar Earth Engine\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'arcpy'"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "import arcpy\n",
    "import requests\n",
    "\n",
    "# Inicializar Earth Engine\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='incendios-461918')\n",
    "\n",
    "def mask_s2_clouds(image):\n",
    "    qa = image.select('QA60')\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    mask_qa = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    scl = image.select('SCL')\n",
    "    mask_scl = (scl.neq(3).And(scl.neq(8)).And(scl.neq(9)).And(scl.neq(10)))\n",
    "    mask_final = mask_qa.And(mask_scl)\n",
    "    return image.updateMask(mask_final)\n",
    "\n",
    "def calcular_ndvi(image):\n",
    "    return image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "\n",
    "def calcular_bais2(image):\n",
    "    b4 = image.select('B4')\n",
    "    b6 = image.select('B6')\n",
    "    b7 = image.select('B7')\n",
    "    b8a = image.select('B8A')\n",
    "    b12 = image.select('B12')\n",
    "    bais2 = image.expression('(1 - sqrt((RE2 * RE3 * N2) / R)) * (sqrt((S2 - N2) / (S2 + N2)) + 1)', {\n",
    "        'RE2': b6, 'RE3': b7, 'N2': b8a, 'R': b4, 'S2': b12\n",
    "    }).rename('BAIS2')\n",
    "    return bais2\n",
    "\n",
    "def calcular_mirbi(image):\n",
    "    return image.expression('10 * (B12 - (B11 + B8A) / 2)', {\n",
    "        'B12': image.select('B12'), 'B11': image.select('B11'), 'B8A': image.select('B8A')\n",
    "    }).rename('MIRBI')\n",
    "\n",
    "def crear_mosaico_por_fecha(coleccion, fecha_inicio, fecha_fin, region):\n",
    "    \"\"\"\n",
    "    Crea un mosaico de todas las im√°genes disponibles en una fecha espec√≠fica\n",
    "    \n",
    "    Args:\n",
    "        coleccion: ImageCollection de Sentinel-2\n",
    "        fecha_inicio: ee.Date de inicio del d√≠a\n",
    "        fecha_fin: ee.Date de fin del d√≠a\n",
    "        region: Geometr√≠a de la regi√≥n de inter√©s\n",
    "    \n",
    "    Returns:\n",
    "        ee.Image: Mosaico de las im√°genes del d√≠a o None si no hay im√°genes\n",
    "    \"\"\"\n",
    "    # Filtrar im√°genes por fecha\n",
    "    imagenes_fecha = (coleccion\n",
    "                     .filterDate(fecha_inicio, fecha_fin)\n",
    "                     .filterBounds(region))\n",
    "    \n",
    "    # Verificar si hay im√°genes\n",
    "    num_imagenes = imagenes_fecha.size()\n",
    "    \n",
    "    try:\n",
    "        num_imagenes_info = num_imagenes.getInfo()\n",
    "        if num_imagenes_info == 0:\n",
    "            return None, 0\n",
    "    except:\n",
    "        return None, 0\n",
    "    \n",
    "    # Crear mosaico (las im√°genes m√°s recientes tendr√°n prioridad)\n",
    "    mosaico = (imagenes_fecha\n",
    "               .sort('system:time_start', False)  # Ordenar por fecha descendente\n",
    "               .mosaic()\n",
    "               .clip(region))\n",
    "    \n",
    "    # Agregar metadatos del mosaico\n",
    "    fecha_str = fecha_inicio.format('YYYY-MM-dd').getInfo()\n",
    "    mosaico = mosaico.set({\n",
    "        'system:time_start': fecha_inicio.millis(),\n",
    "        'fecha_mosaico': fecha_str,\n",
    "        'num_imagenes_mosaico': num_imagenes_info\n",
    "    })\n",
    "    \n",
    "    return mosaico, num_imagenes_info\n",
    "\n",
    "def obtener_fechas_unicas_previas(coleccion, fecha_limite, num_fechas_max=10):\n",
    "    \"\"\"\n",
    "    Obtiene las fechas √∫nicas disponibles en la colecci√≥n antes de una fecha l√≠mite\n",
    "    \n",
    "    Args:\n",
    "        coleccion: ImageCollection\n",
    "        fecha_limite: ee.Date l√≠mite (no incluida)\n",
    "        num_fechas_max: N√∫mero m√°ximo de fechas a retornar\n",
    "    \n",
    "    Returns:\n",
    "        List: Lista de fechas √∫nicas en formato ee.Date\n",
    "    \"\"\"\n",
    "    # Filtrar im√°genes antes de la fecha l√≠mite\n",
    "    imagenes_previas = (coleccion\n",
    "                       .filter(ee.Filter.lt('system:time_start', fecha_limite.millis()))\n",
    "                       .sort('system:time_start', False))\n",
    "    \n",
    "    # Funci√≥n para extraer fecha (YYYY-MM-DD) de una imagen\n",
    "    def extraer_fecha(image):\n",
    "        fecha = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n",
    "        return image.set('fecha_str', fecha)\n",
    "    \n",
    "    # Agregar campo de fecha string\n",
    "    imagenes_con_fecha = imagenes_previas.map(extraer_fecha)\n",
    "    \n",
    "    # Obtener fechas √∫nicas usando distinct\n",
    "    fechas_unicas = imagenes_con_fecha.distinct('fecha_str').limit(num_fechas_max)\n",
    "    \n",
    "    # Convertir a lista para poder iterar\n",
    "    lista_fechas = fechas_unicas.toList(num_fechas_max)\n",
    "    \n",
    "    # Extraer las fechas como ee.Date\n",
    "    fechas_ee = []\n",
    "    try:\n",
    "        num_fechas = fechas_unicas.size().getInfo()\n",
    "        print(f\"üìÖ Encontradas {num_fechas} fechas √∫nicas para procesar\")\n",
    "        \n",
    "        for i in range(num_fechas):\n",
    "            imagen = ee.Image(lista_fechas.get(i))\n",
    "            fecha_ee = ee.Date(imagen.get('system:time_start'))\n",
    "            # Truncar a inicio del d√≠a\n",
    "            fecha_inicio_dia = fecha_ee.update(None, None, None, 0, 0, 0)\n",
    "            fechas_ee.append(fecha_inicio_dia)\n",
    "            \n",
    "            # Mostrar fecha para debug\n",
    "            fecha_str = fecha_ee.format('YYYY-MM-dd').getInfo()\n",
    "            print(f\"   üìÜ Fecha {i+1}: {fecha_str}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al obtener fechas √∫nicas: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return fechas_ee\n",
    "\n",
    "# [Aqu√≠ van todas las funciones auxiliares sin cambios]\n",
    "def obtener_url_feature_service(item_id, usuario, clave):\n",
    "    \"\"\"\n",
    "    Obtiene la URL correcta del feature service desde ArcGIS Online\n",
    "    \"\"\"\n",
    "    portal_url = \"https://www.arcgis.com\"\n",
    "    \n",
    "    # URL para obtener informaci√≥n del item\n",
    "    item_info_url = f\"{portal_url}/sharing/rest/content/items/{item_id}\"\n",
    "    \n",
    "    # Par√°metros para la solicitud\n",
    "    params = {\n",
    "        'f': 'json',\n",
    "        'token': None  # Se obtendr√° el token despu√©s\n",
    "    }\n",
    "    \n",
    "    # Obtener token de autenticaci√≥n\n",
    "    token_url = f\"{portal_url}/sharing/rest/generateToken\"\n",
    "    token_params = {\n",
    "        'username': usuario,\n",
    "        'password': clave,\n",
    "        'referer': portal_url,\n",
    "        'f': 'json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Solicitar token\n",
    "        token_response = requests.post(token_url, data=token_params)\n",
    "        token_data = token_response.json()\n",
    "        \n",
    "        if 'token' not in token_data:\n",
    "            raise Exception(f\"Error al obtener token: {token_data}\")\n",
    "        \n",
    "        token = token_data['token']\n",
    "        params['token'] = token\n",
    "        \n",
    "        # Obtener informaci√≥n del item\n",
    "        item_response = requests.get(item_info_url, params=params)\n",
    "        item_data = item_response.json()\n",
    "        \n",
    "        if 'url' not in item_data:\n",
    "            raise Exception(f\"No se encontr√≥ URL en el item: {item_data}\")\n",
    "        \n",
    "        # Construir URL del feature service\n",
    "        base_url = item_data['url']\n",
    "        if not base_url.endswith('/'):\n",
    "            base_url += '/'\n",
    "        \n",
    "        feature_service_url = f\"{base_url}0\"  # Capa 0\n",
    "        \n",
    "        return feature_service_url, token\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error al obtener URL del feature service: {e}\")\n",
    "\n",
    "def limpiar_archivos_temporales(output_folder_local, nombre_shp):\n",
    "    \"\"\"\n",
    "    Elimina los archivos temporales generados durante el procesamiento\n",
    "    \"\"\"\n",
    "    print(\"üßπ Limpiando archivos temporales...\")\n",
    "    \n",
    "    # Lista de archivos a eliminar\n",
    "    archivos_a_eliminar = [\n",
    "        nombre_shp,  # Archivo original\n",
    "        nombre_shp.replace('.shp', '_filtrado.shp'),\n",
    "        nombre_shp.replace('.shp', '_procesado.shp'),\n",
    "        nombre_shp.replace('.shp', '_filtrado_2.shp'),\n",
    "        nombre_shp.replace('.shp', '_buffered.shp')\n",
    "    ]\n",
    "    \n",
    "    for archivo_shp in archivos_a_eliminar:\n",
    "        try:\n",
    "            archivo_path = os.path.join(output_folder_local, archivo_shp)\n",
    "            \n",
    "            # Eliminar todos los archivos relacionados con el shapefile\n",
    "            extensiones = ['.shp', '.shx', '.dbf', '.prj', '.cpg', '.xml']\n",
    "            \n",
    "            for ext in extensiones:\n",
    "                archivo_completo = archivo_path.replace('.shp', ext)\n",
    "                if os.path.exists(archivo_completo):\n",
    "                    os.remove(archivo_completo)\n",
    "                    print(f\"   üóëÔ∏è Eliminado: {os.path.basename(archivo_completo)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è No se pudo eliminar {archivo_shp}: {e}\")\n",
    "    \n",
    "    print(\"‚úÖ Limpieza de archivos temporales completada\")\n",
    "\n",
    "def agregar_a_arcgis_web(shp_buffered, output_folder_local, nombre_shp):\n",
    "    \"\"\"\n",
    "    Funci√≥n para agregar shapefile a ArcGIS Online con pausas para estabilidad\n",
    "    \"\"\"\n",
    "    portal_url = \"https://www.arcgis.com/\"\n",
    "    usuario = \"unidad.gestion.riesgo\"\n",
    "    clave = \"XXXXXXX\"\n",
    "    item_id = \"7b561988344046d4827141dcf8014b25\"\n",
    "    \n",
    "    print(\"üîê Iniciando sesi√≥n en ArcGIS Online...\")\n",
    "    arcpy.SignInToPortal(portal_url, usuario, clave)\n",
    "    \n",
    "    # Pausa despu√©s del login\n",
    "    time.sleep(3)\n",
    "    print(\"‚úÖ Sesi√≥n iniciada correctamente\")\n",
    "    \n",
    "    # Obtener URL correcta del feature service\n",
    "    try:\n",
    "        feature_service_url, token = obtener_url_feature_service(item_id, usuario, clave)\n",
    "        print(f\"üîó Feature layer URL detectado: {feature_service_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al obtener URL del servicio: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Pausa antes de verificar el shapefile\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Verificar que el shapefile est√© completamente disponible\n",
    "    print(\"üìã Verificando disponibilidad del shapefile...\")\n",
    "    if not arcpy.Exists(shp_buffered):\n",
    "        raise Exception(f\"El shapefile {shp_buffered} no existe o no est√° disponible\")\n",
    "    \n",
    "    # Pausa adicional para asegurar que el archivo est√© completamente creado\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        print(\"üì§ Iniciando carga de datos al servicio web...\")\n",
    "        arcpy.management.Append(inputs=shp_buffered,\n",
    "                                 target=feature_service_url,\n",
    "                                 schema_type=\"NO_TEST\",\n",
    "                                 field_mapping=\"\",\n",
    "                                 subtype=\"\",\n",
    "                                 expression=\"\")\n",
    "        \n",
    "        # Pausa despu√©s de la carga\n",
    "        time.sleep(3)\n",
    "        print(\"‚úÖ Datos agregados exitosamente al servicio web\")\n",
    "        \n",
    "        # Limpiar archivos temporales despu√©s de subida exitosa\n",
    "        limpiar_archivos_temporales(output_folder_local, nombre_shp)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al agregar los datos al feature layer: {e}\")\n",
    "        raise\n",
    "\n",
    "def esperar_archivo_disponible(archivo_path, max_intentos=30, pausa_entre_intentos=2):\n",
    "    \"\"\"\n",
    "    Espera hasta que el archivo est√© completamente disponible para ArcGIS\n",
    "    \"\"\"\n",
    "    for intento in range(max_intentos):\n",
    "        try:\n",
    "            # Verificar existencia b√°sica\n",
    "            if not os.path.exists(archivo_path):\n",
    "                print(f\"‚åõ Intento {intento + 1}: Archivo no existe a√∫n...\")\n",
    "                time.sleep(pausa_entre_intentos)\n",
    "                continue\n",
    "            \n",
    "            # Verificar que ArcGIS pueda acceder al archivo\n",
    "            if arcpy.Exists(archivo_path):\n",
    "                # Intentar describir el archivo para verificar acceso completo\n",
    "                desc = arcpy.Describe(archivo_path)\n",
    "                if desc.dataType == 'ShapeFile':\n",
    "                    print(f\"‚úÖ Archivo disponible despu√©s de {intento + 1} intentos\")\n",
    "                    return True\n",
    "            \n",
    "            print(f\"‚åõ Intento {intento + 1}: Archivo no completamente disponible...\")\n",
    "            time.sleep(pausa_entre_intentos)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚åõ Intento {intento + 1}: Error de acceso - {e}\")\n",
    "            time.sleep(pausa_entre_intentos)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def post_procesar_shapefile(output_folder_local, nombre_shp):\n",
    "    \"\"\"\n",
    "    Funci√≥n para postprocesar shapefile con pausas entre operaciones\n",
    "    \"\"\"\n",
    "    print(\"üîß Iniciando postprocesamiento del shapefile...\")\n",
    "    \n",
    "    # Configurar entorno ArcGIS\n",
    "    arcpy.env.workspace = output_folder_local\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    \n",
    "    # Definir rutas de archivos\n",
    "    shp_original = os.path.join(output_folder_local, nombre_shp)\n",
    "    shp_filtrado = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_filtrado.shp'))\n",
    "    shp_dissolved = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_procesado.shp'))\n",
    "    shp_filtrado_2 = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_filtrado_2.shp'))\n",
    "    shp_buffered = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_buffered.shp'))\n",
    "\n",
    "    # Verificar que el archivo original est√© completamente disponible\n",
    "    print(\"üîç Verificando disponibilidad del archivo original...\")\n",
    "    if not esperar_archivo_disponible(shp_original):\n",
    "        raise Exception(f\"El archivo {shp_original} no est√° disponible despu√©s de m√∫ltiples intentos\")\n",
    "    \n",
    "    # Pausa adicional para estabilidad\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Paso 1: Agregar y calcular campo Area_ha en shapefile original\n",
    "    print(\"üìè Paso 1: Calculando √°rea inicial...\")\n",
    "    try:\n",
    "        # Verificar campos existentes\n",
    "        campos_existentes = [f.name for f in arcpy.ListFields(shp_original)]\n",
    "        if 'Area_ha' not in campos_existentes:\n",
    "            print(\"   ‚ûï Agregando campo Area_ha...\")\n",
    "            arcpy.AddField_management(shp_original, 'Area_ha', 'DOUBLE')\n",
    "            time.sleep(3)  # Pausa despu√©s de agregar campo\n",
    "            print(\"   ‚úÖ Campo Area_ha agregado\")\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è Campo Area_ha ya existe\")\n",
    "        \n",
    "        print(\"   üìê Calculando geometr√≠a...\")\n",
    "        arcpy.CalculateGeometryAttributes_management(shp_original, [[\"Area_ha\", \"AREA_GEODESIC\"]], area_unit=\"HECTARES\")\n",
    "        time.sleep(4)  # Pausa despu√©s del c√°lculo de geometr√≠a\n",
    "        print(\"‚úÖ √Årea inicial calculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 1: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 2: Filtrar por √°rea >= 0.1 hect√°reas\n",
    "    print(\"üîç Paso 2: Filtrando por √°rea >= 0.1 hect√°reas...\")\n",
    "    try:\n",
    "        arcpy.MakeFeatureLayer_management(shp_original, 'temp_layer', '\"Area_ha\" >= 0.1')\n",
    "        time.sleep(3)  # Pausa despu√©s de crear layer temporal\n",
    "        \n",
    "        arcpy.CopyFeatures_management('temp_layer', shp_filtrado)\n",
    "        time.sleep(4)  # Pausa despu√©s de copiar features\n",
    "        \n",
    "        arcpy.Delete_management('temp_layer')\n",
    "        time.sleep(2)  # Pausa despu√©s de eliminar layer temporal\n",
    "        print(\"‚úÖ Filtrado inicial completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 2: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 3: Dissolver pol√≠gonos\n",
    "    print(\"üîÑ Paso 3: Disolviendo pol√≠gonos...\")\n",
    "    try:\n",
    "        arcpy.Dissolve_management(shp_filtrado, shp_dissolved, dissolve_field='fecha_obje', multi_part='SINGLE_PART')\n",
    "        time.sleep(8)  # Pausa m√°s larga despu√©s del dissolve (operaci√≥n compleja)\n",
    "        print(\"‚úÖ Dissolve completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 3: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 4: Recalcular √°rea despu√©s del dissolve\n",
    "    print(\"üìè Paso 4: Recalculando √°rea despu√©s del dissolve...\")\n",
    "    try:\n",
    "        # Verificar campos existentes\n",
    "        campos_existentes = [f.name for f in arcpy.ListFields(shp_dissolved)]\n",
    "        if 'Area_ha' not in campos_existentes:\n",
    "            print(\"   ‚ûï Agregando campo Area_ha...\")\n",
    "            arcpy.AddField_management(shp_dissolved, 'Area_ha', 'DOUBLE')\n",
    "            time.sleep(3)  # Pausa despu√©s de agregar campo\n",
    "            print(\"   ‚úÖ Campo Area_ha agregado\")\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è Campo Area_ha ya existe\")\n",
    "        \n",
    "        print(\"   üìê Calculando geometr√≠a...\")\n",
    "        arcpy.CalculateGeometryAttributes_management(shp_dissolved, [[\"Area_ha\", \"AREA_GEODESIC\"]], area_unit=\"HECTARES\")\n",
    "        time.sleep(4)  # Pausa despu√©s del c√°lculo de geometr√≠a\n",
    "        print(\"‚úÖ √Årea recalculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 4: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 5: Segundo filtro por √°rea >= 0.1 hect√°reas\n",
    "    print(\"üîç Paso 5: Segundo filtro por √°rea >= 0.1 hect√°reas...\")\n",
    "    try:\n",
    "        arcpy.MakeFeatureLayer_management(shp_dissolved, 'temp_layer_2', '\"Area_ha\" >= 0.1')\n",
    "        time.sleep(3)  # Pausa despu√©s de crear layer temporal\n",
    "        \n",
    "        arcpy.CopyFeatures_management('temp_layer_2', shp_filtrado_2)\n",
    "        time.sleep(4)  # Pausa despu√©s de copiar features\n",
    "        \n",
    "        arcpy.Delete_management('temp_layer_2')\n",
    "        time.sleep(2)  # Pausa despu√©s de eliminar layer temporal\n",
    "        print(\"‚úÖ Segundo filtrado completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 5: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 6: Crear buffer\n",
    "    print(\"üìê Paso 6: Creando buffer de 0.1 metros...\")\n",
    "    try:\n",
    "        arcpy.Buffer_analysis(shp_filtrado_2, shp_buffered, \"0.1 Meters\", dissolve_option=\"NONE\", method=\"PLANAR\")\n",
    "        time.sleep(6)  # Pausa despu√©s del buffer (operaci√≥n compleja)\n",
    "        print(\"‚úÖ Buffer creado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 6: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 7: Calcular √°rea final\n",
    "    print(\"üìè Paso 7: Calculando √°rea final...\")\n",
    "    try:\n",
    "        # Verificar campos existentes\n",
    "        campos_existentes = [f.name for f in arcpy.ListFields(shp_buffered)]\n",
    "        if 'Area_ha' not in campos_existentes:\n",
    "            print(\"   ‚ûï Agregando campo Area_ha...\")\n",
    "            arcpy.AddField_management(shp_buffered, 'Area_ha', 'DOUBLE')\n",
    "            time.sleep(3)  # Pausa despu√©s de agregar campo\n",
    "            print(\"   ‚úÖ Campo Area_ha agregado\")\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è Campo Area_ha ya existe\")\n",
    "        \n",
    "        print(\"   üìê Calculando geometr√≠a...\")\n",
    "        arcpy.CalculateGeometryAttributes_management(shp_buffered, [[\"Area_ha\", \"AREA_GEODESIC\"]], area_unit=\"HECTARES\")\n",
    "        time.sleep(4)  # Pausa despu√©s del c√°lculo de geometr√≠a\n",
    "        print(\"‚úÖ √Årea final calculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Paso 7: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"üéØ Procesamiento post-exportaci√≥n completo: {shp_buffered}\")\n",
    "    \n",
    "    # Pausa final antes de enviar a ArcGIS Online\n",
    "    time.sleep(3)\n",
    "    print(\"üåê Enviando a ArcGIS Online...\")\n",
    "    \n",
    "    try:\n",
    "        agregar_a_arcgis_web(shp_buffered, output_folder_local, nombre_shp)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al enviar a ArcGIS Online: {e}\")\n",
    "        raise\n",
    "\n",
    "def procesar_imagen_diaria(region_asset_path, export_folder='GEE_exports_diaria'):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para procesar la imagen del d√≠a actual con mosaicos por fecha\n",
    "    \n",
    "    Args:\n",
    "        region_asset_path (str): Ruta al asset de la regi√≥n en GEE\n",
    "        export_folder (str): Carpeta donde exportar los resultados\n",
    "    \"\"\"\n",
    "    \n",
    "    # Par√°metros\n",
    "    num_fechas_previas = 10  # Cambio: ahora es n√∫mero de fechas √∫nicas, no im√°genes\n",
    "    umbral_diff_ndvi = -0.3\n",
    "    umbral_ndvi_previa = 0.5\n",
    "    umbral_bais2 = 6\n",
    "    umbral_mirbi = 0\n",
    "    \n",
    "    # Cargar regi√≥n\n",
    "    try:\n",
    "        region = ee.FeatureCollection(region_asset_path)\n",
    "        print(f\"Regi√≥n cargada: {region_asset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la regi√≥n: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Obtener fecha de hoy\n",
    "    fecha_hoy = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "    fecha_hoy_str = fecha_hoy.strftime('%Y-%m-%d')\n",
    "    print(f\"Procesando para la fecha: {fecha_hoy_str}\")\n",
    "    \n",
    "    # Crear fechas para Earth Engine\n",
    "    fecha_ee_inicio = ee.Date(fecha_hoy_str)\n",
    "    fecha_ee_fin = fecha_ee_inicio.advance(1, 'day')\n",
    "    \n",
    "    # Cargar colecci√≥n Sentinel-2 SIN filtro de nubes para imagen objetivo\n",
    "    coleccion = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "                 .filterBounds(region)\n",
    "                 .map(mask_s2_clouds)\n",
    "                 .map(lambda img: img.clip(region)))\n",
    "    \n",
    "    # Cargar colecci√≥n Sentinel-2 CON filtro de nubes para im√°genes previas\n",
    "    coleccion_prev = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50))\n",
    "                     .filterBounds(region)\n",
    "                     .map(mask_s2_clouds)\n",
    "                     .map(lambda img: img.clip(region)))\n",
    "    \n",
    "    print(\"Colecciones Sentinel-2 cargadas\")\n",
    "    \n",
    "    # ===== CREAR MOSAICO PARA IMAGEN OBJETIVO =====\n",
    "    print(\"üñºÔ∏è Creando mosaico para imagen objetivo...\")\n",
    "    mosaico_objetivo, num_img_objetivo = crear_mosaico_por_fecha(\n",
    "        coleccion, fecha_ee_inicio, fecha_ee_fin, region\n",
    "    )\n",
    "    \n",
    "    if mosaico_objetivo is None:\n",
    "        print(f\"‚ùå No hay imagen disponible para hoy ({fecha_hoy_str}). No se procesar√°.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úÖ Mosaico objetivo creado con {num_img_objetivo} imagen(es)\")\n",
    "    \n",
    "    # Calcular √≠ndices para la imagen objetivo\n",
    "    ndvi_objetivo = calcular_ndvi(mosaico_objetivo)\n",
    "    bais_current = calcular_bais2(mosaico_objetivo)\n",
    "    mirbi_current = calcular_mirbi(mosaico_objetivo)\n",
    "    \n",
    "    # ===== OBTENER FECHAS √öNICAS PREVIAS =====\n",
    "    print(\"üìÖ Obteniendo fechas √∫nicas previas...\")\n",
    "    fechas_previas = obtener_fechas_unicas_previas(\n",
    "        coleccion_prev, \n",
    "        fecha_ee_inicio, \n",
    "        num_fechas_previas\n",
    "    )\n",
    "    \n",
    "    if len(fechas_previas) == 0:\n",
    "        print(\"‚ùå No se encontraron fechas previas para comparar\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úÖ Se procesar√°n {len(fechas_previas)} fechas √∫nicas\")\n",
    "    \n",
    "    # ===== PROCESAR CADA FECHA PREVIA =====\n",
    "    vectores_finales = []\n",
    "    \n",
    "    for j, fecha_previa in enumerate(fechas_previas):\n",
    "        print(f\"\\nüîÑ Procesando fecha previa {j+1}/{len(fechas_previas)}\")\n",
    "        \n",
    "        try:\n",
    "            # Crear fechas de inicio y fin del d√≠a\n",
    "            fecha_prev_inicio = fecha_previa\n",
    "            fecha_prev_fin = fecha_previa.advance(1, 'day')\n",
    "            \n",
    "            # Crear mosaico para esta fecha\n",
    "            print(\"   üñºÔ∏è Creando mosaico para fecha previa...\")\n",
    "            mosaico_prev, num_img_prev = crear_mosaico_por_fecha(\n",
    "                coleccion_prev, fecha_prev_inicio, fecha_prev_fin, region\n",
    "            )\n",
    "            \n",
    "            if mosaico_prev is None:\n",
    "                print(\"   ‚ùå No se pudo crear mosaico para esta fecha\")\n",
    "                continue\n",
    "            \n",
    "            fecha_prev_str = fecha_prev_inicio.format('YYYY-MM-dd').getInfo()\n",
    "            print(f\"   ‚úÖ Mosaico creado con {num_img_prev} imagen(es) para {fecha_prev_str}\")\n",
    "            \n",
    "            # Calcular NDVI para mosaico previo\n",
    "            ndvi_prev = calcular_ndvi(mosaico_prev)\n",
    "            \n",
    "            # Calcular diferencia NDVI\n",
    "            diff_ndvi = ndvi_objetivo.subtract(ndvi_prev).rename('Diferencia_NDVI')\n",
    "            \n",
    "            # Crear m√°scara binaria\n",
    "            zonas = diff_ndvi.lt(umbral_diff_ndvi).selfMask()\n",
    "            zonas_clip = zonas.clip(region)\n",
    "            \n",
    "            # Reducir a vectores\n",
    "            vectores = zonas_clip.reduceToVectors(\n",
    "                geometry=region,\n",
    "                scale=12.5,\n",
    "                geometryType='polygon',\n",
    "                eightConnected=True,\n",
    "                labelProperty='zone',\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "            \n",
    "            # Verificar si se generaron vectores\n",
    "            num_vectores = vectores.size().getInfo()\n",
    "            if num_vectores == 0:\n",
    "                print(f\"   ‚ùå No se generaron vectores iniciales para {fecha_prev_str}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   ‚úÖ Se generaron {num_vectores} vectores iniciales para {fecha_prev_str}\")\n",
    "            \n",
    "            # Unir pol√≠gonos\n",
    "            union_poligono = vectores.geometry()\n",
    "            poligono_current = ee.FeatureCollection([ee.Feature(union_poligono)])\n",
    "            \n",
    "            # DEBUG: Mostrar valores de los √≠ndices antes del filtrado\n",
    "            print(f\"   üìä Evaluando filtros combinados:\")\n",
    "            \n",
    "            # Aplicar condici√≥n combinada CON DEBUG\n",
    "            mask_bais2 = bais_current.lt(umbral_bais2)\n",
    "            mask_mirbi = mirbi_current.gt(umbral_mirbi)\n",
    "            mask_ndvi_prev = ndvi_prev.gt(umbral_ndvi_previa)\n",
    "            \n",
    "            # Verificar cada m√°scara por separado\n",
    "            area_bais2 = mask_bais2.clip(poligono_current).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            area_mirbi = mask_mirbi.clip(poligono_current).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            area_ndvi = mask_ndvi_prev.clip(poligono_current).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            print(f\"      - BAIS2 < {umbral_bais2}: {area_bais2}\")\n",
    "            print(f\"      - MIRBI > {umbral_mirbi}: {area_mirbi}\")\n",
    "            print(f\"      - NDVI_prev > {umbral_ndvi_previa}: {area_ndvi}\")\n",
    "            \n",
    "            # Combinar m√°scaras\n",
    "            combined_mask = (mask_bais2.Or(mask_mirbi)\n",
    "                            .And(mask_ndvi_prev)\n",
    "                            .selfMask()\n",
    "                            .rename('Combined_mask'))\n",
    "            \n",
    "            # Recortar m√°scara\n",
    "            image_clip_union = combined_mask.clip(poligono_current)\n",
    "            \n",
    "            # Verificar si la m√°scara combinada tiene p√≠xeles\n",
    "            area_combinada = image_clip_union.reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            print(f\"      - M√°scara combinada: {area_combinada}\")\n",
    "            \n",
    "            if area_combinada.get('Combined_mask', 0) == 0:\n",
    "                print(f\"   ‚ö†Ô∏è La m√°scara combinada est√° vac√≠a para {fecha_prev_str}\")\n",
    "                print(f\"   üí° Sugerencia: Ajustar umbrales - BAIS2:{umbral_bais2}, MIRBI:{umbral_mirbi}, NDVI:{umbral_ndvi_previa}\")\n",
    "                continue\n",
    "            \n",
    "            # Reducir a vectores finales\n",
    "            vectores_final = image_clip_union.reduceToVectors(\n",
    "                geometry=region,\n",
    "                scale=12.5,\n",
    "                geometryType='polygon',\n",
    "                eightConnected=True,\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "            \n",
    "            # Verificar vectores finales\n",
    "            num_vectores_final = vectores_final.size().getInfo()\n",
    "            if num_vectores_final == 0:\n",
    "                print(f\"   ‚ö†Ô∏è No se generaron vectores finales para {fecha_prev_str}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   ‚úÖ Se generaron {num_vectores_final} vectores finales para {fecha_prev_str}\")\n",
    "            \n",
    "            # Agregar metadatos\n",
    "            vectores_final = vectores_final.map(lambda feature: feature.set({\n",
    "                'fecha_objetivo': fecha_hoy_str,\n",
    "                'fecha_previa': fecha_prev_str,\n",
    "                'indice_comparacion': j,\n",
    "                'num_img_objetivo': num_img_objetivo,\n",
    "                'num_img_previa': num_img_prev\n",
    "            }))\n",
    "            \n",
    "            vectores_finales.append(vectores_final)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error procesando fecha previa {j+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combinar todos los vectores\n",
    "    if vectores_finales:\n",
    "        vectores_combinados = ee.FeatureCollection(vectores_finales).flatten()\n",
    "        \n",
    "        # Verificar si hay resultados\n",
    "        num_resultados = vectores_combinados.size().getInfo()\n",
    "        if num_resultados > 0:\n",
    "            # Exportar\n",
    "            fecha_limpia = fecha_hoy_str.replace('-', '_')\n",
    "            descripcion = f'Vectores_Mosaicos_{fecha_limpia}'\n",
    "            \n",
    "            task = ee.batch.Export.table.toDrive(\n",
    "                collection=vectores_combinados,\n",
    "                description=descripcion,\n",
    "                folder=export_folder,\n",
    "                fileFormat='SHP'\n",
    "            )\n",
    "            \n",
    "            task.start()\n",
    "            print(f\"\\nüöÄ Exportaci√≥n iniciada: {descripcion}\")\n",
    "            print(f\"üìä N√∫mero total de features a exportar: {num_resultados}\")\n",
    "            print(f\"üìÖ Fechas procesadas: {len(fechas_previas)}\")\n",
    "            \n",
    "            # Esperar que el usuario sincronice el archivo SHP en su carpeta local\n",
    "            nombre_shp = f'Vectores_Mosaicos_{fecha_limpia}.shp'\n",
    "            carpeta_local = r\"G:\\Mi unidad\\GEE_exports_diaria\"\n",
    "\n",
    "            print(\"\\n‚è≥ Esperando a que el archivo est√© disponible localmente para su procesamiento...\")\n",
    "\n",
    "            # Esperar a que aparezca el archivo SHP en la carpeta\n",
    "            shp_path = os.path.join(carpeta_local, nombre_shp)\n",
    "            espera_max = 300  # 5 minutos\n",
    "            tiempo_espera = 0\n",
    "\n",
    "            while not os.path.exists(shp_path) and tiempo_espera < espera_max:\n",
    "                time.sleep(10)\n",
    "                tiempo_espera += 10\n",
    "                print(f\"   ‚åõ Esperando archivo: {nombre_shp} ({tiempo_espera}s)\")\n",
    "\n",
    "            if os.path.exists(shp_path):\n",
    "                try:\n",
    "                    # Agregar metadatos al shapefile antes del procesamiento\n",
    "                    print(f\"\\nüìã Archivo encontrado, iniciando postprocesamiento...\")\n",
    "                    post_procesar_shapefile(carpeta_local, nombre_shp)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error durante el postprocesamiento: {e}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No se encontr√≥ el archivo {nombre_shp} despu√©s de {espera_max} segundos.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå No se encontraron √°reas de cambio para exportar\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå No se generaron vectores para ninguna fecha previa\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para ejecutar el an√°lisis diario con mosaicos\n",
    "    \"\"\"\n",
    "    # Configuraci√≥n\n",
    "    REGION_ASSET_PATH = 'projects/ee-ezabaleta/assets/Zona'  # Cambia por tu ruta\n",
    "    EXPORT_FOLDER = 'GEE_exports_diaria'  # Carpeta en Google Drive\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"AN√ÅLISIS DIARIO DE CAMBIOS CON MOSAICOS - GOOGLE EARTH ENGINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîß Funcionalidades nuevas:\")\n",
    "    print(\"   ‚Ä¢ Mosaicos autom√°ticos por fecha (m√∫ltiples im√°genes ‚Üí 1 mosaico)\")\n",
    "    print(\"   ‚Ä¢ Fechas √∫nicas (sin duplicados)\")\n",
    "    print(\"   ‚Ä¢ Mejor cobertura espacial\")\n",
    "    print(\"   ‚Ä¢ Metadatos de n√∫mero de im√°genes por mosaico\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        resultado = procesar_imagen_diaria(REGION_ASSET_PATH, EXPORT_FOLDER)\n",
    "        \n",
    "        if resultado:\n",
    "            print(\"\\n‚úÖ Procesamiento completado exitosamente\")\n",
    "            print(\"üéØ Beneficios obtenidos:\")\n",
    "            print(\"   ‚Ä¢ Mayor cobertura temporal (m√°s fechas √∫nicas)\")\n",
    "            print(\"   ‚Ä¢ Mejor cobertura espacial (mosaicos completos)\")\n",
    "            print(\"   ‚Ä¢ Eliminaci√≥n de duplicados por fecha\")\n",
    "            print(\"   ‚Ä¢ Metadatos detallados de composici√≥n de mosaicos\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Procesamiento terminado sin resultados\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error durante el procesamiento: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script calculo Severidad por poligonos\n",
    "\n",
    "Este codigo realiza el calculo de severidad en uno o varios poligonos cargados utilizando la informacion de las bandas en imagenes sentinel, este script guarda la informacion en rasters independientes para cada id de poligono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import gcd\n",
    "import ee\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "import arcpy\n",
    "import geopandas as gpd\n",
    "\n",
    "# Inicializar Earth Engine\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='incendios-461918')\n",
    "\n",
    "# Cargar capa de pol√≠gonos (GeoJSON, Shapefile, etc.)\n",
    "gdf = gpd.read_file(\"E:\\ERLUAN\\Amenas_Incendios\\Vectores\\Finales_puntos\\Vectores_prueba_severidad.shp\")  # Aseg√∫rate que tenga columna 'fecha' y geometr√≠a\n",
    "\n",
    "# Par√°metros\n",
    "dias_rango = 60\n",
    "max_nubes = 50\n",
    "max_nubes_poligono = 10  # Porcentaje m√°ximo de nubes permitido dentro del pol√≠gono\n",
    "\n",
    "# Funci√≥n para enmascarar nubes\n",
    "def maskS2sr(image):\n",
    "    qa = image.select('QA60')\n",
    "    cloudBitMask = (1 << 10)\n",
    "    cirrusBitMask = (1 << 11)\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "    return image.updateMask(mask).divide(10000).copyProperties(image, image.propertyNames())\n",
    "\n",
    "# Funci√≥n para verificar nubes dentro del pol√≠gono\n",
    "def check_clouds_in_polygon(image, geometry):\n",
    "    qa = image.select('QA60')\n",
    "    cloudBitMask = (1 << 10)\n",
    "    cirrusBitMask = (1 << 11)\n",
    "    \n",
    "    # Crear m√°scara de nubes y cirrus\n",
    "    cloud_mask = qa.bitwiseAnd(cloudBitMask).neq(0).Or(qa.bitwiseAnd(cirrusBitMask).neq(0))\n",
    "    \n",
    "    # Calcular el porcentaje de nubes en el pol√≠gono\n",
    "    area_total = ee.Image.pixelArea().reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=geometry,\n",
    "        scale=10,\n",
    "        maxPixels=1e13\n",
    "    ).get('area')\n",
    "    \n",
    "    area_nubes = ee.Image.pixelArea().updateMask(cloud_mask).reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=geometry,\n",
    "        scale=10,\n",
    "        maxPixels=1e13\n",
    "    ).get('area')\n",
    "    \n",
    "    # Manejar casos donde no hay nubes (area_nubes ser√≠a null)\n",
    "    area_nubes = ee.Number(area_nubes).divide(ee.Number(area_total)).multiply(100)\n",
    "    porcentaje_nubes = ee.Number(ee.Algorithms.If(area_nubes, area_nubes, 0))\n",
    "    \n",
    "    return porcentaje_nubes\n",
    "\n",
    "# Funci√≥n para obtener primer mosaico v√°lido (sin nubes en el pol√≠gono)\n",
    "def get_mosaico_valid(geometry, start_date, end_date):\n",
    "    date_list = ee.List.sequence(0, ee.Date(end_date).difference(ee.Date(start_date), 'day').subtract(1))\n",
    "    fechas = date_list.map(lambda d: ee.Date(start_date).advance(d, 'day'))\n",
    "\n",
    "    def iterate_func(fecha, result):\n",
    "        result = ee.Dictionary(result)\n",
    "        found = result.get('encontrado')\n",
    "        fecha = ee.Date(fecha)\n",
    "\n",
    "        # Filtrar im√°genes del d√≠a\n",
    "        coleccion_dia = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(geometry) \\\n",
    "            .filterDate(fecha, fecha.advance(1, 'day')) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_nubes))\n",
    "\n",
    "        # Verificar si hay im√°genes disponibles\n",
    "        cond_imagenes = coleccion_dia.size().gt(0)\n",
    "\n",
    "        def check_polygon_clouds():\n",
    "            # Tomar la primera imagen para verificar nubes en el pol√≠gono\n",
    "            primera_imagen = ee.Image(coleccion_dia.first())\n",
    "            porcentaje_nubes_poligono = check_clouds_in_polygon(primera_imagen, geometry)\n",
    "            \n",
    "            # Verificar si el porcentaje de nubes en el pol√≠gono es aceptable\n",
    "            imagen_valida = porcentaje_nubes_poligono.lt(max_nubes_poligono)\n",
    "            \n",
    "            return ee.Dictionary({\n",
    "                'encontrado': True, \n",
    "                'fecha': fecha,\n",
    "                'porcentaje_nubes': porcentaje_nubes_poligono,\n",
    "                'imagen_valida': imagen_valida\n",
    "            })\n",
    "\n",
    "        return ee.Algorithms.If(found,\n",
    "            result,\n",
    "            ee.Algorithms.If(cond_imagenes,\n",
    "                ee.Algorithms.If(\n",
    "                    ee.Dictionary(check_polygon_clouds()).get('imagen_valida'),\n",
    "                    check_polygon_clouds(),\n",
    "                    result\n",
    "                ),\n",
    "                result\n",
    "            )\n",
    "        )\n",
    "\n",
    "    resultado = ee.List(fechas).iterate(iterate_func, ee.Dictionary({'encontrado': False}))\n",
    "    resultado = ee.Dictionary(resultado)\n",
    "\n",
    "    # Verificar si se encontr√≥ una imagen v√°lida\n",
    "    encontrado = resultado.get('encontrado')\n",
    "    \n",
    "    def create_mosaic():\n",
    "        fecha_valida = ee.Date(resultado.get('fecha'))\n",
    "        porcentaje_nubes_final = resultado.get('porcentaje_nubes')\n",
    "        \n",
    "        coleccion = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(geometry) \\\n",
    "            .filterDate(fecha_valida, fecha_valida.advance(1, 'day')) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_nubes)) \\\n",
    "            .map(maskS2sr)\n",
    "\n",
    "        mosaico = coleccion.mosaic().clip(geometry).set({\n",
    "            'fecha_real': fecha_valida.format('YYYY-MM-dd'),\n",
    "            'porcentaje_nubes_poligono': porcentaje_nubes_final,\n",
    "            'imagen_encontrada': True\n",
    "        })\n",
    "        \n",
    "        return mosaico\n",
    "    \n",
    "    def empty_image():\n",
    "        return ee.Image().set({\n",
    "            'fecha_real': 'No encontrada',\n",
    "            'porcentaje_nubes_poligono': -1,\n",
    "            'imagen_encontrada': False\n",
    "        })\n",
    "    \n",
    "    return ee.Algorithms.If(encontrado, create_mosaic(), empty_image())\n",
    "\n",
    "# Iterar sobre cada pol√≠gono\n",
    "for idx, row in gdf.iterrows():\n",
    "    fecha_str = row['fecha_obje']\n",
    "    from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "    # Dentro del bucle for\n",
    "    if isinstance(row.geometry, MultiPolygon):\n",
    "        geom_shapely = list(row.geometry.geoms)[0]  # Usa solo el primer pol√≠gono\n",
    "    else:\n",
    "        geom_shapely = row.geometry\n",
    "\n",
    "    geom_coords = list(geom_shapely.exterior.coords)\n",
    "    geom = ee.Geometry.Polygon([geom_coords])\n",
    "    target_date = ee.Date(fecha_str)\n",
    "\n",
    "    fecha_inicio_antes = target_date.advance(-dias_rango, 'day')\n",
    "    fecha_fin_antes = target_date\n",
    "    fecha_inicio_despues = target_date.advance(1, 'day')\n",
    "    fecha_fin_despues = target_date.advance(dias_rango + 1, 'day')\n",
    "\n",
    "    try:\n",
    "        print(f'üîç Procesando pol√≠gono {idx}, fecha {fecha_str}')\n",
    "        \n",
    "        mosaico_antes = get_mosaico_valid(geom, fecha_inicio_antes, fecha_fin_antes)\n",
    "        mosaico_despues = get_mosaico_valid(geom, fecha_inicio_despues, fecha_fin_despues)\n",
    "\n",
    "        # Verificar que ambas im√°genes fueron encontradas\n",
    "        try:\n",
    "            imagen_antes_ok = ee.Image(mosaico_antes).get('imagen_encontrada').getInfo()\n",
    "            imagen_despues_ok = ee.Image(mosaico_despues).get('imagen_encontrada').getInfo()\n",
    "        except:\n",
    "            # Si hay error al obtener la propiedad, asumir que no se encontraron im√°genes\n",
    "            imagen_antes_ok = False\n",
    "            imagen_despues_ok = False\n",
    "        \n",
    "        # Solo proceder si ambas im√°genes son v√°lidas\n",
    "        if imagen_antes_ok and imagen_despues_ok:\n",
    "            try:\n",
    "                # Obtener informaci√≥n de las nubes\n",
    "                fecha_antes = ee.Image(mosaico_antes).get('fecha_real').getInfo()\n",
    "                fecha_despues = ee.Image(mosaico_despues).get('fecha_real').getInfo()\n",
    "                nubes_antes = ee.Image(mosaico_antes).get('porcentaje_nubes_poligono').getInfo()\n",
    "                nubes_despues = ee.Image(mosaico_despues).get('porcentaje_nubes_poligono').getInfo()\n",
    "                \n",
    "                # Mostrar informaci√≥n de las im√°genes encontradas\n",
    "                print(f'  üìÖ Imagen ANTES: {fecha_antes} (nubes: {nubes_antes:.1f}%)')\n",
    "                print(f'  üìÖ Imagen DESPU√âS: {fecha_despues} (nubes: {nubes_despues:.1f}%)')\n",
    "\n",
    "                # Calcular dNBR\n",
    "                def calc_nbr(img):\n",
    "                    return ee.Image(img).normalizedDifference(['B8', 'B12']).rename('NBR')\n",
    "\n",
    "                nbr_antes = calc_nbr(mosaico_antes)\n",
    "                nbr_despues = calc_nbr(mosaico_despues)\n",
    "                dnbr = nbr_antes.subtract(nbr_despues).multiply(1000).rename('dNBR')\n",
    "\n",
    "                # Exportar a Drive\n",
    "                export_id = f'dNBR_{idx}_{fecha_str}'\n",
    "                task = ee.batch.Export.image.toDrive(\n",
    "                    image=dnbr,\n",
    "                    description=export_id,\n",
    "                    folder='GEE_SENTINEL',\n",
    "                    fileNamePrefix=export_id,\n",
    "                    region=geom,\n",
    "                    scale=10,\n",
    "                    maxPixels=1e13,\n",
    "                    crs='EPSG:4326'\n",
    "                )\n",
    "                task.start()\n",
    "                print(f'‚úÖ Exportando dNBR para pol√≠gono {idx}, fecha {fecha_str}')\n",
    "                print(f'   Task ID: {task.id}')\n",
    "                \n",
    "                # Opcional: esperar un poco entre exportaciones para no sobrecargar\n",
    "                import time\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e_inner:\n",
    "                print(f'‚ö†Ô∏è Error al procesar im√°genes v√°lidas para pol√≠gono {idx}: {e_inner}')\n",
    "        else:\n",
    "            if not imagen_antes_ok:\n",
    "                print(f'‚ö†Ô∏è No se encontr√≥ imagen ANTES v√°lida (sin nubes) para pol√≠gono {idx}')\n",
    "            if not imagen_despues_ok:\n",
    "                print(f'‚ö†Ô∏è No se encontr√≥ imagen DESPU√âS v√°lida (sin nubes) para pol√≠gono {idx}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è Error en pol√≠gono {idx}: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Areas_Quemadas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
