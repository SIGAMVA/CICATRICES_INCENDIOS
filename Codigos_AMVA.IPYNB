{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion areas quemedas historico\n",
    "\n",
    "En este codigo se realiza el procedimiento para la deteccion de areas quemadas en un periodo de tiempo dado. este se realiza por medio de bloques cada x dias para evitar problemas de memoria o tiempo. El script funciona con autenticacion de google earth engine asi que se tiene que crear un usuario y projecto con acceso a la API de google earth engine en google cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=_ZkMg_NovOriedmxQIH7xzrq94C0eccNzLLt998Z6KM&tc=rn4MDBxKuCc7yKZCezoEDy_hwWLVPKTvnYXAk8L6wzs&cc=fyyT7OuphG-qONpNmPGP5pGWyTDHaUX-GJM0bMCUSSY>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=_ZkMg_NovOriedmxQIH7xzrq94C0eccNzLLt998Z6KM&tc=rn4MDBxKuCc7yKZCezoEDy_hwWLVPKTvnYXAk8L6wzs&cc=fyyT7OuphG-qONpNmPGP5pGWyTDHaUX-GJM0bMCUSSY</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** Earth Engine *** Share your feedback by taking our Annual Developer Satisfaction Survey: https://google.qualtrics.com/jfe/form/SV_7TDKVSyKvBdmMqW?ref=4i2o6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector inicializado. RegiÃ³n cargada con 1 features\n",
      "Filtro nubes objetivo: < 100%\n",
      "Filtro nubes previas: < 50%\n",
      "DÃ­as por bloque: 4\n",
      "ğŸš€ Iniciando procesamiento desde 2025-09 hasta 2025-09\n",
      "ğŸ“‹ Procesando en bloques de 4 dÃ­as\n",
      "ğŸ”„ Creando mosaicos para fechas objetivo e imÃ¡genes previas\n",
      "ğŸ”„ Usando 4 fechas previas para comparaciÃ³n\n",
      "\n",
      "ğŸ”„ BLOQUE 1: 2025-09-01 a 2025-09-04\n",
      "  ğŸ“… Procesando bloque: 2025-09-01 a 2025-09-04\n",
      "    Procesando mosaico objetivo del 2025-09-01 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-12: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-08: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-01 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-01 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 1 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-01 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-01 (mosaico 2 imgs) vs 2025-08-08 (1 imagen): 0 polÃ­gonos\n",
      "    Procesando mosaico objetivo del 2025-09-02 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-12: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-08: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-02 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 6 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-02 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 7 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-02 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 9 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-02 (mosaico 2 imgs) vs 2025-08-08 (1 imagen): 1 polÃ­gonos\n",
      "    Procesando mosaico objetivo del 2025-09-04 (3 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-12: 2 imÃ¡genes â†’ mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-04 (mosaico 3 imgs) vs 2025-08-20 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-04 (mosaico 3 imgs) vs 2025-08-12 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "    Mosaico 2025-09-04 ya procesado, omitiendo 2025-09-04\n",
      "  âœ… Bloque completado: 24 polÃ­gonos detectados\n",
      "  ğŸ“¤ ExportaciÃ³n iniciada: Bloque_20250901_a_20250904\n",
      "\n",
      "ğŸ”„ BLOQUE 2: 2025-09-05 a 2025-09-08\n",
      "  ğŸ“… Procesando bloque: 2025-09-05 a 2025-09-08\n",
      "    Procesando mosaico objetivo del 2025-09-07 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-12: 2 imÃ¡genes â†’ mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-07 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-07 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-07 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 1 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-07 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 1 polÃ­gonos\n",
      "    Mosaico 2025-09-07 ya procesado, omitiendo 2025-09-06\n",
      "    Mosaico 2025-09-07 ya procesado, omitiendo 2025-09-07\n",
      "    Procesando mosaico objetivo del 2025-09-09 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-12: 2 imÃ¡genes â†’ mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 5 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 15 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 18 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 47 polÃ­gonos\n",
      "  âœ… Bloque completado: 87 polÃ­gonos detectados\n",
      "  ğŸ“¤ ExportaciÃ³n iniciada: Bloque_20250905_a_20250908\n",
      "\n",
      "ğŸ”„ BLOQUE 3: 2025-09-09 a 2025-09-12\n",
      "  ğŸ“… Procesando bloque: 2025-09-09 a 2025-09-12\n",
      "    Procesando mosaico objetivo del 2025-09-09 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-12: 2 imÃ¡genes â†’ mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 5 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 15 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 18 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-09 (mosaico 2 imgs) vs 2025-08-12 (mosaico 2 imgs): 47 polÃ­gonos\n",
      "    Procesando mosaico objetivo del 2025-09-11 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-09: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "        - 2025-08-20: 2 imÃ¡genes â†’ mosaico\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-11 (mosaico 2 imgs) vs 2025-09-09 (mosaico 2 imgs): 4 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-11 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-11 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 2 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-11 (mosaico 2 imgs) vs 2025-08-20 (mosaico 2 imgs): 4 polÃ­gonos\n",
      "    Mosaico 2025-09-11 ya procesado, omitiendo 2025-09-11\n",
      "    Procesando mosaico objetivo del 2025-09-12 (2 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-11: 1 imagen\n",
      "        - 2025-09-09: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-12 (mosaico 2 imgs) vs 2025-09-11 (1 imagen): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-12 (mosaico 2 imgs) vs 2025-09-09 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-12 (mosaico 2 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-12 (mosaico 2 imgs) vs 2025-08-22 (1 imagen): 0 polÃ­gonos\n",
      "  âœ… Bloque completado: 95 polÃ­gonos detectados\n",
      "  ğŸ“¤ ExportaciÃ³n iniciada: Bloque_20250909_a_20250912\n",
      "\n",
      "ğŸ”„ BLOQUE 4: 2025-09-13 a 2025-09-16\n",
      "  ğŸ“… Procesando bloque: 2025-09-13 a 2025-09-16\n",
      "    Procesando mosaico objetivo del 2025-09-14 (4 imÃ¡genes)\n",
      "    Creando mosaicos para 4 fechas previas...\n",
      "      Fechas Ãºnicas encontradas: 4 de 4 solicitadas\n",
      "        - 2025-09-11: 1 imagen\n",
      "        - 2025-09-09: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-09-02: 2 imÃ¡genes â†’ mosaico\n",
      "        - 2025-08-22: 1 imagen\n",
      "    Creados 4 mosaicos previos\n",
      "      - ComparaciÃ³n 2025-09-14 (mosaico 4 imgs) vs 2025-09-11 (1 imagen): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-14 (mosaico 4 imgs) vs 2025-09-09 (mosaico 2 imgs): 2 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-14 (mosaico 4 imgs) vs 2025-09-02 (mosaico 2 imgs): 0 polÃ­gonos\n",
      "      - ComparaciÃ³n 2025-09-14 (mosaico 4 imgs) vs 2025-08-22 (1 imagen): 0 polÃ­gonos\n",
      "    Mosaico 2025-09-14 ya procesado, omitiendo 2025-09-14\n",
      "  âœ… Bloque completado: 2 polÃ­gonos detectados\n",
      "  ğŸ“¤ ExportaciÃ³n iniciada: Bloque_20250913_a_20250916\n",
      "\n",
      "ğŸ”„ BLOQUE 5: 2025-09-17 a 2025-09-20\n",
      "  ğŸ“… Procesando bloque: 2025-09-17 a 2025-09-20\n",
      "  ğŸ“„ No se encontraron Ã¡reas quemadas en este bloque\n",
      "\n",
      "ğŸ”„ BLOQUE 6: 2025-09-21 a 2025-09-24\n",
      "  ğŸ“… Procesando bloque: 2025-09-21 a 2025-09-24\n",
      "  ğŸ“„ No se encontraron Ã¡reas quemadas en este bloque\n",
      "\n",
      "ğŸ”„ BLOQUE 7: 2025-09-25 a 2025-09-28\n",
      "  ğŸ“… Procesando bloque: 2025-09-25 a 2025-09-28\n",
      "  ğŸ“„ No se encontraron Ã¡reas quemadas en este bloque\n",
      "\n",
      "ğŸ”„ BLOQUE 8: 2025-09-29 a 2025-09-30\n",
      "  ğŸ“… Procesando bloque: 2025-09-29 a 2025-09-30\n",
      "  ğŸ“„ No se encontraron Ã¡reas quemadas en este bloque\n",
      "\n",
      "ğŸ‰ Â¡Procesamiento completado! Se procesaron 8 bloques.\n",
      "ğŸ“‹ Revisa tu Google Drive para los archivos exportados.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='incendios-461918')\n",
    "\n",
    "\n",
    "class QuemadaDetector:\n",
    "    def __init__(self, region_asset_path, max_dias_busqueda=1, num_imagenes_previas=5, \n",
    "                 nubes_max_objetivo=100, nubes_max_previas=50, dias_por_bloque=10):\n",
    "        \"\"\"\n",
    "        Inicializar el detector de Ã¡reas quemadas\n",
    "        \n",
    "        Args:\n",
    "            region_asset_path (str): Ruta del asset de la regiÃ³n en GEE\n",
    "            max_dias_busqueda (int): MÃ¡ximo de dÃ­as para buscar imagen si no hay en fecha exacta\n",
    "            num_imagenes_previas (int): NÃºmero de imÃ¡genes previas para comparar\n",
    "            nubes_max_objetivo (float): % mÃ¡ximo de nubes para imÃ¡genes objetivo\n",
    "            nubes_max_previas (float): % mÃ¡ximo de nubes para imÃ¡genes previas\n",
    "            dias_por_bloque (int): NÃºmero de dÃ­as por bloque de procesamiento\n",
    "        \"\"\"\n",
    "        self.region = ee.FeatureCollection(region_asset_path)\n",
    "        self.max_dias_busqueda = max_dias_busqueda\n",
    "        self.num_imagenes_previas = num_imagenes_previas\n",
    "        self.nubes_max_objetivo = nubes_max_objetivo\n",
    "        self.nubes_max_previas = nubes_max_previas\n",
    "        self.dias_por_bloque = dias_por_bloque\n",
    "        \n",
    "        # ParÃ¡metros de umbrales\n",
    "        self.umbral_diff_ndvi = -0.3\n",
    "        self.umbral_ndvi_prev = 0.5\n",
    "        self.umbral_bais2 = 6\n",
    "        self.umbral_mirbi = 0\n",
    "        \n",
    "        # ColecciÃ³n base para imÃ¡genes objetivo\n",
    "        self.coleccion_objetivo = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "            .filterBounds(self.region) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', self.nubes_max_objetivo)) \\\n",
    "            .map(self.mask_s2_clouds) \\\n",
    "            .map(lambda img: img.clip(self.region))\n",
    "        \n",
    "        # ColecciÃ³n base para imÃ¡genes previas\n",
    "        self.coleccion_previas = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "            .filterBounds(self.region) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', self.nubes_max_previas)) \\\n",
    "            .map(self.mask_s2_clouds) \\\n",
    "            .map(lambda img: img.clip(self.region))\n",
    "        \n",
    "        print(f\"Detector inicializado. RegiÃ³n cargada con {self.region.size().getInfo()} features\")\n",
    "        print(f\"Filtro nubes objetivo: < {self.nubes_max_objetivo}%\")\n",
    "        print(f\"Filtro nubes previas: < {self.nubes_max_previas}%\")\n",
    "        print(f\"DÃ­as por bloque: {self.dias_por_bloque}\")\n",
    "    \n",
    "    def mask_s2_clouds(self, image):\n",
    "        \"\"\"FunciÃ³n mejorada para enmascarar nubes (QA60 + SCL)\"\"\"\n",
    "        # 1. MÃ¡scara QA60 (nubes y cirros)\n",
    "        qa = image.select('QA60')\n",
    "        cloud_bit_mask = 1 << 10\n",
    "        cirrus_bit_mask = 1 << 11\n",
    "        mask_qa = qa.bitwiseAnd(cloud_bit_mask).eq(0) \\\n",
    "                   .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "        \n",
    "        # 2. MÃ¡scara SCL: nubes medias, altas, sombras y cirros delgados\n",
    "        scl = image.select('SCL')\n",
    "        mask_scl = scl.neq(3) \\\n",
    "                     .And(scl.neq(8)) \\\n",
    "                     .And(scl.neq(9)) \\\n",
    "                     .And(scl.neq(10))\n",
    "        \n",
    "        # Combinar mÃ¡scaras\n",
    "        mask_final = mask_qa.And(mask_scl)\n",
    "        \n",
    "        return image.updateMask(mask_final)\n",
    "    \n",
    "    def calcular_ndvi(self, image):\n",
    "        \"\"\"Calcular NDVI usando las bandas B8 (NIR) y B4 (rojo)\"\"\"\n",
    "        return image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    \n",
    "    def calcular_bais2(self, image):\n",
    "        \"\"\"Calcular Ã­ndice BAIS2 para detecciÃ³n de Ã¡reas quemadas\"\"\"\n",
    "        b4 = image.select('B4')\n",
    "        b6 = image.select('B6')\n",
    "        b7 = image.select('B7')\n",
    "        b8a = image.select('B8A')\n",
    "        b12 = image.select('B12')\n",
    "        \n",
    "        bais2 = image.expression(\n",
    "            '(1 - sqrt((RE2 * RE3 * N2) / R)) * (sqrt((S2 - N2) / (S2 + N2)) + 1)', {\n",
    "                'RE2': b6,\n",
    "                'RE3': b7,\n",
    "                'N2': b8a,\n",
    "                'R': b4,\n",
    "                'S2': b12\n",
    "            }\n",
    "        ).rename('BAIS2')\n",
    "        \n",
    "        return bais2\n",
    "    \n",
    "    def calcular_mirbi(self, image):\n",
    "        \"\"\"Calcular MIRBI para detectar Ã¡reas quemadas\"\"\"\n",
    "        return image.expression(\n",
    "            '10 * (B12 - (B11 + B8A) / 2)', {\n",
    "                'B12': image.select('B12'),\n",
    "                'B11': image.select('B11'),\n",
    "                'B8A': image.select('B8A')\n",
    "            }\n",
    "        ).rename('MIRBI')\n",
    "    \n",
    "    def buscar_mosaico_en_fecha(self, fecha_objetivo, coleccion):\n",
    "        \"\"\"\n",
    "        Buscar y crear mosaico para una fecha especÃ­fica, avanzando dÃ­a a dÃ­a si es necesario\n",
    "        \n",
    "        Args:\n",
    "            fecha_objetivo (ee.Date): Fecha objetivo\n",
    "            coleccion (ee.ImageCollection): ColecciÃ³n donde buscar\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (mosaico, fecha_real, num_imagenes, encontrada)\n",
    "        \"\"\"\n",
    "        current_date = fecha_objetivo\n",
    "        \n",
    "        for i in range(self.max_dias_busqueda):\n",
    "            start_date = current_date\n",
    "            end_date = current_date.advance(1, 'day')\n",
    "            \n",
    "            img_collection = coleccion.filterDate(start_date, end_date)\n",
    "            num_imagenes = img_collection.size().getInfo()\n",
    "            \n",
    "            # Verificar si hay imÃ¡genes\n",
    "            if num_imagenes > 0:\n",
    "                fecha_real = current_date.format('YYYY-MM-dd')\n",
    "                \n",
    "                if num_imagenes == 1:\n",
    "                    # Si solo hay una imagen, usarla directamente\n",
    "                    mosaico = ee.Image(img_collection.first())\n",
    "                else:\n",
    "                    # Si hay mÃºltiples imÃ¡genes, crear mosaico\n",
    "                    mosaico = img_collection.mosaic()\n",
    "                \n",
    "                # Agregar metadatos al mosaico\n",
    "                mosaico = mosaico.set({\n",
    "                    'fecha_mosaico': fecha_real,\n",
    "                    'num_imagenes_mosaico': num_imagenes,\n",
    "                    'system:time_start': current_date.millis()\n",
    "                })\n",
    "                \n",
    "                return mosaico, fecha_real, num_imagenes, True\n",
    "            \n",
    "            current_date = current_date.advance(1, 'day')\n",
    "        \n",
    "        return None, None, 0, False\n",
    "    \n",
    "    def crear_mosaicos_por_fecha(self, coleccion, num_fechas):\n",
    "        \"\"\"\n",
    "        Crear mosaicos para las N fechas mÃ¡s recientes, agrupando imÃ¡genes por fecha\n",
    "        \n",
    "        Args:\n",
    "            coleccion (ee.ImageCollection): ColecciÃ³n de imÃ¡genes\n",
    "            num_fechas (int): NÃºmero de fechas Ãºnicas a obtener\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de mosaicos Ãºnicos por fecha\n",
    "        \"\"\"\n",
    "        # Obtener lista de fechas Ãºnicas\n",
    "        def extract_date(image):\n",
    "            return ee.Feature(None, {\n",
    "                'date': ee.Date(image.get('system:time_start')).format('YYYY-MM-dd'),\n",
    "                'timestamp': image.get('system:time_start')\n",
    "            })\n",
    "        \n",
    "        # Extraer fechas de todas las imÃ¡genes\n",
    "        fechas_collection = coleccion.map(extract_date)\n",
    "        fechas_distintas = fechas_collection.distinct(['date']).sort('timestamp', False)\n",
    "        \n",
    "        # Tomar solo las N fechas mÃ¡s recientes\n",
    "        fechas_lista = fechas_distintas.limit(num_fechas).aggregate_array('date')\n",
    "        fechas_info = fechas_lista.getInfo()\n",
    "        \n",
    "        print(f\"      Fechas Ãºnicas encontradas: {len(fechas_info)} de {num_fechas} solicitadas\")\n",
    "        \n",
    "        mosaicos = []\n",
    "        \n",
    "        for fecha_str in fechas_info:\n",
    "            # Filtrar imÃ¡genes de esta fecha especÃ­fica\n",
    "            fecha_ee = ee.Date(fecha_str)\n",
    "            inicio_dia = fecha_ee\n",
    "            fin_dia = fecha_ee.advance(1, 'day')\n",
    "            \n",
    "            imagenes_fecha = coleccion.filterDate(inicio_dia, fin_dia)\n",
    "            num_imagenes = imagenes_fecha.size().getInfo()\n",
    "            \n",
    "            if num_imagenes > 0:\n",
    "                if num_imagenes == 1:\n",
    "                    # Si solo hay una imagen, usarla directamente\n",
    "                    mosaico = ee.Image(imagenes_fecha.first())\n",
    "                    print(f\"        - {fecha_str}: 1 imagen\")\n",
    "                else:\n",
    "                    # Si hay mÃºltiples imÃ¡genes, crear mosaico\n",
    "                    mosaico = imagenes_fecha.mosaic()\n",
    "                    print(f\"        - {fecha_str}: {num_imagenes} imÃ¡genes â†’ mosaico\")\n",
    "                \n",
    "                # Agregar metadatos al mosaico\n",
    "                mosaico = mosaico.set({\n",
    "                    'fecha_mosaico': fecha_str,\n",
    "                    'num_imagenes_mosaico': num_imagenes,\n",
    "                    'system:time_start': fecha_ee.millis()\n",
    "                })\n",
    "                \n",
    "                mosaicos.append(mosaico)\n",
    "        \n",
    "        return mosaicos\n",
    "    \n",
    "    def procesar_bloque(self, fecha_inicio, fecha_fin):\n",
    "        \"\"\"\n",
    "        Procesar un bloque especÃ­fico de dÃ­as\n",
    "        \n",
    "        Args:\n",
    "            fecha_inicio (datetime.date): Fecha de inicio del bloque\n",
    "            fecha_fin (datetime.date): Fecha de fin del bloque\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de vectores del bloque\n",
    "        \"\"\"\n",
    "        print(f\"  ğŸ“… Procesando bloque: {fecha_inicio} a {fecha_fin}\")\n",
    "        \n",
    "        vectores_bloque = []\n",
    "        imagenes_procesadas = set()\n",
    "        \n",
    "        # Procesar cada dÃ­a del bloque\n",
    "        current_date = fecha_inicio\n",
    "        while current_date <= fecha_fin:\n",
    "            fecha_str = current_date.strftime('%Y-%m-%d')\n",
    "            fecha_ee = ee.Date(fecha_str)\n",
    "            \n",
    "            # Buscar y crear mosaico para la fecha objetivo\n",
    "            mosaico_objetivo, fecha_real_ee, num_imgs_objetivo, encontrada = self.buscar_mosaico_en_fecha(fecha_ee, self.coleccion_objetivo)\n",
    "            \n",
    "            if not encontrada:\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            # Obtener fecha real como string\n",
    "            fecha_real = fecha_real_ee.getInfo()\n",
    "            \n",
    "            # Evitar procesar la misma fecha varias veces\n",
    "            if fecha_real in imagenes_procesadas:\n",
    "                print(f\"    Mosaico {fecha_real} ya procesado, omitiendo {fecha_str}\")\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            imagenes_procesadas.add(fecha_real)\n",
    "            \n",
    "            # Mostrar informaciÃ³n del mosaico objetivo\n",
    "            if num_imgs_objetivo == 1:\n",
    "                print(f\"    Procesando mosaico objetivo del {fecha_real} (1 imagen)\")\n",
    "            else:\n",
    "                print(f\"    Procesando mosaico objetivo del {fecha_real} ({num_imgs_objetivo} imÃ¡genes)\")\n",
    "            \n",
    "            # Para el % de nubes, calculamos un promedio ponderado aproximado\n",
    "            # (En un mosaico esto es mÃ¡s complejo, pero usamos el valor mÃ¡ximo permitido como referencia)\n",
    "            nubes_objetivo = self.nubes_max_objetivo  # Valor representativo para mosaicos\n",
    "            \n",
    "            # Calcular Ã­ndices para mosaico objetivo\n",
    "            ndvi_objetivo = self.calcular_ndvi(mosaico_objetivo)\n",
    "            bais_objetivo = self.calcular_bais2(mosaico_objetivo)\n",
    "            mirbi_objetivo = self.calcular_mirbi(mosaico_objetivo)\n",
    "            \n",
    "            # Obtener imÃ¡genes previas y crear mosaicos por fecha\n",
    "            target_millis = ee.Number(mosaico_objetivo.get('system:time_start'))\n",
    "            imagenes_previas_collection = self.coleccion_previas \\\n",
    "                .filter(ee.Filter.lt('system:time_start', target_millis)) \\\n",
    "                .sort('system:time_start', False)\n",
    "            \n",
    "            # Verificar si hay imÃ¡genes previas\n",
    "            if imagenes_previas_collection.size().getInfo() == 0:\n",
    "                print(f\"    No hay imÃ¡genes previas para {fecha_real}\")\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            # Crear mosaicos por fecha\n",
    "            print(f\"    Creando mosaicos para {self.num_imagenes_previas} fechas previas...\")\n",
    "            mosaicos_previos = self.crear_mosaicos_por_fecha(imagenes_previas_collection, self.num_imagenes_previas)\n",
    "            \n",
    "            if len(mosaicos_previos) == 0:\n",
    "                print(f\"    No se pudieron crear mosaicos previos para {fecha_real}\")\n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            print(f\"    Creados {len(mosaicos_previos)} mosaicos previos\")\n",
    "            \n",
    "            # Procesar cada mosaico previo\n",
    "            for mosaico_prev in mosaicos_previos:\n",
    "                fecha_prev = mosaico_prev.get('fecha_mosaico').getInfo()\n",
    "                num_imgs_mosaico = mosaico_prev.get('num_imagenes_mosaico').getInfo()\n",
    "                \n",
    "                # Para el % de nubes, usamos un valor representativo\n",
    "                nubes_prev = self.nubes_max_previas  # Valor por defecto para mosaicos\n",
    "                \n",
    "                # Calcular NDVI del mosaico previo\n",
    "                ndvi_prev = self.calcular_ndvi(mosaico_prev)\n",
    "                \n",
    "                # Calcular diferencia de NDVI\n",
    "                diff_ndvi = ndvi_objetivo.subtract(ndvi_prev).rename('Diferencia_NDVI')\n",
    "                \n",
    "                # Crear mÃ¡scara inicial: zonas donde diff NDVI < umbral\n",
    "                zonas = diff_ndvi.lt(self.umbral_diff_ndvi).selfMask()\n",
    "                zonas_clip = zonas.clip(self.region)\n",
    "                \n",
    "                # Convertir a vectores para obtener geometrÃ­a de Ã¡reas de interÃ©s\n",
    "                try:\n",
    "                    vectores_temp = zonas_clip.reduceToVectors(\n",
    "                        geometry=self.region,\n",
    "                        scale=12.5,\n",
    "                        geometryType='polygon',\n",
    "                        eightConnected=True,\n",
    "                        labelProperty='zone',\n",
    "                        maxPixels=1e8\n",
    "                    )\n",
    "                    \n",
    "                    if vectores_temp.size().getInfo() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Unir polÃ­gonos\n",
    "                    union_poligono = vectores_temp.geometry()\n",
    "                    if union_poligono.area(maxError=1).getInfo() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    poligono_fc = ee.FeatureCollection([ee.Feature(union_poligono)])\n",
    "                    \n",
    "                    # Aplicar mÃ¡scara combinada\n",
    "                    combined_mask = bais_objetivo.lt(self.umbral_bais2) \\\n",
    "                                   .Or(mirbi_objetivo.gt(self.umbral_mirbi)) \\\n",
    "                                   .And(ndvi_prev.gt(self.umbral_ndvi_prev)) \\\n",
    "                                   .selfMask() \\\n",
    "                                   .rename('Combined_mask')\n",
    "                    \n",
    "                    # Recortar a Ã¡rea de interÃ©s\n",
    "                    image_clip_union = combined_mask.clip(poligono_fc)\n",
    "                    \n",
    "                    # Vectorizar resultado final\n",
    "                    vectores_final = image_clip_union.reduceToVectors(\n",
    "                        geometry=self.region,\n",
    "                        scale=12.5,\n",
    "                        geometryType='polygon',\n",
    "                        eightConnected=True,\n",
    "                        maxPixels=1e8\n",
    "                    )\n",
    "                    \n",
    "                    # Agregar metadatos\n",
    "                    vectores_final = vectores_final.map(lambda feature: \n",
    "                        feature.set({\n",
    "                            'fecha_objetivo': fecha_real,\n",
    "                            'fecha_previa': fecha_prev,\n",
    "                            'nubes_objetivo': nubes_objetivo,\n",
    "                            'nubes_previa': nubes_prev,\n",
    "                            'imagenes_objetivo': num_imgs_objetivo,\n",
    "                            'imagenes_mosaico': num_imgs_mosaico,\n",
    "                            'bloque_inicio': fecha_inicio.strftime('%Y-%m-%d'),\n",
    "                            'bloque_fin': fecha_fin.strftime('%Y-%m-%d')\n",
    "                        })\n",
    "                    )\n",
    "                    \n",
    "                    vectores_bloque.append(vectores_final)\n",
    "                    num_poligonos = vectores_final.size().getInfo()\n",
    "                    \n",
    "                    # Mostrar informaciÃ³n detallada\n",
    "                    objetivo_info = f\"mosaico {num_imgs_objetivo} imgs\" if num_imgs_objetivo > 1 else \"1 imagen\"\n",
    "                    previa_info = f\"mosaico {num_imgs_mosaico} imgs\" if num_imgs_mosaico > 1 else \"1 imagen\"\n",
    "                    \n",
    "                    print(f\"      - ComparaciÃ³n {fecha_real} ({objetivo_info}) vs {fecha_prev} ({previa_info}): {num_poligonos} polÃ­gonos\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error procesando comparaciÃ³n {fecha_real} vs {fecha_prev}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "        return vectores_bloque\n",
    "    \n",
    "    def procesar_rango(self, start_year, start_month, end_year, end_month, export_folder='GEE_exports'):\n",
    "        \"\"\"\n",
    "        Procesar un rango de fechas dividiÃ©ndolo en bloques\n",
    "        \n",
    "        Args:\n",
    "            start_year (int): AÃ±o inicial\n",
    "            start_month (int): Mes inicial (1-12)\n",
    "            end_year (int): AÃ±o final\n",
    "            end_month (int): Mes final (1-12)\n",
    "            export_folder (str): Carpeta de exportaciÃ³n en Google Drive\n",
    "        \"\"\"\n",
    "        print(f\"ğŸš€ Iniciando procesamiento desde {start_year}-{start_month:02d} hasta {end_year}-{end_month:02d}\")\n",
    "        print(f\"ğŸ“‹ Procesando en bloques de {self.dias_por_bloque} dÃ­as\")\n",
    "        print(f\"ğŸ”„ Creando mosaicos para fechas objetivo e imÃ¡genes previas\")\n",
    "        print(f\"ğŸ”„ Usando {self.num_imagenes_previas} fechas previas para comparaciÃ³n\")\n",
    "        \n",
    "        # Fechas de inicio y fin\n",
    "        fecha_inicio_total = datetime.date(start_year, start_month, 1)\n",
    "        if end_month == 12:\n",
    "            fecha_fin_total = datetime.date(end_year + 1, 1, 1) - datetime.timedelta(days=1)\n",
    "        else:\n",
    "            fecha_fin_total = datetime.date(end_year, end_month + 1, 1) - datetime.timedelta(days=1)\n",
    "        \n",
    "        # Procesar por bloques\n",
    "        current_start = fecha_inicio_total\n",
    "        bloque_num = 1\n",
    "        \n",
    "        while current_start <= fecha_fin_total:\n",
    "            # Calcular fin del bloque actual\n",
    "            current_end = min(\n",
    "                current_start + datetime.timedelta(days=self.dias_por_bloque - 1),\n",
    "                fecha_fin_total\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nğŸ”„ BLOQUE {bloque_num}: {current_start} a {current_end}\")\n",
    "            \n",
    "            # Procesar bloque\n",
    "            vectores_bloque = self.procesar_bloque(current_start, current_end)\n",
    "            \n",
    "            # Exportar resultados del bloque si hay datos\n",
    "            if vectores_bloque:\n",
    "                vectores_combinados = ee.FeatureCollection(vectores_bloque).flatten()\n",
    "                total_poligonos = vectores_combinados.size().getInfo()\n",
    "                \n",
    "                if total_poligonos > 0:\n",
    "                    print(f\"  âœ… Bloque completado: {total_poligonos} polÃ­gonos detectados\")\n",
    "                    \n",
    "                    # Crear nombre de archivo descriptivo\n",
    "                    start_str = current_start.strftime('%Y%m%d')\n",
    "                    end_str = current_end.strftime('%Y%m%d')\n",
    "                    description = f'Bloque_{start_str}_a_{end_str}'\n",
    "                    \n",
    "                    try:\n",
    "                        task = ee.batch.Export.table.toDrive(\n",
    "                            collection=vectores_combinados,\n",
    "                            description=description,\n",
    "                            folder=export_folder,\n",
    "                            fileFormat='SHP'\n",
    "                        )\n",
    "                        \n",
    "                        task.start()\n",
    "                        print(f\"  ğŸ“¤ ExportaciÃ³n iniciada: {description}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Error en exportaciÃ³n del bloque: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"  ğŸ“„ Bloque sin polÃ­gonos para exportar\")\n",
    "            else:\n",
    "                print(f\"  ğŸ“„ No se encontraron Ã¡reas quemadas en este bloque\")\n",
    "            \n",
    "            # Avanzar al siguiente bloque\n",
    "            current_start = current_end + datetime.timedelta(days=1)\n",
    "            bloque_num += 1\n",
    "            \n",
    "            # Esperar un poco entre bloques\n",
    "            time.sleep(2)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Â¡Procesamiento completado! Se procesaron {bloque_num-1} bloques.\")\n",
    "        print(\"ğŸ“‹ Revisa tu Google Drive para los archivos exportados.\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicializar detector\n",
    "    detector = QuemadaDetector(\n",
    "        region_asset_path='projects/ee-ezabaleta/assets/Zona',\n",
    "        max_dias_busqueda=3,\n",
    "        num_imagenes_previas=4,\n",
    "        nubes_max_objetivo=100,    # MÃ¡ximo 100% de nubes para imÃ¡genes objetivo\n",
    "        nubes_max_previas=50,     # MÃ¡ximo 50% de nubes para imÃ¡genes previas\n",
    "        dias_por_bloque=4     # Procesar en bloques de 5 dÃ­as\n",
    "    )\n",
    "    \n",
    "    # Procesar rango de fechas\n",
    "    # Ejemplo: procesar desde enero 2025 hasta junio 2025\n",
    "    detector.procesar_rango(\n",
    "        start_year=2025,\n",
    "        start_month=9,\n",
    "        end_year=2025,\n",
    "        end_month=9,\n",
    "        export_folder='GEE_exports'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtro areas quemadas por area y conversion a shp\n",
    "\n",
    "En este codigo se unen todos los bloques calculados en el script anterior, se filtran por area y se guarda en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Iniciando procesamiento de Ã¡reas quemadas\n",
      "ğŸ“‚ Carpeta de entrada: G:\\Mi unidad\\GEE_exports\n",
      "ğŸ’¾ Archivo de salida: G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\n",
      "------------------------------------------------------------\n",
      "ğŸ”„ Iniciando merge de archivos SHP...\n",
      "ğŸ“ Encontrados 72 archivos SHP\n",
      "  ğŸ“„ Procesando: Bloque_20200101_a_20200104.shp\n",
      "    âœ… Agregado: 216 features\n",
      "  ğŸ“„ Procesando: Bloque_20200105_a_20200108.shp\n",
      "    âœ… Agregado: 165 features\n",
      "  ğŸ“„ Procesando: Bloque_20200109_a_20200112.shp\n",
      "    âœ… Agregado: 244 features\n",
      "  ğŸ“„ Procesando: Bloque_20200113_a_20200116.shp\n",
      "    âœ… Agregado: 718 features\n",
      "  ğŸ“„ Procesando: Bloque_20200117_a_20200120.shp\n",
      "    âœ… Agregado: 457 features\n",
      "  ğŸ“„ Procesando: Bloque_20200125_a_20200128.shp\n",
      "    âœ… Agregado: 180 features\n",
      "  ğŸ“„ Procesando: Bloque_20200129_a_20200201.shp\n",
      "    âœ… Agregado: 138 features\n",
      "  ğŸ“„ Procesando: Bloque_20200202_a_20200205.shp\n",
      "    âœ… Agregado: 91 features\n",
      "  ğŸ“„ Procesando: Bloque_20200206_a_20200209.shp\n",
      "    âœ… Agregado: 223 features\n",
      "  ğŸ“„ Procesando: Bloque_20200210_a_20200213.shp\n",
      "    âœ… Agregado: 165 features\n",
      "  ğŸ“„ Procesando: Bloque_20200214_a_20200217.shp\n",
      "    âœ… Agregado: 65 features\n",
      "  ğŸ“„ Procesando: Bloque_20200218_a_20200221.shp\n",
      "    âœ… Agregado: 114 features\n",
      "  ğŸ“„ Procesando: Bloque_20200222_a_20200225.shp\n",
      "    âœ… Agregado: 18 features\n",
      "  ğŸ“„ Procesando: Bloque_20200321_a_20200324.shp\n",
      "    âœ… Agregado: 95 features\n",
      "  ğŸ“„ Procesando: Bloque_20200317_a_20200320.shp\n",
      "    âœ… Agregado: 189 features\n",
      "  ğŸ“„ Procesando: Bloque_20200313_a_20200316.shp\n",
      "    âœ… Agregado: 127 features\n",
      "  ğŸ“„ Procesando: Bloque_20200309_a_20200312.shp\n",
      "    âœ… Agregado: 245 features\n",
      "  ğŸ“„ Procesando: Bloque_20200305_a_20200308.shp\n",
      "    âœ… Agregado: 315 features\n",
      "  ğŸ“„ Procesando: Bloque_20200301_a_20200304.shp\n",
      "    âœ… Agregado: 1 features\n",
      "  ğŸ“„ Procesando: Bloque_20200226_a_20200229.shp\n",
      "    âœ… Agregado: 136 features\n",
      "  ğŸ“„ Procesando: Bloque_20200325_a_20200328.shp\n",
      "    âœ… Agregado: 177 features\n",
      "  ğŸ“„ Procesando: Bloque_20200329_a_20200401.shp\n",
      "    âœ… Agregado: 3 features\n",
      "  ğŸ“„ Procesando: Bloque_20200402_a_20200405.shp\n",
      "    âœ… Agregado: 154 features\n",
      "  ğŸ“„ Procesando: Bloque_20200414_a_20200417.shp\n",
      "    âœ… Agregado: 61 features\n",
      "  ğŸ“„ Procesando: Bloque_20200422_a_20200425.shp\n",
      "    âœ… Agregado: 6 features\n",
      "  ğŸ“„ Procesando: Bloque_20200426_a_20200429.shp\n",
      "    âœ… Agregado: 7 features\n",
      "  ğŸ“„ Procesando: Bloque_20200430_a_20200503.shp\n",
      "    âœ… Agregado: 1 features\n",
      "  ğŸ“„ Procesando: Bloque_20200601_a_20200604.shp\n",
      "    âœ… Agregado: 6 features\n",
      "  ğŸ“„ Procesando: Bloque_20200524_a_20200527.shp\n",
      "    âœ… Agregado: 17 features\n",
      "  ğŸ“„ Procesando: Bloque_20200520_a_20200523.shp\n",
      "    âœ… Agregado: 191 features\n",
      "  ğŸ“„ Procesando: Bloque_20200516_a_20200519.shp\n",
      "    âœ… Agregado: 50 features\n",
      "  ğŸ“„ Procesando: Bloque_20200512_a_20200515.shp\n",
      "    âœ… Agregado: 3 features\n",
      "  ğŸ“„ Procesando: Bloque_20200605_a_20200608.shp\n",
      "    âœ… Agregado: 105 features\n",
      "  ğŸ“„ Procesando: Bloque_20200621_a_20200624.shp\n",
      "    âœ… Agregado: 62 features\n",
      "  ğŸ“„ Procesando: Bloque_20200703_a_20200706.shp\n",
      "    âœ… Agregado: 10 features\n",
      "  ğŸ“„ Procesando: Bloque_20200705_a_20200708.shp\n",
      "    âœ… Agregado: 10 features\n",
      "  ğŸ“„ Procesando: Bloque_20200713_a_20200716.shp\n",
      "    âœ… Agregado: 136 features\n",
      "  ğŸ“„ Procesando: Bloque_20200717_a_20200720.shp\n",
      "    âœ… Agregado: 136 features\n",
      "  ğŸ“„ Procesando: Bloque_20200814_a_20200817.shp\n",
      "    âœ… Agregado: 19 features\n",
      "  ğŸ“„ Procesando: Bloque_20200810_a_20200813.shp\n",
      "    âœ… Agregado: 221 features\n",
      "  ğŸ“„ Procesando: Bloque_20200802_a_20200805.shp\n",
      "    âœ… Agregado: 113 features\n",
      "  ğŸ“„ Procesando: Bloque_20200729_a_20200801.shp\n",
      "    âœ… Agregado: 47 features\n",
      "  ğŸ“„ Procesando: Bloque_20200725_a_20200728.shp\n",
      "    âœ… Agregado: 102 features\n",
      "  ğŸ“„ Procesando: Bloque_20200721_a_20200724.shp\n",
      "    âœ… Agregado: 49 features\n",
      "  ğŸ“„ Procesando: Bloque_20200826_a_20200829.shp\n",
      "    âœ… Agregado: 39 features\n",
      "  ğŸ“„ Procesando: Bloque_20200830_a_20200902.shp\n",
      "    âœ… Agregado: 176 features\n",
      "  ğŸ“„ Procesando: Bloque_20200903_a_20200906.shp\n",
      "    âœ… Agregado: 124 features\n",
      "  ğŸ“„ Procesando: Bloque_20200907_a_20200910.shp\n",
      "    âœ… Agregado: 184 features\n",
      "  ğŸ“„ Procesando: Bloque_20200911_a_20200914.shp\n",
      "    âœ… Agregado: 16 features\n",
      "  ğŸ“„ Procesando: Bloque_20200915_a_20200918.shp\n",
      "    âœ… Agregado: 49 features\n",
      "  ğŸ“„ Procesando: Bloque_20200919_a_20200922.shp\n",
      "    âœ… Agregado: 9 features\n",
      "  ğŸ“„ Procesando: Bloque_20200923_a_20200926.shp\n",
      "    âœ… Agregado: 300 features\n",
      "  ğŸ“„ Procesando: Bloque_20200927_a_20200930.shp\n",
      "    âœ… Agregado: 60 features\n",
      "  ğŸ“„ Procesando: Bloque_20201001_a_20201004.shp\n",
      "    âœ… Agregado: 31 features\n",
      "  ğŸ“„ Procesando: Bloque_20201009_a_20201012.shp\n",
      "    âœ… Agregado: 634 features\n",
      "  ğŸ“„ Procesando: Bloque_20201013_a_20201016.shp\n",
      "    âœ… Agregado: 294 features\n",
      "  ğŸ“„ Procesando: Bloque_20201017_a_20201020.shp\n",
      "    âœ… Agregado: 161 features\n",
      "  ğŸ“„ Procesando: Bloque_20201021_a_20201024.shp\n",
      "    âœ… Agregado: 43 features\n",
      "  ğŸ“„ Procesando: Bloque_20201102_a_20201105.shp\n",
      "    âœ… Agregado: 14 features\n",
      "  ğŸ“„ Procesando: Bloque_20201106_a_20201109.shp\n",
      "    âœ… Agregado: 13 features\n",
      "  ğŸ“„ Procesando: Bloque_20201110_a_20201113.shp\n",
      "    âœ… Agregado: 27 features\n",
      "  ğŸ“„ Procesando: Bloque_20201114_a_20201117.shp\n",
      "    âœ… Agregado: 26 features\n",
      "  ğŸ“„ Procesando: Bloque_20201212_a_20201215.shp\n",
      "    âœ… Agregado: 109 features\n",
      "  ğŸ“„ Procesando: Bloque_20201208_a_20201211.shp\n",
      "    âœ… Agregado: 14 features\n",
      "  ğŸ“„ Procesando: Bloque_20201204_a_20201207.shp\n",
      "    âœ… Agregado: 37 features\n",
      "  ğŸ“„ Procesando: Bloque_20201130_a_20201203.shp\n",
      "    âœ… Agregado: 90 features\n",
      "  ğŸ“„ Procesando: Bloque_20201126_a_20201129.shp\n",
      "    âœ… Agregado: 54 features\n",
      "  ğŸ“„ Procesando: Bloque_20201122_a_20201125.shp\n",
      "    âœ… Agregado: 98 features\n",
      "  ğŸ“„ Procesando: Bloque_20201216_a_20201219.shp\n",
      "    âœ… Agregado: 46 features\n",
      "  ğŸ“„ Procesando: Bloque_20201220_a_20201223.shp\n",
      "    âœ… Agregado: 279 features\n",
      "  ğŸ“„ Procesando: Bloque_20201224_a_20201227.shp\n",
      "    âœ… Agregado: 59 features\n",
      "  ğŸ“„ Procesando: Bloque_20201228_a_20201231.shp\n",
      "    âœ… Agregado: 20 features\n",
      "ğŸ”„ Combinando GeoDataFrames...\n",
      "âœ… Merge completado: 8594 features totales\n",
      "\n",
      "ğŸ“Š EstadÃ­sticas iniciales:\n",
      "  - Features totales: 8594\n",
      "  - CRS: EPSG:4326\n",
      "\n",
      "ğŸ”„ Paso 1: Calculando Ã¡rea y filtrando polÃ­gonos < 0.1 ha...\n",
      "ğŸ”„ Calculando Ã¡rea en hectÃ¡reas...\n",
      "  ğŸ“Š Ãrea total inicial: 937.83 ha\n",
      "  ğŸ“Š PolÃ­gonos < 0.1 ha: 7108\n",
      "  âœ… PolÃ­gonos despuÃ©s del filtro: 1486\n",
      "  ğŸ“Š Ãrea total despuÃ©s del filtro: 709.85 ha\n",
      "\n",
      "ğŸ”„ Paso 2: Dissolve por fecha_obje...\n",
      "  ğŸ“Š Fechas Ãºnicas: 70\n",
      "  âœ… PolÃ­gonos despuÃ©s del dissolve: 1057\n",
      "\n",
      "ğŸ”„ Paso 3: Recalculando Ã¡rea y filtrando < 0.1 ha...\n",
      "ğŸ”„ Calculando Ã¡rea en hectÃ¡reas...\n",
      "  ğŸ“Š Ãrea total antes del segundo filtro: 373.68 ha\n",
      "  ğŸ“Š PolÃ­gonos < 0.1 ha: 303\n",
      "  âœ… PolÃ­gonos despuÃ©s del segundo filtro: 754\n",
      "  ğŸ“Š Ãrea total despuÃ©s del segundo filtro: 360.05 ha\n",
      "\n",
      "ğŸ”„ Paso 4: Aplicando buffer de 0.1 metros...\n",
      "\n",
      "ğŸ”„ Paso 5: Recalculando Ã¡rea final...\n",
      "ğŸ”„ Calculando Ã¡rea en hectÃ¡reas...\n",
      "  âœ… Ãrea total final: 362.88 ha\n",
      "  ğŸ“Š PolÃ­gonos finales: 754\n",
      "\n",
      "ğŸ’¾ Guardando resultado en: G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\n",
      "âœ… Procesamiento completado exitosamente!\n",
      "\n",
      "ğŸ“Š RESUMEN FINAL:\n",
      "  - PolÃ­gonos procesados: 754\n",
      "  - Ãrea total: 362.88 ha\n",
      "  - Fechas Ãºnicas: 69\n",
      "  - Rango de fechas: 2020-01-02 a 2020-12-29\n",
      "  - Ãrea promedio por polÃ­gono: 0.48 ha\n",
      "  - Ãrea mÃ¡xima: 7.04 ha\n",
      "  - Ãrea mÃ­nima: 0.11 ha\n",
      "\n",
      "ğŸ‰ Â¡Procesamiento completado exitosamente!\n",
      "ğŸ“ Archivo guardado en: G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd \n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def merge_shp_files(input_folder):\n",
    "    \"\"\"\n",
    "    FunciÃ³n para hacer merge de todos los archivos SHP en una carpeta\n",
    "    manteniendo solo los campos fecha_obje y fecha_prev\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Iniciando merge de archivos SHP...\")\n",
    "    \n",
    "    # Buscar todos los archivos .shp en la carpeta\n",
    "    shp_files = list(Path(input_folder).glob(\"*.shp\"))\n",
    "    \n",
    "    if not shp_files:\n",
    "        print(\"âŒ No se encontraron archivos SHP en la carpeta especificada\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“ Encontrados {len(shp_files)} archivos SHP\")\n",
    "    \n",
    "    # Lista para almacenar los GeoDataFrames\n",
    "    gdfs = []\n",
    "    \n",
    "    for shp_file in shp_files:\n",
    "        try:\n",
    "            print(f\"  ğŸ“„ Procesando: {shp_file.name}\")\n",
    "            gdf = gpd.read_file(shp_file)\n",
    "            \n",
    "            # Verificar si las columnas existen\n",
    "            if 'fecha_obje' in gdf.columns and 'fecha_prev' in gdf.columns:\n",
    "                # Mantener solo las columnas necesarias y la geometrÃ­a\n",
    "                gdf_filtered = gdf[['fecha_obje', 'fecha_prev', 'geometry']].copy()\n",
    "                gdfs.append(gdf_filtered)\n",
    "                print(f\"    âœ… Agregado: {len(gdf_filtered)} features\")\n",
    "            else:\n",
    "                print(f\"    âš ï¸  Columnas faltantes en {shp_file.name}\")\n",
    "                print(f\"    Columnas disponibles: {list(gdf.columns)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Error procesando {shp_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not gdfs:\n",
    "        print(\"âŒ No se pudieron procesar archivos SHP vÃ¡lidos\")\n",
    "        return None\n",
    "    \n",
    "    # Combinar todos los GeoDataFrames\n",
    "    print(\"ğŸ”„ Combinando GeoDataFrames...\")\n",
    "    merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "    \n",
    "    # Asegurar que el CRS sea consistente\n",
    "    if merged_gdf.crs is None:\n",
    "        print(\"âš ï¸  CRS no definido, asignando EPSG:4326\")\n",
    "        merged_gdf = merged_gdf.set_crs('EPSG:4326')\n",
    "    \n",
    "    print(f\"âœ… Merge completado: {len(merged_gdf)} features totales\")\n",
    "    return merged_gdf\n",
    "\n",
    "def calculate_area_hectares(gdf):\n",
    "    \"\"\"\n",
    "    Calcular Ã¡rea en hectÃ¡reas\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Calculando Ã¡rea en hectÃ¡reas...\")\n",
    "    \n",
    "    # Si estÃ¡ en coordenadas geogrÃ¡ficas, proyectar a un sistema mÃ©trico\n",
    "    if gdf.crs.is_geographic:\n",
    "        # Proyectar a Web Mercator para cÃ¡lculo de Ã¡rea\n",
    "        gdf_proj = gdf.to_crs('EPSG:3857')\n",
    "        area_m2 = gdf_proj.geometry.area\n",
    "    else:\n",
    "        area_m2 = gdf.geometry.area\n",
    "    \n",
    "    # Convertir a hectÃ¡reas (1 ha = 10,000 mÂ²)\n",
    "    area_ha = area_m2 / 10000\n",
    "    \n",
    "    return area_ha\n",
    "\n",
    "def process_burned_areas(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    FunciÃ³n principal para procesar las Ã¡reas quemadas siguiendo el workflow especificado\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Iniciando procesamiento de Ã¡reas quemadas\")\n",
    "    print(f\"ğŸ“‚ Carpeta de entrada: {input_folder}\")\n",
    "    print(f\"ğŸ’¾ Archivo de salida: {output_file}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Paso 1: Merge de archivos SHP\n",
    "    merged_gdf = merge_shp_files(input_folder)\n",
    "    if merged_gdf is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“Š EstadÃ­sticas iniciales:\")\n",
    "    print(f\"  - Features totales: {len(merged_gdf)}\")\n",
    "    print(f\"  - CRS: {merged_gdf.crs}\")\n",
    "    \n",
    "    # Paso 2: Crear campo de Ã¡rea y eliminar polÃ­gonos < 0.1 ha\n",
    "    print(\"\\nğŸ”„ Paso 1: Calculando Ã¡rea y filtrando polÃ­gonos < 0.1 ha...\")\n",
    "    merged_gdf['area_ha'] = calculate_area_hectares(merged_gdf)\n",
    "    \n",
    "    # Mostrar estadÃ­sticas antes del filtro\n",
    "    print(f\"  ğŸ“Š Ãrea total inicial: {merged_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  ğŸ“Š PolÃ­gonos < 0.1 ha: {len(merged_gdf[merged_gdf['area_ha'] < 0.1])}\")\n",
    "    \n",
    "    # Filtrar polÃ­gonos >= 0.1 ha\n",
    "    filtered_gdf = merged_gdf[merged_gdf['area_ha'] >= 0.1].copy()\n",
    "    print(f\"  âœ… PolÃ­gonos despuÃ©s del filtro: {len(filtered_gdf)}\")\n",
    "    print(f\"  ğŸ“Š Ãrea total despuÃ©s del filtro: {filtered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    \n",
    "    # Paso 3: Dissolve por fecha_obje sin crear multiparts\n",
    "    print(\"\\nğŸ”„ Paso 2: Dissolve por fecha_obje...\")\n",
    "    print(f\"  ğŸ“Š Fechas Ãºnicas: {filtered_gdf['fecha_obje'].nunique()}\")\n",
    "    \n",
    "    # Realizar dissolve por fecha_obje\n",
    "    dissolved_gdf = filtered_gdf.dissolve(by='fecha_obje', as_index=False)\n",
    "    \n",
    "    # Explotar multiparts si existen\n",
    "    dissolved_gdf = dissolved_gdf.explode(index_parts=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  âœ… PolÃ­gonos despuÃ©s del dissolve: {len(dissolved_gdf)}\")\n",
    "    \n",
    "    # Paso 4: Recalcular Ã¡rea y eliminar polÃ­gonos < 0.1 ha otra vez\n",
    "    print(\"\\nğŸ”„ Paso 3: Recalculando Ã¡rea y filtrando < 0.1 ha...\")\n",
    "    dissolved_gdf['area_ha'] = calculate_area_hectares(dissolved_gdf)\n",
    "    \n",
    "    # Mostrar estadÃ­sticas antes del segundo filtro\n",
    "    print(f\"  ğŸ“Š Ãrea total antes del segundo filtro: {dissolved_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  ğŸ“Š PolÃ­gonos < 0.1 ha: {len(dissolved_gdf[dissolved_gdf['area_ha'] < 0.1])}\")\n",
    "    \n",
    "    # Segundo filtro\n",
    "    final_filtered_gdf = dissolved_gdf[dissolved_gdf['area_ha'] >= 0.1].copy()\n",
    "    print(f\"  âœ… PolÃ­gonos despuÃ©s del segundo filtro: {len(final_filtered_gdf)}\")\n",
    "    print(f\"  ğŸ“Š Ãrea total despuÃ©s del segundo filtro: {final_filtered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    \n",
    "    # Paso 5: Aplicar buffer de 0.1 metros\n",
    "    print(\"\\nğŸ”„ Paso 4: Aplicando buffer de 0.1 metros...\")\n",
    "    \n",
    "    # Si estÃ¡ en coordenadas geogrÃ¡ficas, proyectar temporalmente\n",
    "    if final_filtered_gdf.crs.is_geographic:\n",
    "        # Proyectar a Web Mercator para el buffer\n",
    "        buffered_gdf = final_filtered_gdf.to_crs('EPSG:3857')\n",
    "        buffered_gdf['geometry'] = buffered_gdf.geometry.buffer(0.1)\n",
    "        # Volver al CRS original\n",
    "        buffered_gdf = buffered_gdf.to_crs(final_filtered_gdf.crs)\n",
    "    else:\n",
    "        buffered_gdf = final_filtered_gdf.copy()\n",
    "        buffered_gdf['geometry'] = buffered_gdf.geometry.buffer(0.1)\n",
    "    \n",
    "    # Paso 6: Recalcular Ã¡rea final\n",
    "    print(\"\\nğŸ”„ Paso 5: Recalculando Ã¡rea final...\")\n",
    "    buffered_gdf['area_ha'] = calculate_area_hectares(buffered_gdf)\n",
    "    \n",
    "    print(f\"  âœ… Ãrea total final: {buffered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  ğŸ“Š PolÃ­gonos finales: {len(buffered_gdf)}\")\n",
    "    \n",
    "    # Paso 7: Guardar resultado\n",
    "    print(f\"\\nğŸ’¾ Guardando resultado en: {output_file}\")\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Guardar archivo\n",
    "    buffered_gdf.to_file(output_file, driver='ESRI Shapefile')\n",
    "    \n",
    "    print(\"âœ… Procesamiento completado exitosamente!\")\n",
    "    \n",
    "    # Mostrar resumen final\n",
    "    print(\"\\nğŸ“Š RESUMEN FINAL:\")\n",
    "    print(f\"  - PolÃ­gonos procesados: {len(buffered_gdf)}\")\n",
    "    print(f\"  - Ãrea total: {buffered_gdf['area_ha'].sum():.2f} ha\")\n",
    "    print(f\"  - Fechas Ãºnicas: {buffered_gdf['fecha_obje'].nunique()}\")\n",
    "    print(f\"  - Rango de fechas: {buffered_gdf['fecha_obje'].min()} a {buffered_gdf['fecha_obje'].max()}\")\n",
    "    print(f\"  - Ãrea promedio por polÃ­gono: {buffered_gdf['area_ha'].mean():.2f} ha\")\n",
    "    print(f\"  - Ãrea mÃ¡xima: {buffered_gdf['area_ha'].max():.2f} ha\")\n",
    "    print(f\"  - Ãrea mÃ­nima: {buffered_gdf['area_ha'].min():.2f} ha\")\n",
    "    \n",
    "    return buffered_gdf\n",
    "\n",
    "# ConfiguraciÃ³n y ejecuciÃ³n\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurar rutas\n",
    "    input_folder = r\"G:\\Mi unidad\\GEE_exports\"\n",
    "    output_file = r\"G:\\Mi unidad\\GEE_exports\\areas_quemadas_procesadas.shp\"\n",
    "    \n",
    "    # Verificar que la carpeta existe\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"âŒ La carpeta {input_folder} no existe\")\n",
    "        print(\"Por favor, verifica la ruta y vuelve a intentar\")\n",
    "    else:\n",
    "        # Ejecutar procesamiento\n",
    "        resultado = process_burned_areas(input_folder, output_file)\n",
    "        \n",
    "        if resultado is not None:\n",
    "            print(f\"\\nğŸ‰ Â¡Procesamiento completado exitosamente!\")\n",
    "            print(f\"ğŸ“ Archivo guardado en: {output_file}\")\n",
    "        else:\n",
    "            print(\"âŒ El procesamiento fallÃ³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion areas quemadas diarias\n",
    "\n",
    "En este codigo se utiliza la logica de bandas en imagenes satelitales para definir poligonos que representen areas quemadas para el dia actual, en caso que no se encuentren imagenes disponibles para el dia se omite el proceso. Tambien se actualiza la capa de incendios detectados en ArcGis Web. El script funciona con autenticacion de google earth engine asi que se tiene que crear un usuario y projecto con acceso a la API de google earth engine en google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arcpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01marcpy\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Inicializar Earth Engine\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'arcpy'"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "import arcpy\n",
    "import requests\n",
    "\n",
    "# Inicializar Earth Engine\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='incendios-461918')\n",
    "\n",
    "def mask_s2_clouds(image):\n",
    "    qa = image.select('QA60')\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    mask_qa = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    scl = image.select('SCL')\n",
    "    mask_scl = (scl.neq(3).And(scl.neq(8)).And(scl.neq(9)).And(scl.neq(10)))\n",
    "    mask_final = mask_qa.And(mask_scl)\n",
    "    return image.updateMask(mask_final)\n",
    "\n",
    "def calcular_ndvi(image):\n",
    "    return image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "\n",
    "def calcular_bais2(image):\n",
    "    b4 = image.select('B4')\n",
    "    b6 = image.select('B6')\n",
    "    b7 = image.select('B7')\n",
    "    b8a = image.select('B8A')\n",
    "    b12 = image.select('B12')\n",
    "    bais2 = image.expression('(1 - sqrt((RE2 * RE3 * N2) / R)) * (sqrt((S2 - N2) / (S2 + N2)) + 1)', {\n",
    "        'RE2': b6, 'RE3': b7, 'N2': b8a, 'R': b4, 'S2': b12\n",
    "    }).rename('BAIS2')\n",
    "    return bais2\n",
    "\n",
    "def calcular_mirbi(image):\n",
    "    return image.expression('10 * (B12 - (B11 + B8A) / 2)', {\n",
    "        'B12': image.select('B12'), 'B11': image.select('B11'), 'B8A': image.select('B8A')\n",
    "    }).rename('MIRBI')\n",
    "\n",
    "def crear_mosaico_por_fecha(coleccion, fecha_inicio, fecha_fin, region):\n",
    "    \"\"\"\n",
    "    Crea un mosaico de todas las imÃ¡genes disponibles en una fecha especÃ­fica\n",
    "    \n",
    "    Args:\n",
    "        coleccion: ImageCollection de Sentinel-2\n",
    "        fecha_inicio: ee.Date de inicio del dÃ­a\n",
    "        fecha_fin: ee.Date de fin del dÃ­a\n",
    "        region: GeometrÃ­a de la regiÃ³n de interÃ©s\n",
    "    \n",
    "    Returns:\n",
    "        ee.Image: Mosaico de las imÃ¡genes del dÃ­a o None si no hay imÃ¡genes\n",
    "    \"\"\"\n",
    "    # Filtrar imÃ¡genes por fecha\n",
    "    imagenes_fecha = (coleccion\n",
    "                     .filterDate(fecha_inicio, fecha_fin)\n",
    "                     .filterBounds(region))\n",
    "    \n",
    "    # Verificar si hay imÃ¡genes\n",
    "    num_imagenes = imagenes_fecha.size()\n",
    "    \n",
    "    try:\n",
    "        num_imagenes_info = num_imagenes.getInfo()\n",
    "        if num_imagenes_info == 0:\n",
    "            return None, 0\n",
    "    except:\n",
    "        return None, 0\n",
    "    \n",
    "    # Crear mosaico (las imÃ¡genes mÃ¡s recientes tendrÃ¡n prioridad)\n",
    "    mosaico = (imagenes_fecha\n",
    "               .sort('system:time_start', False)  # Ordenar por fecha descendente\n",
    "               .mosaic()\n",
    "               .clip(region))\n",
    "    \n",
    "    # Agregar metadatos del mosaico\n",
    "    fecha_str = fecha_inicio.format('YYYY-MM-dd').getInfo()\n",
    "    mosaico = mosaico.set({\n",
    "        'system:time_start': fecha_inicio.millis(),\n",
    "        'fecha_mosaico': fecha_str,\n",
    "        'num_imagenes_mosaico': num_imagenes_info\n",
    "    })\n",
    "    \n",
    "    return mosaico, num_imagenes_info\n",
    "\n",
    "def obtener_fechas_unicas_previas(coleccion, fecha_limite, num_fechas_max=10):\n",
    "    \"\"\"\n",
    "    Obtiene las fechas Ãºnicas disponibles en la colecciÃ³n antes de una fecha lÃ­mite\n",
    "    \n",
    "    Args:\n",
    "        coleccion: ImageCollection\n",
    "        fecha_limite: ee.Date lÃ­mite (no incluida)\n",
    "        num_fechas_max: NÃºmero mÃ¡ximo de fechas a retornar\n",
    "    \n",
    "    Returns:\n",
    "        List: Lista de fechas Ãºnicas en formato ee.Date\n",
    "    \"\"\"\n",
    "    # Filtrar imÃ¡genes antes de la fecha lÃ­mite\n",
    "    imagenes_previas = (coleccion\n",
    "                       .filter(ee.Filter.lt('system:time_start', fecha_limite.millis()))\n",
    "                       .sort('system:time_start', False))\n",
    "    \n",
    "    # FunciÃ³n para extraer fecha (YYYY-MM-DD) de una imagen\n",
    "    def extraer_fecha(image):\n",
    "        fecha = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n",
    "        return image.set('fecha_str', fecha)\n",
    "    \n",
    "    # Agregar campo de fecha string\n",
    "    imagenes_con_fecha = imagenes_previas.map(extraer_fecha)\n",
    "    \n",
    "    # Obtener fechas Ãºnicas usando distinct\n",
    "    fechas_unicas = imagenes_con_fecha.distinct('fecha_str').limit(num_fechas_max)\n",
    "    \n",
    "    # Convertir a lista para poder iterar\n",
    "    lista_fechas = fechas_unicas.toList(num_fechas_max)\n",
    "    \n",
    "    # Extraer las fechas como ee.Date\n",
    "    fechas_ee = []\n",
    "    try:\n",
    "        num_fechas = fechas_unicas.size().getInfo()\n",
    "        print(f\"ğŸ“… Encontradas {num_fechas} fechas Ãºnicas para procesar\")\n",
    "        \n",
    "        for i in range(num_fechas):\n",
    "            imagen = ee.Image(lista_fechas.get(i))\n",
    "            fecha_ee = ee.Date(imagen.get('system:time_start'))\n",
    "            # Truncar a inicio del dÃ­a\n",
    "            fecha_inicio_dia = fecha_ee.update(None, None, None, 0, 0, 0)\n",
    "            fechas_ee.append(fecha_inicio_dia)\n",
    "            \n",
    "            # Mostrar fecha para debug\n",
    "            fecha_str = fecha_ee.format('YYYY-MM-dd').getInfo()\n",
    "            print(f\"   ğŸ“† Fecha {i+1}: {fecha_str}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al obtener fechas Ãºnicas: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return fechas_ee\n",
    "\n",
    "# [AquÃ­ van todas las funciones auxiliares sin cambios]\n",
    "def obtener_url_feature_service(item_id, usuario, clave):\n",
    "    \"\"\"\n",
    "    Obtiene la URL correcta del feature service desde ArcGIS Online\n",
    "    \"\"\"\n",
    "    portal_url = \"https://www.arcgis.com\"\n",
    "    \n",
    "    # URL para obtener informaciÃ³n del item\n",
    "    item_info_url = f\"{portal_url}/sharing/rest/content/items/{item_id}\"\n",
    "    \n",
    "    # ParÃ¡metros para la solicitud\n",
    "    params = {\n",
    "        'f': 'json',\n",
    "        'token': None  # Se obtendrÃ¡ el token despuÃ©s\n",
    "    }\n",
    "    \n",
    "    # Obtener token de autenticaciÃ³n\n",
    "    token_url = f\"{portal_url}/sharing/rest/generateToken\"\n",
    "    token_params = {\n",
    "        'username': usuario,\n",
    "        'password': clave,\n",
    "        'referer': portal_url,\n",
    "        'f': 'json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Solicitar token\n",
    "        token_response = requests.post(token_url, data=token_params)\n",
    "        token_data = token_response.json()\n",
    "        \n",
    "        if 'token' not in token_data:\n",
    "            raise Exception(f\"Error al obtener token: {token_data}\")\n",
    "        \n",
    "        token = token_data['token']\n",
    "        params['token'] = token\n",
    "        \n",
    "        # Obtener informaciÃ³n del item\n",
    "        item_response = requests.get(item_info_url, params=params)\n",
    "        item_data = item_response.json()\n",
    "        \n",
    "        if 'url' not in item_data:\n",
    "            raise Exception(f\"No se encontrÃ³ URL en el item: {item_data}\")\n",
    "        \n",
    "        # Construir URL del feature service\n",
    "        base_url = item_data['url']\n",
    "        if not base_url.endswith('/'):\n",
    "            base_url += '/'\n",
    "        \n",
    "        feature_service_url = f\"{base_url}0\"  # Capa 0\n",
    "        \n",
    "        return feature_service_url, token\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error al obtener URL del feature service: {e}\")\n",
    "\n",
    "def limpiar_archivos_temporales(output_folder_local, nombre_shp):\n",
    "    \"\"\"\n",
    "    Elimina los archivos temporales generados durante el procesamiento\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ Limpiando archivos temporales...\")\n",
    "    \n",
    "    # Lista de archivos a eliminar\n",
    "    archivos_a_eliminar = [\n",
    "        nombre_shp,  # Archivo original\n",
    "        nombre_shp.replace('.shp', '_filtrado.shp'),\n",
    "        nombre_shp.replace('.shp', '_procesado.shp'),\n",
    "        nombre_shp.replace('.shp', '_filtrado_2.shp'),\n",
    "        nombre_shp.replace('.shp', '_buffered.shp')\n",
    "    ]\n",
    "    \n",
    "    for archivo_shp in archivos_a_eliminar:\n",
    "        try:\n",
    "            archivo_path = os.path.join(output_folder_local, archivo_shp)\n",
    "            \n",
    "            # Eliminar todos los archivos relacionados con el shapefile\n",
    "            extensiones = ['.shp', '.shx', '.dbf', '.prj', '.cpg', '.xml']\n",
    "            \n",
    "            for ext in extensiones:\n",
    "                archivo_completo = archivo_path.replace('.shp', ext)\n",
    "                if os.path.exists(archivo_completo):\n",
    "                    os.remove(archivo_completo)\n",
    "                    print(f\"   ğŸ—‘ï¸ Eliminado: {os.path.basename(archivo_completo)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ No se pudo eliminar {archivo_shp}: {e}\")\n",
    "    \n",
    "    print(\"âœ… Limpieza de archivos temporales completada\")\n",
    "\n",
    "def agregar_a_arcgis_web(shp_buffered, output_folder_local, nombre_shp):\n",
    "    \"\"\"\n",
    "    FunciÃ³n para agregar shapefile a ArcGIS Online con pausas para estabilidad\n",
    "    \"\"\"\n",
    "    portal_url = \"https://www.arcgis.com/\"\n",
    "    usuario = \"unidad.gestion.riesgo\"\n",
    "    clave = \"XXXXXXX\"\n",
    "    item_id = \"7b561988344046d4827141dcf8014b25\"\n",
    "    \n",
    "    print(\"ğŸ” Iniciando sesiÃ³n en ArcGIS Online...\")\n",
    "    arcpy.SignInToPortal(portal_url, usuario, clave)\n",
    "    \n",
    "    # Pausa despuÃ©s del login\n",
    "    time.sleep(3)\n",
    "    print(\"âœ… SesiÃ³n iniciada correctamente\")\n",
    "    \n",
    "    # Obtener URL correcta del feature service\n",
    "    try:\n",
    "        feature_service_url, token = obtener_url_feature_service(item_id, usuario, clave)\n",
    "        print(f\"ğŸ”— Feature layer URL detectado: {feature_service_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al obtener URL del servicio: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Pausa antes de verificar el shapefile\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Verificar que el shapefile estÃ© completamente disponible\n",
    "    print(\"ğŸ“‹ Verificando disponibilidad del shapefile...\")\n",
    "    if not arcpy.Exists(shp_buffered):\n",
    "        raise Exception(f\"El shapefile {shp_buffered} no existe o no estÃ¡ disponible\")\n",
    "    \n",
    "    # Pausa adicional para asegurar que el archivo estÃ© completamente creado\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ“¤ Iniciando carga de datos al servicio web...\")\n",
    "        arcpy.management.Append(inputs=shp_buffered,\n",
    "                                 target=feature_service_url,\n",
    "                                 schema_type=\"NO_TEST\",\n",
    "                                 field_mapping=\"\",\n",
    "                                 subtype=\"\",\n",
    "                                 expression=\"\")\n",
    "        \n",
    "        # Pausa despuÃ©s de la carga\n",
    "        time.sleep(3)\n",
    "        print(\"âœ… Datos agregados exitosamente al servicio web\")\n",
    "        \n",
    "        # Limpiar archivos temporales despuÃ©s de subida exitosa\n",
    "        limpiar_archivos_temporales(output_folder_local, nombre_shp)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al agregar los datos al feature layer: {e}\")\n",
    "        raise\n",
    "\n",
    "def esperar_archivo_disponible(archivo_path, max_intentos=30, pausa_entre_intentos=2):\n",
    "    \"\"\"\n",
    "    Espera hasta que el archivo estÃ© completamente disponible para ArcGIS\n",
    "    \"\"\"\n",
    "    for intento in range(max_intentos):\n",
    "        try:\n",
    "            # Verificar existencia bÃ¡sica\n",
    "            if not os.path.exists(archivo_path):\n",
    "                print(f\"âŒ› Intento {intento + 1}: Archivo no existe aÃºn...\")\n",
    "                time.sleep(pausa_entre_intentos)\n",
    "                continue\n",
    "            \n",
    "            # Verificar que ArcGIS pueda acceder al archivo\n",
    "            if arcpy.Exists(archivo_path):\n",
    "                # Intentar describir el archivo para verificar acceso completo\n",
    "                desc = arcpy.Describe(archivo_path)\n",
    "                if desc.dataType == 'ShapeFile':\n",
    "                    print(f\"âœ… Archivo disponible despuÃ©s de {intento + 1} intentos\")\n",
    "                    return True\n",
    "            \n",
    "            print(f\"âŒ› Intento {intento + 1}: Archivo no completamente disponible...\")\n",
    "            time.sleep(pausa_entre_intentos)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ› Intento {intento + 1}: Error de acceso - {e}\")\n",
    "            time.sleep(pausa_entre_intentos)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def post_procesar_shapefile(output_folder_local, nombre_shp):\n",
    "    \"\"\"\n",
    "    FunciÃ³n para postprocesar shapefile con pausas entre operaciones\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ Iniciando postprocesamiento del shapefile...\")\n",
    "    \n",
    "    # Configurar entorno ArcGIS\n",
    "    arcpy.env.workspace = output_folder_local\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    \n",
    "    # Definir rutas de archivos\n",
    "    shp_original = os.path.join(output_folder_local, nombre_shp)\n",
    "    shp_filtrado = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_filtrado.shp'))\n",
    "    shp_dissolved = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_procesado.shp'))\n",
    "    shp_filtrado_2 = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_filtrado_2.shp'))\n",
    "    shp_buffered = os.path.join(output_folder_local, nombre_shp.replace('.shp', '_buffered.shp'))\n",
    "\n",
    "    # Verificar que el archivo original estÃ© completamente disponible\n",
    "    print(\"ğŸ” Verificando disponibilidad del archivo original...\")\n",
    "    if not esperar_archivo_disponible(shp_original):\n",
    "        raise Exception(f\"El archivo {shp_original} no estÃ¡ disponible despuÃ©s de mÃºltiples intentos\")\n",
    "    \n",
    "    # Pausa adicional para estabilidad\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Paso 1: Agregar y calcular campo Area_ha en shapefile original\n",
    "    print(\"ğŸ“ Paso 1: Calculando Ã¡rea inicial...\")\n",
    "    try:\n",
    "        # Verificar campos existentes\n",
    "        campos_existentes = [f.name for f in arcpy.ListFields(shp_original)]\n",
    "        if 'Area_ha' not in campos_existentes:\n",
    "            print(\"   â• Agregando campo Area_ha...\")\n",
    "            arcpy.AddField_management(shp_original, 'Area_ha', 'DOUBLE')\n",
    "            time.sleep(3)  # Pausa despuÃ©s de agregar campo\n",
    "            print(\"   âœ… Campo Area_ha agregado\")\n",
    "        else:\n",
    "            print(\"   â„¹ï¸ Campo Area_ha ya existe\")\n",
    "        \n",
    "        print(\"   ğŸ“ Calculando geometrÃ­a...\")\n",
    "        arcpy.CalculateGeometryAttributes_management(shp_original, [[\"Area_ha\", \"AREA_GEODESIC\"]], area_unit=\"HECTARES\")\n",
    "        time.sleep(4)  # Pausa despuÃ©s del cÃ¡lculo de geometrÃ­a\n",
    "        print(\"âœ… Ãrea inicial calculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 1: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 2: Filtrar por Ã¡rea >= 0.1 hectÃ¡reas\n",
    "    print(\"ğŸ” Paso 2: Filtrando por Ã¡rea >= 0.1 hectÃ¡reas...\")\n",
    "    try:\n",
    "        arcpy.MakeFeatureLayer_management(shp_original, 'temp_layer', '\"Area_ha\" >= 0.1')\n",
    "        time.sleep(3)  # Pausa despuÃ©s de crear layer temporal\n",
    "        \n",
    "        arcpy.CopyFeatures_management('temp_layer', shp_filtrado)\n",
    "        time.sleep(4)  # Pausa despuÃ©s de copiar features\n",
    "        \n",
    "        arcpy.Delete_management('temp_layer')\n",
    "        time.sleep(2)  # Pausa despuÃ©s de eliminar layer temporal\n",
    "        print(\"âœ… Filtrado inicial completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 2: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 3: Dissolver polÃ­gonos\n",
    "    print(\"ğŸ”„ Paso 3: Disolviendo polÃ­gonos...\")\n",
    "    try:\n",
    "        arcpy.Dissolve_management(shp_filtrado, shp_dissolved, dissolve_field='fecha_obje', multi_part='SINGLE_PART')\n",
    "        time.sleep(8)  # Pausa mÃ¡s larga despuÃ©s del dissolve (operaciÃ³n compleja)\n",
    "        print(\"âœ… Dissolve completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 3: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 4: Recalcular Ã¡rea despuÃ©s del dissolve\n",
    "    print(\"ğŸ“ Paso 4: Recalculando Ã¡rea despuÃ©s del dissolve...\")\n",
    "    try:\n",
    "        # Verificar campos existentes\n",
    "        campos_existentes = [f.name for f in arcpy.ListFields(shp_dissolved)]\n",
    "        if 'Area_ha' not in campos_existentes:\n",
    "            print(\"   â• Agregando campo Area_ha...\")\n",
    "            arcpy.AddField_management(shp_dissolved, 'Area_ha', 'DOUBLE')\n",
    "            time.sleep(3)  # Pausa despuÃ©s de agregar campo\n",
    "            print(\"   âœ… Campo Area_ha agregado\")\n",
    "        else:\n",
    "            print(\"   â„¹ï¸ Campo Area_ha ya existe\")\n",
    "        \n",
    "        print(\"   ğŸ“ Calculando geometrÃ­a...\")\n",
    "        arcpy.CalculateGeometryAttributes_management(shp_dissolved, [[\"Area_ha\", \"AREA_GEODESIC\"]], area_unit=\"HECTARES\")\n",
    "        time.sleep(4)  # Pausa despuÃ©s del cÃ¡lculo de geometrÃ­a\n",
    "        print(\"âœ… Ãrea recalculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 4: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 5: Segundo filtro por Ã¡rea >= 0.1 hectÃ¡reas\n",
    "    print(\"ğŸ” Paso 5: Segundo filtro por Ã¡rea >= 0.1 hectÃ¡reas...\")\n",
    "    try:\n",
    "        arcpy.MakeFeatureLayer_management(shp_dissolved, 'temp_layer_2', '\"Area_ha\" >= 0.1')\n",
    "        time.sleep(3)  # Pausa despuÃ©s de crear layer temporal\n",
    "        \n",
    "        arcpy.CopyFeatures_management('temp_layer_2', shp_filtrado_2)\n",
    "        time.sleep(4)  # Pausa despuÃ©s de copiar features\n",
    "        \n",
    "        arcpy.Delete_management('temp_layer_2')\n",
    "        time.sleep(2)  # Pausa despuÃ©s de eliminar layer temporal\n",
    "        print(\"âœ… Segundo filtrado completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 5: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 6: Crear buffer\n",
    "    print(\"ğŸ“ Paso 6: Creando buffer de 0.1 metros...\")\n",
    "    try:\n",
    "        arcpy.Buffer_analysis(shp_filtrado_2, shp_buffered, \"0.1 Meters\", dissolve_option=\"NONE\", method=\"PLANAR\")\n",
    "        time.sleep(6)  # Pausa despuÃ©s del buffer (operaciÃ³n compleja)\n",
    "        print(\"âœ… Buffer creado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 6: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Paso 7: Calcular Ã¡rea final\n",
    "    print(\"ğŸ“ Paso 7: Calculando Ã¡rea final...\")\n",
    "    try:\n",
    "        # Verificar campos existentes\n",
    "        campos_existentes = [f.name for f in arcpy.ListFields(shp_buffered)]\n",
    "        if 'Area_ha' not in campos_existentes:\n",
    "            print(\"   â• Agregando campo Area_ha...\")\n",
    "            arcpy.AddField_management(shp_buffered, 'Area_ha', 'DOUBLE')\n",
    "            time.sleep(3)  # Pausa despuÃ©s de agregar campo\n",
    "            print(\"   âœ… Campo Area_ha agregado\")\n",
    "        else:\n",
    "            print(\"   â„¹ï¸ Campo Area_ha ya existe\")\n",
    "        \n",
    "        print(\"   ğŸ“ Calculando geometrÃ­a...\")\n",
    "        arcpy.CalculateGeometryAttributes_management(shp_buffered, [[\"Area_ha\", \"AREA_GEODESIC\"]], area_unit=\"HECTARES\")\n",
    "        time.sleep(4)  # Pausa despuÃ©s del cÃ¡lculo de geometrÃ­a\n",
    "        print(\"âœ… Ãrea final calculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en Paso 7: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"ğŸ¯ Procesamiento post-exportaciÃ³n completo: {shp_buffered}\")\n",
    "    \n",
    "    # Pausa final antes de enviar a ArcGIS Online\n",
    "    time.sleep(3)\n",
    "    print(\"ğŸŒ Enviando a ArcGIS Online...\")\n",
    "    \n",
    "    try:\n",
    "        agregar_a_arcgis_web(shp_buffered, output_folder_local, nombre_shp)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al enviar a ArcGIS Online: {e}\")\n",
    "        raise\n",
    "\n",
    "def procesar_imagen_diaria(region_asset_path, export_folder='GEE_exports_diaria'):\n",
    "    \"\"\"\n",
    "    FunciÃ³n principal para procesar la imagen del dÃ­a actual con mosaicos por fecha\n",
    "    \n",
    "    Args:\n",
    "        region_asset_path (str): Ruta al asset de la regiÃ³n en GEE\n",
    "        export_folder (str): Carpeta donde exportar los resultados\n",
    "    \"\"\"\n",
    "    \n",
    "    # ParÃ¡metros\n",
    "    num_fechas_previas = 10  # Cambio: ahora es nÃºmero de fechas Ãºnicas, no imÃ¡genes\n",
    "    umbral_diff_ndvi = -0.3\n",
    "    umbral_ndvi_previa = 0.5\n",
    "    umbral_bais2 = 6\n",
    "    umbral_mirbi = 0\n",
    "    \n",
    "    # Cargar regiÃ³n\n",
    "    try:\n",
    "        region = ee.FeatureCollection(region_asset_path)\n",
    "        print(f\"RegiÃ³n cargada: {region_asset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la regiÃ³n: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Obtener fecha de hoy\n",
    "    fecha_hoy = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "    fecha_hoy_str = fecha_hoy.strftime('%Y-%m-%d')\n",
    "    print(f\"Procesando para la fecha: {fecha_hoy_str}\")\n",
    "    \n",
    "    # Crear fechas para Earth Engine\n",
    "    fecha_ee_inicio = ee.Date(fecha_hoy_str)\n",
    "    fecha_ee_fin = fecha_ee_inicio.advance(1, 'day')\n",
    "    \n",
    "    # Cargar colecciÃ³n Sentinel-2 SIN filtro de nubes para imagen objetivo\n",
    "    coleccion = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "                 .filterBounds(region)\n",
    "                 .map(mask_s2_clouds)\n",
    "                 .map(lambda img: img.clip(region)))\n",
    "    \n",
    "    # Cargar colecciÃ³n Sentinel-2 CON filtro de nubes para imÃ¡genes previas\n",
    "    coleccion_prev = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50))\n",
    "                     .filterBounds(region)\n",
    "                     .map(mask_s2_clouds)\n",
    "                     .map(lambda img: img.clip(region)))\n",
    "    \n",
    "    print(\"Colecciones Sentinel-2 cargadas\")\n",
    "    \n",
    "    # ===== CREAR MOSAICO PARA IMAGEN OBJETIVO =====\n",
    "    print(\"ğŸ–¼ï¸ Creando mosaico para imagen objetivo...\")\n",
    "    mosaico_objetivo, num_img_objetivo = crear_mosaico_por_fecha(\n",
    "        coleccion, fecha_ee_inicio, fecha_ee_fin, region\n",
    "    )\n",
    "    \n",
    "    if mosaico_objetivo is None:\n",
    "        print(f\"âŒ No hay imagen disponible para hoy ({fecha_hoy_str}). No se procesarÃ¡.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"âœ… Mosaico objetivo creado con {num_img_objetivo} imagen(es)\")\n",
    "    \n",
    "    # Calcular Ã­ndices para la imagen objetivo\n",
    "    ndvi_objetivo = calcular_ndvi(mosaico_objetivo)\n",
    "    bais_current = calcular_bais2(mosaico_objetivo)\n",
    "    mirbi_current = calcular_mirbi(mosaico_objetivo)\n",
    "    \n",
    "    # ===== OBTENER FECHAS ÃšNICAS PREVIAS =====\n",
    "    print(\"ğŸ“… Obteniendo fechas Ãºnicas previas...\")\n",
    "    fechas_previas = obtener_fechas_unicas_previas(\n",
    "        coleccion_prev, \n",
    "        fecha_ee_inicio, \n",
    "        num_fechas_previas\n",
    "    )\n",
    "    \n",
    "    if len(fechas_previas) == 0:\n",
    "        print(\"âŒ No se encontraron fechas previas para comparar\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"âœ… Se procesarÃ¡n {len(fechas_previas)} fechas Ãºnicas\")\n",
    "    \n",
    "    # ===== PROCESAR CADA FECHA PREVIA =====\n",
    "    vectores_finales = []\n",
    "    \n",
    "    for j, fecha_previa in enumerate(fechas_previas):\n",
    "        print(f\"\\nğŸ”„ Procesando fecha previa {j+1}/{len(fechas_previas)}\")\n",
    "        \n",
    "        try:\n",
    "            # Crear fechas de inicio y fin del dÃ­a\n",
    "            fecha_prev_inicio = fecha_previa\n",
    "            fecha_prev_fin = fecha_previa.advance(1, 'day')\n",
    "            \n",
    "            # Crear mosaico para esta fecha\n",
    "            print(\"   ğŸ–¼ï¸ Creando mosaico para fecha previa...\")\n",
    "            mosaico_prev, num_img_prev = crear_mosaico_por_fecha(\n",
    "                coleccion_prev, fecha_prev_inicio, fecha_prev_fin, region\n",
    "            )\n",
    "            \n",
    "            if mosaico_prev is None:\n",
    "                print(\"   âŒ No se pudo crear mosaico para esta fecha\")\n",
    "                continue\n",
    "            \n",
    "            fecha_prev_str = fecha_prev_inicio.format('YYYY-MM-dd').getInfo()\n",
    "            print(f\"   âœ… Mosaico creado con {num_img_prev} imagen(es) para {fecha_prev_str}\")\n",
    "            \n",
    "            # Calcular NDVI para mosaico previo\n",
    "            ndvi_prev = calcular_ndvi(mosaico_prev)\n",
    "            \n",
    "            # Calcular diferencia NDVI\n",
    "            diff_ndvi = ndvi_objetivo.subtract(ndvi_prev).rename('Diferencia_NDVI')\n",
    "            \n",
    "            # Crear mÃ¡scara binaria\n",
    "            zonas = diff_ndvi.lt(umbral_diff_ndvi).selfMask()\n",
    "            zonas_clip = zonas.clip(region)\n",
    "            \n",
    "            # Reducir a vectores\n",
    "            vectores = zonas_clip.reduceToVectors(\n",
    "                geometry=region,\n",
    "                scale=12.5,\n",
    "                geometryType='polygon',\n",
    "                eightConnected=True,\n",
    "                labelProperty='zone',\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "            \n",
    "            # Verificar si se generaron vectores\n",
    "            num_vectores = vectores.size().getInfo()\n",
    "            if num_vectores == 0:\n",
    "                print(f\"   âŒ No se generaron vectores iniciales para {fecha_prev_str}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   âœ… Se generaron {num_vectores} vectores iniciales para {fecha_prev_str}\")\n",
    "            \n",
    "            # Unir polÃ­gonos\n",
    "            union_poligono = vectores.geometry()\n",
    "            poligono_current = ee.FeatureCollection([ee.Feature(union_poligono)])\n",
    "            \n",
    "            # DEBUG: Mostrar valores de los Ã­ndices antes del filtrado\n",
    "            print(f\"   ğŸ“Š Evaluando filtros combinados:\")\n",
    "            \n",
    "            # Aplicar condiciÃ³n combinada CON DEBUG\n",
    "            mask_bais2 = bais_current.lt(umbral_bais2)\n",
    "            mask_mirbi = mirbi_current.gt(umbral_mirbi)\n",
    "            mask_ndvi_prev = ndvi_prev.gt(umbral_ndvi_previa)\n",
    "            \n",
    "            # Verificar cada mÃ¡scara por separado\n",
    "            area_bais2 = mask_bais2.clip(poligono_current).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            area_mirbi = mask_mirbi.clip(poligono_current).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            area_ndvi = mask_ndvi_prev.clip(poligono_current).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            print(f\"      - BAIS2 < {umbral_bais2}: {area_bais2}\")\n",
    "            print(f\"      - MIRBI > {umbral_mirbi}: {area_mirbi}\")\n",
    "            print(f\"      - NDVI_prev > {umbral_ndvi_previa}: {area_ndvi}\")\n",
    "            \n",
    "            # Combinar mÃ¡scaras\n",
    "            combined_mask = (mask_bais2.Or(mask_mirbi)\n",
    "                            .And(mask_ndvi_prev)\n",
    "                            .selfMask()\n",
    "                            .rename('Combined_mask'))\n",
    "            \n",
    "            # Recortar mÃ¡scara\n",
    "            image_clip_union = combined_mask.clip(poligono_current)\n",
    "            \n",
    "            # Verificar si la mÃ¡scara combinada tiene pÃ­xeles\n",
    "            area_combinada = image_clip_union.reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=poligono_current.geometry(),\n",
    "                scale=12.5,\n",
    "                maxPixels=1e8\n",
    "            ).getInfo()\n",
    "            \n",
    "            print(f\"      - MÃ¡scara combinada: {area_combinada}\")\n",
    "            \n",
    "            if area_combinada.get('Combined_mask', 0) == 0:\n",
    "                print(f\"   âš ï¸ La mÃ¡scara combinada estÃ¡ vacÃ­a para {fecha_prev_str}\")\n",
    "                print(f\"   ğŸ’¡ Sugerencia: Ajustar umbrales - BAIS2:{umbral_bais2}, MIRBI:{umbral_mirbi}, NDVI:{umbral_ndvi_previa}\")\n",
    "                continue\n",
    "            \n",
    "            # Reducir a vectores finales\n",
    "            vectores_final = image_clip_union.reduceToVectors(\n",
    "                geometry=region,\n",
    "                scale=12.5,\n",
    "                geometryType='polygon',\n",
    "                eightConnected=True,\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "            \n",
    "            # Verificar vectores finales\n",
    "            num_vectores_final = vectores_final.size().getInfo()\n",
    "            if num_vectores_final == 0:\n",
    "                print(f\"   âš ï¸ No se generaron vectores finales para {fecha_prev_str}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   âœ… Se generaron {num_vectores_final} vectores finales para {fecha_prev_str}\")\n",
    "            \n",
    "            # Agregar metadatos\n",
    "            vectores_final = vectores_final.map(lambda feature: feature.set({\n",
    "                'fecha_objetivo': fecha_hoy_str,\n",
    "                'fecha_previa': fecha_prev_str,\n",
    "                'indice_comparacion': j,\n",
    "                'num_img_objetivo': num_img_objetivo,\n",
    "                'num_img_previa': num_img_prev\n",
    "            }))\n",
    "            \n",
    "            vectores_finales.append(vectores_final)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error procesando fecha previa {j+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combinar todos los vectores\n",
    "    if vectores_finales:\n",
    "        vectores_combinados = ee.FeatureCollection(vectores_finales).flatten()\n",
    "        \n",
    "        # Verificar si hay resultados\n",
    "        num_resultados = vectores_combinados.size().getInfo()\n",
    "        if num_resultados > 0:\n",
    "            # Exportar\n",
    "            fecha_limpia = fecha_hoy_str.replace('-', '_')\n",
    "            descripcion = f'Vectores_Mosaicos_{fecha_limpia}'\n",
    "            \n",
    "            task = ee.batch.Export.table.toDrive(\n",
    "                collection=vectores_combinados,\n",
    "                description=descripcion,\n",
    "                folder=export_folder,\n",
    "                fileFormat='SHP'\n",
    "            )\n",
    "            \n",
    "            task.start()\n",
    "            print(f\"\\nğŸš€ ExportaciÃ³n iniciada: {descripcion}\")\n",
    "            print(f\"ğŸ“Š NÃºmero total de features a exportar: {num_resultados}\")\n",
    "            print(f\"ğŸ“… Fechas procesadas: {len(fechas_previas)}\")\n",
    "            \n",
    "            # Esperar que el usuario sincronice el archivo SHP en su carpeta local\n",
    "            nombre_shp = f'Vectores_Mosaicos_{fecha_limpia}.shp'\n",
    "            carpeta_local = r\"G:\\Mi unidad\\GEE_exports_diaria\"\n",
    "\n",
    "            print(\"\\nâ³ Esperando a que el archivo estÃ© disponible localmente para su procesamiento...\")\n",
    "\n",
    "            # Esperar a que aparezca el archivo SHP en la carpeta\n",
    "            shp_path = os.path.join(carpeta_local, nombre_shp)\n",
    "            espera_max = 300  # 5 minutos\n",
    "            tiempo_espera = 0\n",
    "\n",
    "            while not os.path.exists(shp_path) and tiempo_espera < espera_max:\n",
    "                time.sleep(10)\n",
    "                tiempo_espera += 10\n",
    "                print(f\"   âŒ› Esperando archivo: {nombre_shp} ({tiempo_espera}s)\")\n",
    "\n",
    "            if os.path.exists(shp_path):\n",
    "                try:\n",
    "                    # Agregar metadatos al shapefile antes del procesamiento\n",
    "                    print(f\"\\nğŸ“‹ Archivo encontrado, iniciando postprocesamiento...\")\n",
    "                    post_procesar_shapefile(carpeta_local, nombre_shp)\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error durante el postprocesamiento: {e}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ No se encontrÃ³ el archivo {nombre_shp} despuÃ©s de {espera_max} segundos.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ No se encontraron Ã¡reas de cambio para exportar\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"âŒ No se generaron vectores para ninguna fecha previa\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    FunciÃ³n principal para ejecutar el anÃ¡lisis diario con mosaicos\n",
    "    \"\"\"\n",
    "    # ConfiguraciÃ³n\n",
    "    REGION_ASSET_PATH = 'projects/ee-ezabaleta/assets/Zona'  # Cambia por tu ruta\n",
    "    EXPORT_FOLDER = 'GEE_exports_diaria'  # Carpeta en Google Drive\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ANÃLISIS DIARIO DE CAMBIOS CON MOSAICOS - GOOGLE EARTH ENGINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ”§ Funcionalidades nuevas:\")\n",
    "    print(\"   â€¢ Mosaicos automÃ¡ticos por fecha (mÃºltiples imÃ¡genes â†’ 1 mosaico)\")\n",
    "    print(\"   â€¢ Fechas Ãºnicas (sin duplicados)\")\n",
    "    print(\"   â€¢ Mejor cobertura espacial\")\n",
    "    print(\"   â€¢ Metadatos de nÃºmero de imÃ¡genes por mosaico\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        resultado = procesar_imagen_diaria(REGION_ASSET_PATH, EXPORT_FOLDER)\n",
    "        \n",
    "        if resultado:\n",
    "            print(\"\\nâœ… Procesamiento completado exitosamente\")\n",
    "            print(\"ğŸ¯ Beneficios obtenidos:\")\n",
    "            print(\"   â€¢ Mayor cobertura temporal (mÃ¡s fechas Ãºnicas)\")\n",
    "            print(\"   â€¢ Mejor cobertura espacial (mosaicos completos)\")\n",
    "            print(\"   â€¢ EliminaciÃ³n de duplicados por fecha\")\n",
    "            print(\"   â€¢ Metadatos detallados de composiciÃ³n de mosaicos\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Procesamiento terminado sin resultados\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error durante el procesamiento: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script calculo Severidad por poligonos\n",
    "\n",
    "Este codigo realiza el calculo de severidad en uno o varios poligonos cargados utilizando la informacion de las bandas en imagenes sentinel, este script guarda la informacion en rasters independientes para cada id de poligono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import gcd\n",
    "import ee\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import os\n",
    "import arcpy\n",
    "import geopandas as gpd\n",
    "\n",
    "# Inicializar Earth Engine\n",
    "ee.Authenticate(force=True)\n",
    "ee.Initialize(project='incendios-461918')\n",
    "\n",
    "# Cargar capa de polÃ­gonos (GeoJSON, Shapefile, etc.)\n",
    "gdf = gpd.read_file(\"E:\\ERLUAN\\Amenas_Incendios\\Vectores\\Finales_puntos\\Vectores_prueba_severidad.shp\")  # AsegÃºrate que tenga columna 'fecha' y geometrÃ­a\n",
    "\n",
    "# ParÃ¡metros\n",
    "dias_rango = 60\n",
    "max_nubes = 50\n",
    "max_nubes_poligono = 10  # Porcentaje mÃ¡ximo de nubes permitido dentro del polÃ­gono\n",
    "\n",
    "# FunciÃ³n para enmascarar nubes\n",
    "def maskS2sr(image):\n",
    "    qa = image.select('QA60')\n",
    "    cloudBitMask = (1 << 10)\n",
    "    cirrusBitMask = (1 << 11)\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "    return image.updateMask(mask).divide(10000).copyProperties(image, image.propertyNames())\n",
    "\n",
    "# FunciÃ³n para verificar nubes dentro del polÃ­gono\n",
    "def check_clouds_in_polygon(image, geometry):\n",
    "    qa = image.select('QA60')\n",
    "    cloudBitMask = (1 << 10)\n",
    "    cirrusBitMask = (1 << 11)\n",
    "    \n",
    "    # Crear mÃ¡scara de nubes y cirrus\n",
    "    cloud_mask = qa.bitwiseAnd(cloudBitMask).neq(0).Or(qa.bitwiseAnd(cirrusBitMask).neq(0))\n",
    "    \n",
    "    # Calcular el porcentaje de nubes en el polÃ­gono\n",
    "    area_total = ee.Image.pixelArea().reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=geometry,\n",
    "        scale=10,\n",
    "        maxPixels=1e13\n",
    "    ).get('area')\n",
    "    \n",
    "    area_nubes = ee.Image.pixelArea().updateMask(cloud_mask).reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=geometry,\n",
    "        scale=10,\n",
    "        maxPixels=1e13\n",
    "    ).get('area')\n",
    "    \n",
    "    # Manejar casos donde no hay nubes (area_nubes serÃ­a null)\n",
    "    area_nubes = ee.Number(area_nubes).divide(ee.Number(area_total)).multiply(100)\n",
    "    porcentaje_nubes = ee.Number(ee.Algorithms.If(area_nubes, area_nubes, 0))\n",
    "    \n",
    "    return porcentaje_nubes\n",
    "\n",
    "# FunciÃ³n para obtener primer mosaico vÃ¡lido (sin nubes en el polÃ­gono)\n",
    "def get_mosaico_valid(geometry, start_date, end_date):\n",
    "    date_list = ee.List.sequence(0, ee.Date(end_date).difference(ee.Date(start_date), 'day').subtract(1))\n",
    "    fechas = date_list.map(lambda d: ee.Date(start_date).advance(d, 'day'))\n",
    "\n",
    "    def iterate_func(fecha, result):\n",
    "        result = ee.Dictionary(result)\n",
    "        found = result.get('encontrado')\n",
    "        fecha = ee.Date(fecha)\n",
    "\n",
    "        # Filtrar imÃ¡genes del dÃ­a\n",
    "        coleccion_dia = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(geometry) \\\n",
    "            .filterDate(fecha, fecha.advance(1, 'day')) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_nubes))\n",
    "\n",
    "        # Verificar si hay imÃ¡genes disponibles\n",
    "        cond_imagenes = coleccion_dia.size().gt(0)\n",
    "\n",
    "        def check_polygon_clouds():\n",
    "            # Tomar la primera imagen para verificar nubes en el polÃ­gono\n",
    "            primera_imagen = ee.Image(coleccion_dia.first())\n",
    "            porcentaje_nubes_poligono = check_clouds_in_polygon(primera_imagen, geometry)\n",
    "            \n",
    "            # Verificar si el porcentaje de nubes en el polÃ­gono es aceptable\n",
    "            imagen_valida = porcentaje_nubes_poligono.lt(max_nubes_poligono)\n",
    "            \n",
    "            return ee.Dictionary({\n",
    "                'encontrado': True, \n",
    "                'fecha': fecha,\n",
    "                'porcentaje_nubes': porcentaje_nubes_poligono,\n",
    "                'imagen_valida': imagen_valida\n",
    "            })\n",
    "\n",
    "        return ee.Algorithms.If(found,\n",
    "            result,\n",
    "            ee.Algorithms.If(cond_imagenes,\n",
    "                ee.Algorithms.If(\n",
    "                    ee.Dictionary(check_polygon_clouds()).get('imagen_valida'),\n",
    "                    check_polygon_clouds(),\n",
    "                    result\n",
    "                ),\n",
    "                result\n",
    "            )\n",
    "        )\n",
    "\n",
    "    resultado = ee.List(fechas).iterate(iterate_func, ee.Dictionary({'encontrado': False}))\n",
    "    resultado = ee.Dictionary(resultado)\n",
    "\n",
    "    # Verificar si se encontrÃ³ una imagen vÃ¡lida\n",
    "    encontrado = resultado.get('encontrado')\n",
    "    \n",
    "    def create_mosaic():\n",
    "        fecha_valida = ee.Date(resultado.get('fecha'))\n",
    "        porcentaje_nubes_final = resultado.get('porcentaje_nubes')\n",
    "        \n",
    "        coleccion = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(geometry) \\\n",
    "            .filterDate(fecha_valida, fecha_valida.advance(1, 'day')) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_nubes)) \\\n",
    "            .map(maskS2sr)\n",
    "\n",
    "        mosaico = coleccion.mosaic().clip(geometry).set({\n",
    "            'fecha_real': fecha_valida.format('YYYY-MM-dd'),\n",
    "            'porcentaje_nubes_poligono': porcentaje_nubes_final,\n",
    "            'imagen_encontrada': True\n",
    "        })\n",
    "        \n",
    "        return mosaico\n",
    "    \n",
    "    def empty_image():\n",
    "        return ee.Image().set({\n",
    "            'fecha_real': 'No encontrada',\n",
    "            'porcentaje_nubes_poligono': -1,\n",
    "            'imagen_encontrada': False\n",
    "        })\n",
    "    \n",
    "    return ee.Algorithms.If(encontrado, create_mosaic(), empty_image())\n",
    "\n",
    "# Iterar sobre cada polÃ­gono\n",
    "for idx, row in gdf.iterrows():\n",
    "    fecha_str = row['fecha_obje']\n",
    "    from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "    # Dentro del bucle for\n",
    "    if isinstance(row.geometry, MultiPolygon):\n",
    "        geom_shapely = list(row.geometry.geoms)[0]  # Usa solo el primer polÃ­gono\n",
    "    else:\n",
    "        geom_shapely = row.geometry\n",
    "\n",
    "    geom_coords = list(geom_shapely.exterior.coords)\n",
    "    geom = ee.Geometry.Polygon([geom_coords])\n",
    "    target_date = ee.Date(fecha_str)\n",
    "\n",
    "    fecha_inicio_antes = target_date.advance(-dias_rango, 'day')\n",
    "    fecha_fin_antes = target_date\n",
    "    fecha_inicio_despues = target_date.advance(1, 'day')\n",
    "    fecha_fin_despues = target_date.advance(dias_rango + 1, 'day')\n",
    "\n",
    "    try:\n",
    "        print(f'ğŸ” Procesando polÃ­gono {idx}, fecha {fecha_str}')\n",
    "        \n",
    "        mosaico_antes = get_mosaico_valid(geom, fecha_inicio_antes, fecha_fin_antes)\n",
    "        mosaico_despues = get_mosaico_valid(geom, fecha_inicio_despues, fecha_fin_despues)\n",
    "\n",
    "        # Verificar que ambas imÃ¡genes fueron encontradas\n",
    "        try:\n",
    "            imagen_antes_ok = ee.Image(mosaico_antes).get('imagen_encontrada').getInfo()\n",
    "            imagen_despues_ok = ee.Image(mosaico_despues).get('imagen_encontrada').getInfo()\n",
    "        except:\n",
    "            # Si hay error al obtener la propiedad, asumir que no se encontraron imÃ¡genes\n",
    "            imagen_antes_ok = False\n",
    "            imagen_despues_ok = False\n",
    "        \n",
    "        # Solo proceder si ambas imÃ¡genes son vÃ¡lidas\n",
    "        if imagen_antes_ok and imagen_despues_ok:\n",
    "            try:\n",
    "                # Obtener informaciÃ³n de las nubes\n",
    "                fecha_antes = ee.Image(mosaico_antes).get('fecha_real').getInfo()\n",
    "                fecha_despues = ee.Image(mosaico_despues).get('fecha_real').getInfo()\n",
    "                nubes_antes = ee.Image(mosaico_antes).get('porcentaje_nubes_poligono').getInfo()\n",
    "                nubes_despues = ee.Image(mosaico_despues).get('porcentaje_nubes_poligono').getInfo()\n",
    "                \n",
    "                # Mostrar informaciÃ³n de las imÃ¡genes encontradas\n",
    "                print(f'  ğŸ“… Imagen ANTES: {fecha_antes} (nubes: {nubes_antes:.1f}%)')\n",
    "                print(f'  ğŸ“… Imagen DESPUÃ‰S: {fecha_despues} (nubes: {nubes_despues:.1f}%)')\n",
    "\n",
    "                # Calcular dNBR\n",
    "                def calc_nbr(img):\n",
    "                    return ee.Image(img).normalizedDifference(['B8', 'B12']).rename('NBR')\n",
    "\n",
    "                nbr_antes = calc_nbr(mosaico_antes)\n",
    "                nbr_despues = calc_nbr(mosaico_despues)\n",
    "                dnbr = nbr_antes.subtract(nbr_despues).multiply(1000).rename('dNBR')\n",
    "\n",
    "                # Exportar a Drive\n",
    "                export_id = f'dNBR_{idx}_{fecha_str}'\n",
    "                task = ee.batch.Export.image.toDrive(\n",
    "                    image=dnbr,\n",
    "                    description=export_id,\n",
    "                    folder='GEE_SENTINEL',\n",
    "                    fileNamePrefix=export_id,\n",
    "                    region=geom,\n",
    "                    scale=10,\n",
    "                    maxPixels=1e13,\n",
    "                    crs='EPSG:4326'\n",
    "                )\n",
    "                task.start()\n",
    "                print(f'âœ… Exportando dNBR para polÃ­gono {idx}, fecha {fecha_str}')\n",
    "                print(f'   Task ID: {task.id}')\n",
    "                \n",
    "                # Opcional: esperar un poco entre exportaciones para no sobrecargar\n",
    "                import time\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e_inner:\n",
    "                print(f'âš ï¸ Error al procesar imÃ¡genes vÃ¡lidas para polÃ­gono {idx}: {e_inner}')\n",
    "        else:\n",
    "            if not imagen_antes_ok:\n",
    "                print(f'âš ï¸ No se encontrÃ³ imagen ANTES vÃ¡lida (sin nubes) para polÃ­gono {idx}')\n",
    "            if not imagen_despues_ok:\n",
    "                print(f'âš ï¸ No se encontrÃ³ imagen DESPUÃ‰S vÃ¡lida (sin nubes) para polÃ­gono {idx}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ Error en polÃ­gono {idx}: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Areas_Quemadas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
